{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kepners/ChopOnions/blob/main/Trndzo3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Install Required Packages\n",
        "\n",
        "!pip install --upgrade openai==0.28.1 python-dotenv praw rake_nltk newsapi-python --upgrade lxml_html_clean bleach --upgrade newspaper3k feedparser aiohttp nest_asyncio structlog beautifulsoup4 cachetools fuzzywuzzy python-Levenshtein nltk pytrends ratelimit prettytable bs4 configparser structlog\n",
        "print(\"pip install completed\")\n",
        "import nest_asyncio\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "import lxml\n",
        "import newspaper\n",
        "\n",
        "print(f\"lxml version: {lxml.__version__}\")\n",
        "print(f\"newspaper version: {newspaper.__version__}\")\n",
        "\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "print(\"vader_lexicon downloaded\")\n",
        "nltk.download('stopwords')\n",
        "print(\"stopwords downloaded\")\n",
        "nltk.download('punkt')\n",
        "print(\"punkt downloaded\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "print(\"averaged_perceptron_tagger downloaded\")\n",
        "nltk.download('maxent_ne_chunker')\n",
        "print(\"maxent_ne_chunker downloaded\")\n",
        "nltk.download('words')\n",
        "print(\"words downloaded\")\n",
        "nltk.download('wordnet')\n",
        "print(\"wordnet downloaded\")\n",
        "nltk.download('punkt_tab')\n",
        "print('punkt_tab downloaded')\n",
        "\n",
        "\n",
        "print(\"Installation and NLTK data download completed successfully.\")"
      ],
      "metadata": {
        "id": "CGNrZ1kNMSXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 2: Mount Google Drive and Load Configuration\n",
        "from google.colab import drive\n",
        "import configparser\n",
        "import os\n",
        "import openai  # import the openai module\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to your config.ini in Google Drive\n",
        "config_path = '/content/drive/MyDrive/Secrets/config.ini'  # Update this path as needed\n",
        "\n",
        "# Check if config.ini exists at the specified path\n",
        "if not os.path.exists(config_path):\n",
        "    raise FileNotFoundError(f\"config.ini not found at {config_path}\")\n",
        "\n",
        "# Load configuration using configparser\n",
        "config = configparser.ConfigParser()\n",
        "config.read(config_path)\n",
        "\n",
        "# Retrieve OpenAI API key\n",
        "try:\n",
        "    openai_api_key = config.get('openai', 'api_key', fallback=None)\n",
        "except KeyError:\n",
        "    raise ValueError(\"OpenAI API key not found or invalid format in config.ini under [openai] section.\")\n",
        "\n",
        "if not openai_api_key:\n",
        "    raise ValueError(\"OpenAI API key not found or invalid format in config.ini under [openai] section.\")\n",
        "\n",
        "# Set the OpenAI API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "# Retrieve NewsAPI key\n",
        "try:\n",
        "    newsapi_key = config.get('newsapi', 'api_key')\n",
        "except KeyError:\n",
        "    raise ValueError(\"NewsAPI key not found in config.ini under [newsapi] section.\")\n",
        "\n",
        "# Retrieve caching configurations\n",
        "rss_cache_ttl = config.getint('CACHING', 'RSS_CACHE_TTL', fallback=86400)\n",
        "trend_cache_ttl = config.getint('CACHING', 'TREND_CACHE_TTL', fallback=3600)\n",
        "openai_cache_ttl = config.getint('CACHING', 'OPENAI_CACHE_TTL', fallback=86400)\n",
        "\n",
        "# Retrieve rate limiting configurations\n",
        "rss_calls_per_day = config.getint('RATE_LIMITING', 'RSS_CALLS_PER_DAY', fallback=1000)\n",
        "\n",
        "print(\"Configuration loaded successfully.\")\n"
      ],
      "metadata": {
        "id": "BNlyvpVs_QsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 3: Define Data Structures\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional\n",
        "\n",
        "@dataclass\n",
        "class GoogleTrend:\n",
        "    topic: str\n",
        "    interest: str\n",
        "    sentiment: str\n",
        "\n",
        "@dataclass\n",
        "class Trend:\n",
        "    title: str\n",
        "    description: str\n",
        "    source: str\n",
        "    approx_traffic: str\n",
        "    sentiment: str\n",
        "    google_trend: Optional[GoogleTrend] = None\n",
        "\n",
        "@dataclass\n",
        "class ScriptOptions:\n",
        "    style: str = \"Normal Script\"\n",
        "    tone: str = \"Informative\"\n",
        "    length: str = \"60 seconds\"\n",
        "\n",
        "print(\"Data structures defined successfully.\")\n"
      ],
      "metadata": {
        "id": "4wkdmt7yp0mR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIud78FOZR3M"
      },
      "outputs": [],
      "source": [
        "# Block 4: Configuration and Initialization\n",
        "\n",
        "import configparser\n",
        "from pytrends.request import TrendReq # This import will now work\n",
        "from newsapi import NewsApiClient\n",
        "\n",
        "# Initialize OpenAI\n",
        "api_key = openai_api_key\n",
        "\n",
        "# Initialize NewsAPI\n",
        "newsapi = NewsApiClient(api_key=newsapi_key)\n",
        "\n",
        "# Initialize PyTrends with custom requests arguments\n",
        "pytrends = TrendReq(hl='en-US', tz=360, requests_args={'headers': {'User-Agent': 'Mozilla/5.0'}})\n",
        "\n",
        "print(\"Configuration and initialization completed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGPK1IRBXfBm"
      },
      "outputs": [],
      "source": [
        "# Block 5: Create utils/data_processing.py and utils/__init__.py\n",
        "\n",
        "# Define the directory where helper functions will reside\n",
        "utils_dir = 'utils'\n",
        "\n",
        "# Create the 'utils' directory if it doesn't exist\n",
        "os.makedirs(utils_dir, exist_ok=True)\n",
        "\n",
        "# Define the path for the __init__.py file to make 'utils' a package\n",
        "init_path = os.path.join(utils_dir, '__init__.py')\n",
        "\n",
        "# Create an empty __init__.py file if it doesn't exist\n",
        "if not os.path.exists(init_path):\n",
        "    with open(init_path, 'w') as file:\n",
        "        pass  # Creating an empty __init__.py\n",
        "    print(f\"Created empty '__init__.py' at '{init_path}' to make 'utils' a package.\")\n",
        "else:\n",
        "    print(f\"'__init__.py' already exists at '{init_path}'.\")\n",
        "\n",
        "# Define the path for the data_processing.py file\n",
        "data_processing_path = os.path.join(utils_dir, 'data_processing.py')\n",
        "\n",
        "# Define the content for data_processing.py\n",
        "data_processing_code = \"\"\"\n",
        "# data_processing.py\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ne_chunk, pos_tag\n",
        "from nltk.tree import Tree\n",
        "\n",
        "def clean_text(text):\n",
        "    '''\n",
        "    Cleans the input text by removing URLs, special characters, and stopwords.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to clean.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned text.\n",
        "    '''\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\\\S+', '', text)\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^A-Za-z\\\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    # Join the words back into a single string\n",
        "    cleaned_text = ' '.join(filtered_words)\n",
        "    return cleaned_text\n",
        "\n",
        "def extract_entities(text):\n",
        "    '''\n",
        "    Extracts named entities from the input text.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to extract entities from.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of named entities.\n",
        "    '''\n",
        "    def get_entities(tree):\n",
        "        entities = []\n",
        "        for subtree in tree:\n",
        "            if isinstance(subtree, Tree):\n",
        "                entity = \" \".join([token for token, pos in subtree.leaves()])\n",
        "                entities.append(entity)\n",
        "        return entities\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = pos_tag(tokens)\n",
        "    chunked = ne_chunk(tagged)\n",
        "    entities = get_entities(chunked)\n",
        "    return entities\n",
        "\"\"\"\n",
        "\n",
        "# Write the data_processing.py file\n",
        "with open(data_processing_path, 'w') as file:\n",
        "    file.write(data_processing_code)\n",
        "\n",
        "print(f\"Created '{data_processing_path}' successfully.\")\n",
        "\n",
        "# Optional: Verify the creation by listing the 'utils' directory\n",
        "print(\"\\nVerifying the creation of 'data_processing.py' and '__init__.py':\")\n",
        "!ls -l utils/\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(f\"Current Working Directory: {os.getcwd()}\")\n"
      ],
      "metadata": {
        "id": "vLVzZkCSYp0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "C5RZdBozr4hJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6ee7938-1df6-47a6-e304-d5c61f11ff01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the country for trending topics data (e.g., United States or US):\n",
            "Country: us\n",
            "\n",
            "You selected: United States\n",
            "\n",
            "Select the time frame for trending topics:\n",
            "A. Last 4 hours\n",
            "B. Last 24 hours\n",
            "C. Last 7 days\n",
            "Enter the letter of the time frame you're interested in: b\n",
            "✨ Almost there, thank you for your patience! (This could take up to 2 minutes. Please wait.)   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:{\"event\": \"Rate limit hit for topic 'John Mulaney'. Retrying after a delay...\", \"timestamp\": \"2024-11-13T20:32:53.342323Z\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Gathering the latest trends... (This could take up to 2 minutes. Please wait.)   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:{\"event\": \"Rate limit hit for topic 'John Mulaney'. Retrying after a delay...\", \"timestamp\": \"2024-11-13T20:32:55.578518Z\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏳ Processing data, please wait... (This could take up to 2 minutes. Please wait.)   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:__main__:{\"event\": \"Rate limit hit for topic 'John Mulaney'. Retrying after a delay...\", \"timestamp\": \"2024-11-13T20:32:59.816619Z\"}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                    \n",
            "\n",
            "Currently Trending in United States in the last 24 hours (Showing 1 to 10 of 10):\n",
            "\n",
            "+-----+-----------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "| No. |      Topic      |               Description                |    Source   |   Approx Traffic   | Sentiment |\n",
            "+-----+-----------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  1  |    John Thune   |     The text provided does not offer     | Google News | 1.863905325443787  |  Positive |\n",
            "|     |                 |   sufficient information to generate a   |             |                    |           |\n",
            "|     |                 |  detailed and comprehensive summary. It  |             |                    |           |\n",
            "|     |                 | only mentions \"Thune,\" likely referring  |             |                    |           |\n",
            "|     |                 | to John Thune, a U.S. Senator from South |             |                    |           |\n",
            "|     |                 |    Dakota, and his position as Senate    |             |                    |           |\n",
            "|     |                 |    majority leader. However, without     |             |                    |           |\n",
            "|     |                 |      further context or details, a       |             |                    |           |\n",
            "|     |                 |     comprehensive summary cannot be      |             |                    |           |\n",
            "|     |                 |                provided.                 |             |                    |           |\n",
            "+-----+-----------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  2  |      Canary     |  The text provided does not contain any  | Google News | 22.662721893491124 |  Positive |\n",
            "|     |                 | information to summarize. Please provide |             |                    |           |\n",
            "|     |                 |     a detailed text about \"Canary\".      |             |                    |           |\n",
            "+-----+-----------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  3  |     Pangolin    |  The text appears to be about pangolins  | Google News | 10.355029585798816 |  Positive |\n",
            "|     |                 |   but there is no detailed information   |             |                    |           |\n",
            "|     |                 |  provided to summarize. Please provide   |             |                    |           |\n",
            "|     |                 |    more information or context about     |             |                    |           |\n",
            "|     |                 |  pangolins for a comprehensive summary.  |             |                    |           |\n",
            "+-----+-----------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  4  |   Dave Coulier  |  The text provided does not contain any  | Google News | 4.763313609467455  |  Positive |\n",
            "|     |                 |    information to summarize. It only     |             |                    |           |\n",
            "|     |                 | includes the name \"Dave Coulier\", who is |             |                    |           |\n",
            "|     |                 |     a well-known American actor and      |             |                    |           |\n",
            "|     |                 |   comedian, famous for his role in the   |             |                    |           |\n",
            "|     |                 | sitcom \"Full House\". Please provide more |             |                    |           |\n",
            "|     |                 |  details or context for a comprehensive  |             |                    |           |\n",
            "|     |                 |                 summary.                 |             |                    |           |\n",
            "+-----+-----------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  5  |    Wednesday    | As an AI, I need you to provide the text | Google News | 37.10059171597633  |  Positive |\n",
            "|     |                 |       you'd like me to summarize.        |             |                    |           |\n",
            "|     |                 | Unfortunately, you haven't provided any  |             |                    |           |\n",
            "|     |                 | text beyond the word \"Wednesday\". Please |             |                    |           |\n",
            "|     |                 |   provide the text and I'll be glad to   |             |                    |           |\n",
            "|     |                 |          help you summarize it.          |             |                    |           |\n",
            "+-----+-----------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  6  |    Cava stock   |  The text provided does not contain any  | Google News |  8.57396449704142  |  Positive |\n",
            "|     |                 |    information or details about \"Cava    |             |                    |           |\n",
            "|     |                 |  stock\" that can be summarized. Please   |             |                    |           |\n",
            "|     |                 |  provide a detailed text to generate a   |             |                    |           |\n",
            "|     |                 |          comprehensive summary.          |             |                    |           |\n",
            "+-----+-----------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  7  | Whoopi Goldberg |  The text provided does not contain any  | Google News | 20.124260355029588 |  Negative |\n",
            "|     |                 |   information or context about Whoopi    |             |                    |           |\n",
            "|     |                 |   Goldberg, hence a summary cannot be    |             |                    |           |\n",
            "|     |                 |   generated. Please provide a detailed   |             |                    |           |\n",
            "|     |                 |         text for summarization.          |             |                    |           |\n",
            "+-----+-----------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  8  |   Chanel Banks  |  The text provided does not include any  | Google News | 4.905325443786983  |  Positive |\n",
            "|     |                 | details or context about \"Chanel Banks\"  |             |                    |           |\n",
            "|     |                 |  that can be summarized. Please provide  |             |                    |           |\n",
            "|     |                 | more information or context to generate  |             |                    |           |\n",
            "|     |                 |         a comprehensive summary.         |             |                    |           |\n",
            "+-----+-----------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  9  |   John Mulaney  |  The text provided does not contain any  | Google News | 29.988165680473372 |  Positive |\n",
            "|     |                 |  details, context, or key points about   |             |                    |           |\n",
            "|     |                 |    John Mulaney to summarize. Please     |             |                    |           |\n",
            "|     |                 |        provide more information.         |             |                    |           |\n",
            "+-----+-----------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  10 |   Pete Hegseth  |     The text refers to Pete Hegseth      | Google News | 2.7041420118343193 |  Positive |\n",
            "|     |                 | potentially serving as the Secretary of  |             |                    |           |\n",
            "|     |                 | Defense under the Trump administration.  |             |                    |           |\n",
            "|     |                 | The repeated phrases suggest a focus on  |             |                    |           |\n",
            "|     |                 |    this specific role and individual.    |             |                    |           |\n",
            "|     |                 |  However, the text does not provide any  |             |                    |           |\n",
            "|     |                 | additional details or context about Pete |             |                    |           |\n",
            "|     |                 |  Hegseth's qualifications, actions, or   |             |                    |           |\n",
            "|     |                 |    the implications of his potential     |             |                    |           |\n",
            "|     |                 |               appointment.               |             |                    |           |\n",
            "+-----+-----------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "Type 'more' or '+' to view more results, enter the number of the topic to select it, or any other key to exit: 1\n",
            "\n",
            "You selected: John Thune\n",
            "\n",
            "You can customize the script generation. If you wish to skip any option, just press Enter.\n",
            "Select script style (Flashy / Expressive / Normal): Normal\n",
            "Select script tone (e.g., Informative, Persuasive, Emotional): happy\n",
            "Enter script length in seconds (e.g., 60, 120): 30\n",
            "\n",
            "Selected Options - Style: Normal, Tone: happy, Length: 30 seconds\n",
            "\n",
            "Generating script for the selected topic...\n",
            "Script generated successfully.\n",
            "\n",
            "### Generated Script for 'John Thune':\n",
            "\n",
            "(Upbeat Music)\n",
            "\n",
            "Narrator: \"Meet John Thune, a beacon of leadership and integrity. Hailing proudly from the heart of South Dakota, Thune serves as a U.S. Senator, representing the unwavering spirit of his home state.\"\n",
            "\n",
            "(Images of John Thune working diligently)\n",
            "\n",
            "Narrator: \"Thune's role as Senate Majority Leader not only showcases his influence but also his commitment to making a difference. His dedication is as vast as the South Dakota prairies!\"\n",
            "\n",
            "(Images of South Dakota landscapes, transitioning to Thune interacting\n",
            "\n",
            "**Source:** Google News\n",
            "\n",
            "====\n",
            "\n",
            "Script generation completed.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import structlog\n",
        "from prettytable import PrettyTable, HRuleStyle  # Import HRuleStyle for hrules\n",
        "from cachetools import TTLCache, cached\n",
        "from fuzzywuzzy import process\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from rake_nltk import Rake\n",
        "import feedparser\n",
        "import time\n",
        "import re\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "import openai\n",
        "import openai.error  # Add this line\n",
        "import textwrap  # For wrapping text in the table\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from newsapi import NewsApiClient\n",
        "from bs4 import BeautifulSoup\n",
        "import difflib\n",
        "import nltk\n",
        "import threading\n",
        "import configparser  # Import configparser\n",
        "import random\n",
        "import requests\n",
        "from newspaper import Article\n",
        "from pytrends.request import TrendReq\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "import pandas as pd\n",
        "pd.set_option('future.no_silent_downcasting', True)\n",
        "from random import uniform\n",
        "import time\n",
        "import logging\n",
        "from contextlib import redirect_stdout\n",
        "\n",
        "# ----------------------------\n",
        "# Suppress NLTK logging messages\n",
        "# ----------------------------\n",
        "logging.getLogger('nltk').setLevel(logging.CRITICAL)\n",
        "\n",
        "# Suppress stdout for NLTK downloads\n",
        "with open(os.devnull, 'w') as f:\n",
        "    with redirect_stdout(f):\n",
        "        try:\n",
        "            nltk.data.find('tokenizers/punkt')\n",
        "            nltk.data.find('corpora/stopwords')\n",
        "            nltk.data.find('sentiment/vader_lexicon.zip')\n",
        "        except LookupError:\n",
        "            nltk.download('vader_lexicon')\n",
        "            nltk.download('stopwords')\n",
        "            nltk.download('punkt')\n",
        "# ----------------------------\n",
        "# Structured Logging with structlog\n",
        "# ----------------------------\n",
        "\n",
        "structlog.configure(\n",
        "    processors=[\n",
        "        structlog.processors.TimeStamper(fmt=\"iso\"),\n",
        "        structlog.processors.JSONRenderer()\n",
        "    ],\n",
        "    context_class=dict,\n",
        "    logger_factory=structlog.stdlib.LoggerFactory(),\n",
        "    wrapper_class=structlog.stdlib.BoundLogger,\n",
        "    cache_logger_on_first_use=True,\n",
        ")\n",
        "\n",
        "logger = structlog.get_logger()\n",
        "\n",
        "# ----------------------------\n",
        "# Read API Keys from config.ini\n",
        "# ----------------------------\n",
        "\n",
        "# Initialize ConfigParser\n",
        "config = configparser.ConfigParser()\n",
        "config.read('/content/drive/MyDrive/Secrets/config.ini')  # Ensure this path is correct\n",
        "\n",
        "# Handle missing API keys gracefully\n",
        "try:\n",
        "    openai_api_key = config.get('openai', 'api_key')\n",
        "    newsapi_api_key = config.get('newsapi', 'api_key')\n",
        "except (configparser.NoSectionError, configparser.NoOptionError) as e:\n",
        "    logger.error(f\"Error reading API keys from config.ini: {e}\", exc_info=True)\n",
        "    sys.exit(\"Failed to read API keys from config.ini. Please ensure the file exists and contains the necessary keys.\")\n",
        "\n",
        "# Set your OpenAI API key from config.ini\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "# Initialize NewsApiClient with your API key from config.ini\n",
        "newsapi = NewsApiClient(api_key=newsapi_api_key)\n",
        "\n",
        "# ----------------------------\n",
        "# Caching\n",
        "# ----------------------------\n",
        "\n",
        "# Define caching constants\n",
        "TREND_CACHE_TTL = 3600       # 1 hour in seconds\n",
        "OPENAI_CACHE_TTL = 86400     # 1 day in seconds\n",
        "\n",
        "# Initialize caches with defined TTLs\n",
        "trends_cache = TTLCache(maxsize=100, ttl=TREND_CACHE_TTL)\n",
        "openai_cache = TTLCache(maxsize=1000, ttl=OPENAI_CACHE_TTL)\n",
        "\n",
        "# ----------------------------\n",
        "# Rate Limiting for RSS Feeds\n",
        "# ----------------------------\n",
        "\n",
        "ONE_DAY = 86400               # Seconds in one day\n",
        "RSS_CALLS_PER_DAY = 100       # Maximum number of RSS feed calls per day\n",
        "\n",
        "@sleep_and_retry\n",
        "@limits(calls=RSS_CALLS_PER_DAY, period=ONE_DAY)\n",
        "def fetch_rss_feed_sync(rss_url):\n",
        "    # Placeholder for synchronous RSS feed fetching if needed\n",
        "    pass\n",
        "\n",
        "# ----------------------------\n",
        "# Initialize Sentiment Analyzer and Keyword Extractor\n",
        "# ----------------------------\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize Sentiment Analyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "rake = Rake()\n",
        "\n",
        "# ----------------------------\n",
        "# OpenAI API Key and Semaphore\n",
        "# ----------------------------\n",
        "\n",
        "# Define a semaphore to limit concurrent requests\n",
        "google_trends_semaphore = asyncio.Semaphore(2)  # Reduce concurrency to 2\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Helper Functions\n",
        "# ----------------------------\n",
        "\n",
        "# Available countries with multiple codes\n",
        "available_countries = {\n",
        "    'United States': ['US', 'USA'],\n",
        "    'Canada': ['CA'],\n",
        "    'United Kingdom': ['GB', 'UK'],\n",
        "    'Australia': ['AU'],\n",
        "    'India': ['IN'],\n",
        "    'Germany': ['DE'],\n",
        "    'France': ['FR'],\n",
        "    'Japan': ['JP'],\n",
        "    'Brazil': ['BR'],\n",
        "    'South Korea': ['KR'],\n",
        "    'Argentina': ['AR'],\n",
        "    'Mexico': ['MX'],\n",
        "    'Singapore': ['SG'],\n",
        "    'Spain': ['ES'],\n",
        "    'Italy': ['IT'],\n",
        "    'Netherlands': ['NL'],\n",
        "    'Poland': ['PL'],\n",
        "    'Sweden': ['SE'],\n",
        "    'Switzerland': ['CH'],\n",
        "    # Add more countries and their codes as needed\n",
        "}\n",
        "\n",
        "def get_matching_country(input_country, available_countries):\n",
        "    \"\"\"\n",
        "    Matches user input to available countries.\n",
        "\n",
        "    Parameters:\n",
        "        input_country (str): The country input by the user.\n",
        "        available_countries (dict): Dictionary mapping country names to lists of codes.\n",
        "\n",
        "    Returns:\n",
        "        str or None: The matched country name or None if no match.\n",
        "    \"\"\"\n",
        "    normalized_input = input_country.lower()\n",
        "    for country, codes in available_countries.items():\n",
        "        # Check if input matches the country name\n",
        "        if normalized_input == country.lower():\n",
        "            return country\n",
        "        # Check if input matches any of the country codes\n",
        "        if normalized_input in [code.lower() for code in codes]:\n",
        "            return country\n",
        "    # If no direct match, attempt fuzzy matching\n",
        "    country_names = list(available_countries.keys())\n",
        "    closest_matches = difflib.get_close_matches(input_country, country_names, n=1, cutoff=0.8)\n",
        "    if closest_matches:\n",
        "        return closest_matches[0]\n",
        "    return None\n",
        "\n",
        "def sanitize_topic(topic):\n",
        "    \"\"\"\n",
        "    Sanitizes the topic string by removing unwanted characters.\n",
        "\n",
        "    Parameters:\n",
        "        topic (str): The topic to sanitize.\n",
        "\n",
        "    Returns:\n",
        "        str: The sanitized topic.\n",
        "    \"\"\"\n",
        "    return re.sub(r'[^\\w\\s]', '', topic)\n",
        "\n",
        "def extract_source(url):\n",
        "    \"\"\"\n",
        "    Extracts the main domain name from the URL to identify the source.\n",
        "\n",
        "    Parameters:\n",
        "        url (str): The URL of the news article.\n",
        "\n",
        "    Returns:\n",
        "        str: The name of the source.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from urllib.parse import urlparse\n",
        "        domain = urlparse(url).netloc\n",
        "        domain = domain.lower()\n",
        "        domain_mapping = {\n",
        "            'cbsnews.com': 'CBS News',\n",
        "            'cnn.com': 'CNN',\n",
        "            'foxnews.com': 'Fox News',\n",
        "            'abcnews.go.com': 'ABC News',\n",
        "            'bbc.co.uk': 'BBC',\n",
        "            'google.com': 'Google News',\n",
        "            'news.google.com': 'Google News',\n",
        "            'reuters.com': 'Reuters',\n",
        "            'theguardian.com': 'The Guardian',\n",
        "            'nytimes.com': 'The New York Times',\n",
        "            'usatoday.com': 'USA Today',\n",
        "            'fortworthstar.com': 'Fort Worth Star-Telegram',\n",
        "            'wcnc.com': 'WCNC',\n",
        "            'apnews.com': 'AP News',\n",
        "            'floridatoday.com': 'Florida Today',\n",
        "            'msnbc.com': 'MSNBC News',\n",
        "            # Add more mappings as needed\n",
        "        }\n",
        "        # Handle subdomains (e.g., edition.cnn.com)\n",
        "        domain_parts = domain.split('.')\n",
        "        if len(domain_parts) > 2:\n",
        "            domain = '.'.join(domain_parts[-2:])\n",
        "        return domain_mapping.get(domain, domain.capitalize())\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting source from URL '{url}': {e}\", exc_info=True)\n",
        "        return \"Unknown Source\"\n",
        "\n",
        "def broaden_query(query):\n",
        "    \"\"\"\n",
        "    Broadens the query by adding synonyms or related terms.\n",
        "\n",
        "    Parameters:\n",
        "        query (str): The original query.\n",
        "\n",
        "    Returns:\n",
        "        str: The broadened query.\n",
        "    \"\"\"\n",
        "    # Placeholder for query broadening logic\n",
        "    return query\n",
        "\n",
        "@cached(trends_cache)\n",
        "def fetch_google_trends_cached(topic, timeframe='now 7-d', max_retries=5):\n",
        "    \"\"\"\n",
        "    Fetches Google Trends data for the given topic with retry logic.\n",
        "    \"\"\"\n",
        "    pytrends = TrendReq(hl='en-US', tz=360)\n",
        "    attempt = 0\n",
        "    wait_time = 2  # Initial wait time in seconds\n",
        "\n",
        "    while attempt < max_retries:\n",
        "        try:\n",
        "            pytrends.build_payload([topic], timeframe=timeframe)\n",
        "            interest_over_time = pytrends.interest_over_time()\n",
        "            if interest_over_time is not None and not interest_over_time.empty:\n",
        "                avg_interest = interest_over_time[topic].mean()\n",
        "                sentiment = 'Neutral'\n",
        "                return {'topic': topic, 'interest': avg_interest, 'sentiment': sentiment}\n",
        "            else:\n",
        "                logger.error(f\"No data returned for topic '{topic}' from Google Trends.\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            # Gracefully handle rate limits (429) and other errors\n",
        "            if \"429\" in str(e):\n",
        "                logger.warning(f\"Rate limit hit for topic '{topic}'. Retrying after a delay...\")\n",
        "            else:\n",
        "                logger.error(f\"Error fetching Google Trends data for '{topic}': {e}\", exc_info=True)\n",
        "\n",
        "            time.sleep(wait_time)\n",
        "            wait_time = min(wait_time * 2, 60)  # Exponential backoff\n",
        "            attempt += 1\n",
        "\n",
        "    logger.error(f\"Max retries exceeded for Google Trends data fetch for '{topic}'.\")\n",
        "    return None\n",
        "\n",
        "def fetch_google_trends(topic, timeframe='now 7-d', retries=3, backoff_factor=2):\n",
        "    \"\"\"\n",
        "    Fetches Google Trends data with retries and backoff.\n",
        "    \"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            # Initialize pytrends\n",
        "            from pytrends.request import TrendReq\n",
        "            pytrends = TrendReq(hl='en-US', tz=360)\n",
        "            # Prepare the payload for pytrends\n",
        "            pytrends.build_payload([topic], timeframe=timeframe)\n",
        "            interest_over_time = pytrends.interest_over_time()\n",
        "            if not interest_over_time.empty:\n",
        "                # Get the average interest\n",
        "                avg_interest = interest_over_time[topic].mean()\n",
        "                sentiment = analyze_sentiment(topic)\n",
        "                return {'topic': topic, 'interest': avg_interest, 'sentiment': sentiment}\n",
        "            else:\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching Google Trends data for '{topic}': {e}\", exc_info=True)\n",
        "            time.sleep(backoff_factor * (2 ** attempt))\n",
        "    return None\n",
        "\n",
        "async def generate_summary_async(content, max_retries=5):\n",
        "    \"\"\"\n",
        "    Generates a summary of the content using OpenAI's GPT with retry logic.\n",
        "\n",
        "    Parameters:\n",
        "        content (str): The content to summarize.\n",
        "        max_retries (int): Maximum number of retries upon failure.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated summary or a fallback message.\n",
        "    \"\"\"\n",
        "    attempt = 0\n",
        "    wait_time = 1  # Initial wait time in seconds\n",
        "\n",
        "    while attempt < max_retries:\n",
        "        try:\n",
        "            summary_prompt = (\n",
        "                \"Generate a detailed and comprehensive summary of the following text. \"\n",
        "                \"Make sure to include key points, context, and any relevant details:\\n\\n\"\n",
        "                f\"{content}\\n\\nSummary:\"\n",
        "            )\n",
        "\n",
        "            # Use OpenAI API to generate the summary\n",
        "            async with openai_semaphore:\n",
        "                response = await openai.ChatCompletion.acreate(\n",
        "                    model=\"gpt-4\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes text.\"},\n",
        "                        {\"role\": \"user\", \"content\": summary_prompt}\n",
        "                    ],\n",
        "                    max_tokens=500,  # Adjust for summary length\n",
        "                    temperature=0.5,\n",
        "                )\n",
        "            summary = response.choices[0].message['content'].strip()\n",
        "\n",
        "            # Return a meaningful summary\n",
        "            if len(summary) > 50:  # Ensure the summary is not too short\n",
        "                return summary\n",
        "            else:\n",
        "                logger.warning(\"Generated summary is too short. Retrying...\")\n",
        "                attempt += 1\n",
        "                continue\n",
        "        except openai.error.RateLimitError as e:\n",
        "            # Handle rate limit error with retries and backoff\n",
        "            logger.error({\n",
        "                \"event\": \"RateLimitError in generate_summary_async\",\n",
        "                \"error\": str(e),\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            match = re.search(r\"Please try again in ([\\d\\.]+)s\", str(e))\n",
        "            wait_time = float(match.group(1)) if match else min(wait_time * 2, 60)\n",
        "            logger.info(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n",
        "            await asyncio.sleep(wait_time)\n",
        "            attempt += 1\n",
        "        except Exception as e:\n",
        "            # Catch unexpected errors and log them\n",
        "            logger.error({\n",
        "                \"event\": \"Unexpected error in generate_summary_async\",\n",
        "                \"error\": str(e),\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            return f\"Error occurred during summary generation: {str(e)}\"\n",
        "\n",
        "    # Log if all retries fail\n",
        "    logger.error(\"Max retries exceeded for generate_summary_async. Returning original content.\")\n",
        "    return \"No detailed summary could be generated for this content.\"\n",
        "\n",
        "\n",
        "# Handle rate limit error\n",
        "async def generate_summary_async(content, max_retries=5):\n",
        "    \"\"\"\n",
        "    Generates a summary of the content using OpenAI's GPT with retry logic.\n",
        "\n",
        "    Parameters:\n",
        "        content (str): The content to summarize.\n",
        "        max_retries (int): Maximum number of retries upon failure.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated summary or a fallback message.\n",
        "    \"\"\"\n",
        "    attempt = 0\n",
        "    wait_time = 1  # Initial wait time in seconds\n",
        "\n",
        "    while attempt < max_retries:\n",
        "        try:\n",
        "            summary_prompt = (\n",
        "                \"Generate a detailed and comprehensive summary of the following text. \"\n",
        "                \"Make sure to include key points, context, and any relevant details:\\n\\n\"\n",
        "                f\"{content}\\n\\nSummary:\"\n",
        "            )\n",
        "\n",
        "            # Use OpenAI API to generate the summary\n",
        "            async with openai_semaphore:\n",
        "                response = await openai.ChatCompletion.acreate(\n",
        "                    model=\"gpt-4\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes text.\"},\n",
        "                        {\"role\": \"user\", \"content\": summary_prompt}\n",
        "                    ],\n",
        "                    max_tokens=500,  # Adjust for summary length\n",
        "                    temperature=0.5,\n",
        "                )\n",
        "            summary = response.choices[0].message['content'].strip()\n",
        "\n",
        "            # Return a meaningful summary\n",
        "            if len(summary) > 50:  # Ensure the summary is not too short\n",
        "                return summary\n",
        "            else:\n",
        "                logger.warning(\"Generated summary is too short. Retrying...\")\n",
        "                attempt += 1\n",
        "                continue\n",
        "        except openai.error.RateLimitError as e:\n",
        "            # Handle rate limit error with retries and backoff\n",
        "            logger.error({\n",
        "                \"event\": \"RateLimitError in generate_summary_async\",\n",
        "                \"error\": str(e),\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            match = re.search(r\"Please try again in ([\\d\\.]+)s\", str(e))\n",
        "            wait_time = float(match.group(1)) if match else min(wait_time * 2, 60)\n",
        "            logger.info(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n",
        "            await asyncio.sleep(wait_time)\n",
        "            attempt += 1\n",
        "        except Exception as e:\n",
        "            # Catch unexpected errors and log them\n",
        "            logger.error({\n",
        "                \"event\": \"Unexpected error in generate_summary_async\",\n",
        "                \"error\": str(e),\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            return f\"Error occurred during summary generation: {str(e)}\"\n",
        "\n",
        "    # Log if all retries fail\n",
        "    logger.error(\"Max retries exceeded for generate_summary_async. Returning original content.\")\n",
        "    return \"No detailed summary could be generated for this content.\"\n",
        "\n",
        "# Use OpenAI API to generate the summary\n",
        "async def generate_summary_async(content, max_retries=5):\n",
        "    \"\"\"\n",
        "    Generates a summary of the content using OpenAI's GPT with retry logic.\n",
        "\n",
        "    Parameters:\n",
        "        content (str): The content to summarize.\n",
        "        max_retries (int): Maximum number of retries upon failure.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated summary or a fallback message.\n",
        "    \"\"\"\n",
        "    attempt = 0\n",
        "    wait_time = 1  # Initial wait time in seconds\n",
        "\n",
        "    while attempt < max_retries:\n",
        "        try:\n",
        "            # Construct the prompt dynamically\n",
        "            summary_prompt = (\n",
        "                \"Generate a detailed and comprehensive summary of the following text. \"\n",
        "                \"Make sure to include key points, context, and any relevant details:\\n\\n\"\n",
        "                f\"{content}\\n\\nSummary:\"\n",
        "            )\n",
        "\n",
        "            # Use OpenAI API to generate the summary\n",
        "            async with openai_semaphore:\n",
        "                response = await openai.ChatCompletion.acreate(\n",
        "                    model=\"gpt-4\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes text.\"},\n",
        "                        {\"role\": \"user\", \"content\": summary_prompt}\n",
        "                    ],\n",
        "                    max_tokens=500,  # Adjust for summary length\n",
        "                    temperature=0.5,\n",
        "                )\n",
        "            summary = response.choices[0].message['content'].strip()\n",
        "\n",
        "            # Return a meaningful summary\n",
        "            if len(summary) > 50:  # Ensure the summary is not too short\n",
        "                return summary\n",
        "            else:\n",
        "                logger.warning(\"Generated summary is too short. Retrying...\")\n",
        "                attempt += 1\n",
        "                continue\n",
        "        except openai.error.RateLimitError as e:\n",
        "            # Handle rate limit error with retries and backoff\n",
        "            logger.error({\n",
        "                \"event\": \"RateLimitError in generate_summary_async\",\n",
        "                \"error\": str(e),\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            match = re.search(r\"Please try again in ([\\d\\.]+)s\", str(e))\n",
        "            wait_time = float(match.group(1)) if match else min(wait_time * 2, 60)\n",
        "            logger.info(f\"Rate limit exceeded. Retrying in {wait_time} seconds...\")\n",
        "            await asyncio.sleep(wait_time)\n",
        "            attempt += 1\n",
        "        except Exception as e:\n",
        "            # Catch unexpected errors and log them\n",
        "            logger.error({\n",
        "                \"event\": \"Unexpected error in generate_summary_async\",\n",
        "                \"error\": str(e),\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            return f\"Error occurred during summary generation: {str(e)}\"\n",
        "\n",
        "    # Log if all retries fail\n",
        "    logger.error(\"Max retries exceeded for generate_summary_async. Returning original content.\")\n",
        "    return \"No detailed summary could be generated for this content.\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    \"\"\"\n",
        "    Analyzes the sentiment of the given text using NLTK's VADER.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to analyze.\n",
        "\n",
        "    Returns:\n",
        "        str: 'Positive', 'Neutral', or 'Negative'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not text:\n",
        "            return 'Neutral'\n",
        "        text = str(text)  # Ensure text is a string\n",
        "        scores = sid.polarity_scores(text)\n",
        "        compound_score = scores['compound']\n",
        "        if compound_score >= 0.05:\n",
        "            return 'Positive'\n",
        "        elif compound_score <= -0.05:\n",
        "            return 'Negative'\n",
        "        else:\n",
        "            return 'Neutral'\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in analyze_sentiment: {e}\", exc_info=True)\n",
        "        return 'Neutral'\n",
        "\n",
        "def map_topic_to_trends_query(topic_title):\n",
        "    \"\"\"\n",
        "    Maps a topic title to a Google Trends query.\n",
        "\n",
        "    Parameters:\n",
        "        topic_title (str): The title of the topic.\n",
        "\n",
        "    Returns:\n",
        "        str: The mapped query.\n",
        "    \"\"\"\n",
        "    # Placeholder for mapping logic\n",
        "    return topic_title\n",
        "\n",
        "def extract_keywords(topic_title):\n",
        "    \"\"\"\n",
        "    Extracts keywords from the topic title using RAKE.\n",
        "\n",
        "    Parameters:\n",
        "        topic_title (str): The title of the topic.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of extracted keywords.\n",
        "    \"\"\"\n",
        "    rake.extract_keywords_from_text(topic_title)\n",
        "    return rake.get_ranked_phrases()\n",
        "\n",
        "def fetch_newsapi_articles(topic, page_size=5):\n",
        "    \"\"\"\n",
        "    Fetches articles from NewsAPI for a given topic.\n",
        "\n",
        "    Parameters:\n",
        "        topic (str): The topic to search articles for.\n",
        "        page_size (int): Number of articles to fetch.\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: A list of articles.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use the initialized NewsApiClient\n",
        "        all_articles = newsapi.get_everything(q=topic,\n",
        "                                              language='en',\n",
        "                                              sort_by='relevancy',\n",
        "                                              page_size=page_size)\n",
        "        if all_articles.get('status') != 'ok':\n",
        "            raise Exception(f\"NewsAPI Error: {all_articles.get('message', 'Unknown error')}\")\n",
        "        articles = all_articles['articles']\n",
        "        return articles\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching articles from NewsAPI: {e}\", exc_info=True)\n",
        "        return []\n",
        "\n",
        "# ----------------------------\n",
        "# Trend Data Classes\n",
        "# ----------------------------\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional\n",
        "\n",
        "@dataclass\n",
        "class GoogleTrend:\n",
        "    topic: str\n",
        "    interest: float\n",
        "    sentiment: str\n",
        "\n",
        "@dataclass\n",
        "class Trend:\n",
        "    title: str\n",
        "    description: str\n",
        "    source: str\n",
        "    approx_traffic: str\n",
        "    sentiment: str\n",
        "    google_trend: Optional[GoogleTrend] = None\n",
        "\n",
        "@dataclass\n",
        "class ScriptOptions:\n",
        "    style: str = \"Normal\"\n",
        "    tone: str = \"Informative\"\n",
        "    length: str = \"60 seconds\"\n",
        "\n",
        "# ----------------------------\n",
        "# Fetch Trending Topics Function\n",
        "# ----------------------------\n",
        "\n",
        "\n",
        "async def fetch_full_article_content(url):\n",
        "    \"\"\"\n",
        "    Fetches the full article content from the given URL.\n",
        "\n",
        "    Parameters:\n",
        "        url (str): The URL of the article.\n",
        "\n",
        "    Returns:\n",
        "        str: The full article content or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        article = Article(url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        if not article.text or len(article.text) < 100:  # Minimum content check\n",
        "            raise ValueError(f\"Insufficient content in article at {url}\")\n",
        "        return article.text\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching full article content from {url}: {e}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "\n",
        "async def fetch_trending_topics_rss_async(geo='US', limit=10):\n",
        "    \"\"\"\n",
        "    Asynchronously fetches trending topics from multiple RSS feeds.\n",
        "    \"\"\"\n",
        "    rss_feeds = [\n",
        "        f\"https://trends.google.com/trends/trendingsearches/daily/rss?geo={geo}\",\n",
        "        f\"https://news.google.com/rss?hl=en-{geo}&gl={geo}&ceid={geo}:en\",\n",
        "        # Additional RSS feeds\n",
        "        \"http://rss.cnn.com/rss/edition.rss\",\n",
        "        \"http://news.yahoo.com/rss/\",\n",
        "        \"https://www.theguardian.com/uk/rss\",\n",
        "        \"https://news.un.org/feed/subscribe/en/news/all/rss.xml\"\n",
        "        # Add more feeds as needed\n",
        "    ]\n",
        "\n",
        "    trending_topics = []  # Initialize the list to store trending topics\n",
        "\n",
        "    tasks = []\n",
        "    for rss_url in rss_feeds:\n",
        "        tasks.append(fetch_rss_feed_async(rss_url, limit))\n",
        "\n",
        "    results = await asyncio.gather(*tasks)\n",
        "\n",
        "    for rss_url, result in zip(rss_feeds, results):\n",
        "        if not result:\n",
        "            continue\n",
        "        for entry in result:\n",
        "            # Process each entry\n",
        "            title = entry.get('title', 'No Title')\n",
        "            link = entry.get('link', '')\n",
        "            summary = entry.get('summary') or entry.get('description') or entry.get('title') or 'No description available.'\n",
        "            source = extract_source(link)\n",
        "            approx_traffic = 'N/A'\n",
        "\n",
        "            trending_topics.append({\n",
        "                'title': title,\n",
        "                'description': summary,\n",
        "                'source': source,\n",
        "                'approx_traffic': approx_traffic,\n",
        "                'sentiment': 'Neutral',  # Placeholder; actual sentiment will be determined later\n",
        "                'published': datetime.now(timezone.utc)\n",
        "            })\n",
        "    return trending_topics  # Correctly indented outside the loop\n",
        "\n",
        "async def fetch_rss_feed_async(rss_url, limit):\n",
        "    \"\"\"\n",
        "    Asynchronously fetches and parses an RSS feed.\n",
        "\n",
        "    Parameters:\n",
        "        rss_url (str): The URL of the RSS feed.\n",
        "        limit (int): Number of entries to fetch.\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: A list of RSS feed entries.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
        "    }\n",
        "    try:\n",
        "        async with aiohttp.ClientSession(headers=headers) as session:\n",
        "            async with session.get(rss_url, timeout=10) as response:\n",
        "                if response.status != 200:\n",
        "                    logger.error(f\"Failed to fetch RSS feed from {rss_url}. Status code: {response.status}\")\n",
        "                    return []\n",
        "                content = await response.text()\n",
        "                feed = feedparser.parse(content)\n",
        "                entries = feed.entries[:limit]\n",
        "                return entries\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching RSS feed from {rss_url}: {e}\", exc_info=True)\n",
        "        return []\n",
        "\n",
        "# ----------------------------\n",
        "# Aggregate Trends Data Function\n",
        "# ----------------------------\n",
        "\n",
        "async def aggregate_trends_data(rss_trends, selected_timeframe_hours):\n",
        "    \"\"\"\n",
        "    Aggregates data for each trend by fetching NewsAPI articles and Google Trends data.\n",
        "    \"\"\"\n",
        "    aggregated_trends = []\n",
        "    now = datetime.now(timezone.utc)\n",
        "\n",
        "    for trend in rss_trends:\n",
        "        # Filter based on the selected time frame\n",
        "        time_diff = now - trend['published']\n",
        "        if time_diff.total_seconds() > selected_timeframe_hours * 3600:\n",
        "            continue  # Skip trends outside the time frame\n",
        "\n",
        "        topic_title = trend['title']\n",
        "        # Clean the description by stripping HTML\n",
        "        soup = BeautifulSoup(trend['description'], 'html.parser')\n",
        "        clean_description = soup.get_text(separator=' ', strip=True)\n",
        "        # Optionally, limit the description length\n",
        "        clean_description = (clean_description[:200] + '...') if len(clean_description) > 200 else clean_description\n",
        "\n",
        "        # Fetch NewsAPI articles (if needed)\n",
        "        # newsapi_articles = fetch_newsapi_articles(topic_title, page_size=5)\n",
        "        # Process articles as needed (e.g., summarize, analyze sentiment)\n",
        "\n",
        "        # Fetch Google Trends data\n",
        "        google_trend_data = fetch_google_trends_cached(topic_title, timeframe='now 7-d')\n",
        "        if google_trend_data:\n",
        "            google_trend = GoogleTrend(**google_trend_data)\n",
        "            approx_traffic = str(google_trend.interest)\n",
        "        else:\n",
        "            google_trend = None\n",
        "            approx_traffic = 'Unavailable'\n",
        "\n",
        "        # Generate summary asynchronously if description is available\n",
        "        if clean_description and clean_description != 'No description available.':\n",
        "          summary = await generate_summary_async(clean_description)\n",
        "        else:\n",
        "            summary = clean_description  # Use existing description\n",
        "\n",
        "\n",
        "        sentiment = analyze_sentiment(summary)\n",
        "\n",
        "        aggregated_trends.append(Trend(\n",
        "            title=topic_title,\n",
        "            description=summary,\n",
        "            source=trend['source'],\n",
        "            approx_traffic=approx_traffic,\n",
        "            sentiment=sentiment,\n",
        "            google_trend=google_trend\n",
        "        ))\n",
        "\n",
        "        if len(aggregated_trends) >= 10:\n",
        "            break  # Limit to 10 trends\n",
        "\n",
        "    return aggregated_trends\n",
        "\n",
        "# ----------------------------\n",
        "# Define fetch_and_aggregate_trending_data Function\n",
        "# ----------------------------\n",
        "\n",
        "async def fetch_and_aggregate_trending_data(country_code, timeframe_hours):\n",
        "    \"\"\"\n",
        "    Fetches trending topics and aggregates data.\n",
        "\n",
        "    Parameters:\n",
        "        country_code (str): The country code for fetching trends.\n",
        "        timeframe_hours (int): The timeframe in hours to filter trends.\n",
        "\n",
        "    Returns:\n",
        "        List[Trend]: A list of aggregated Trend objects.\n",
        "    \"\"\"\n",
        "    rss_trends = await fetch_trending_topics_rss_async(geo=country_code, limit=20)\n",
        "    aggregated_trends = await aggregate_trends_data(rss_trends, timeframe_hours)\n",
        "    return aggregated_trends\n",
        "\n",
        "# ----------------------------\n",
        "# Generate and Display Script Function\n",
        "# ----------------------------\n",
        "\n",
        "async def generate_and_display_script_async(selected_topic_data: Trend):\n",
        "    \"\"\"\n",
        "    Prompts the user for script customization options, generates the script,\n",
        "    and displays it.\n",
        "\n",
        "    Parameters:\n",
        "        selected_topic_data (Trend): The data associated with the selected topic.\n",
        "    \"\"\"\n",
        "    print(\"\\nYou can customize the script generation. If you wish to skip any option, just press Enter.\")\n",
        "\n",
        "    # Select script style\n",
        "    style_input = input(\"Select script style (Flashy / Expressive / Normal): \").strip()\n",
        "    style = style_input if style_input else \"Normal\"\n",
        "\n",
        "    # Select script tone\n",
        "    tone_input = input(\"Select script tone (e.g., Informative, Persuasive, Emotional): \").strip()\n",
        "    tone = tone_input if tone_input else \"Informative\"\n",
        "\n",
        "    # Enter script length\n",
        "    length_input = input(\"Enter script length in seconds (e.g., 60, 120): \").strip()\n",
        "    length = f\"{length_input} seconds\" if length_input.isdigit() else \"60 seconds\"\n",
        "\n",
        "    # Create ScriptOptions instance\n",
        "    options = ScriptOptions(\n",
        "        style=style,\n",
        "        tone=tone,\n",
        "        length=length\n",
        "    )\n",
        "\n",
        "    print(\"\\nSelected Options - Style: {0}, Tone: {1}, Length: {2}\".format(options.style, options.tone, options.length))\n",
        "\n",
        "    print(\"\\nGenerating script for the selected topic...\")\n",
        "    try:\n",
        "        script = await generate_script_for_topic_async(selected_topic_data.title, selected_topic_data, options=options)\n",
        "        print(\"Script generated successfully.\")\n",
        "    except Exception as e:\n",
        "        logger.error({\n",
        "            \"error\": str(e),\n",
        "            \"event\": \"generate_script_error\",\n",
        "            \"timestamp\": datetime.utcnow().isoformat()\n",
        "        })\n",
        "        script = \"Failed to generate script.\"\n",
        "\n",
        "    # Display the script\n",
        "    print(f\"\\n### Generated Script for '{selected_topic_data.title}':\\n\")\n",
        "    print(script)\n",
        "    print(\"\\n**Source:**\", selected_topic_data.source)\n",
        "    print(\"\\n====\\n\")\n",
        "\n",
        "    print(\"Script generation completed.\")\n",
        "\n",
        "async def generate_script_for_topic_async(title, trend_data, options: ScriptOptions):\n",
        "    \"\"\"\n",
        "    Generates a script based on the topic and options using OpenAI's GPT.\n",
        "\n",
        "    Parameters:\n",
        "        title (str): The title of the topic.\n",
        "        trend_data (Trend): The trend data.\n",
        "        options (ScriptOptions): User-selected script options.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated script.\n",
        "    \"\"\"\n",
        "    # Calculate an approximate max token count based on script length\n",
        "    words_per_minute = 150\n",
        "    desired_word_count = int(words_per_minute * (int(options.length.split()[0]) / 60))\n",
        "    max_tokens = int(desired_word_count * 1.5)  # GPT uses tokens, and 1 word ≈ 1.5 tokens\n",
        "\n",
        "    # Construct the prompt for OpenAI\n",
        "    prompt = (\n",
        "        f\"Generate a {options.length} script for the topic '{title}' with a {options.tone} tone \"\n",
        "        f\"and a {options.style} style. The script should include:\\n\"\n",
        "        f\"- Key details: {trend_data.description}\\n\"\n",
        "        f\"- Sentiment: {trend_data.sentiment}\\n\"\n",
        "        f\"- Traffic: {trend_data.approx_traffic}\\n\\n\"\n",
        "        f\"Use additional knowledge about '{title}' from related articles or general information. \"\n",
        "        f\"Ensure the script is engaging and suitable for a {options.length} duration.\"\n",
        "    )\n",
        "\n",
        "    attempt = 0\n",
        "    max_retries = 5\n",
        "    wait_time = 1  # Initial wait time in seconds\n",
        "\n",
        "    while attempt < max_retries:\n",
        "        try:\n",
        "            async with google_trends_semaphore:\n",
        "                response = await openai.ChatCompletion.acreate(\n",
        "                    model=\"gpt-4\",\n",
        "                    messages=[\n",
        "                        {\n",
        "                            \"role\": \"system\",\n",
        "                            \"content\": \"You are a creative scriptwriter that generates scripts based on provided data.\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": prompt\n",
        "                        }\n",
        "                    ],\n",
        "                    max_tokens=max_tokens,  # Dynamically set max tokens\n",
        "                    temperature=0.7,\n",
        "                )\n",
        "            script = response.choices[0].message['content'].strip()\n",
        "            return script\n",
        "        except openai.error.RateLimitError as e:\n",
        "            # Handle rate limit error\n",
        "            logger.error({\n",
        "                \"event\": \"RateLimitError in generate_script_for_topic_async\",\n",
        "                \"error\": str(e),\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            match = re.search(r\"Please try again in ([\\d\\.]+)s\", str(e))\n",
        "            if match:\n",
        "                wait_time = float(match.group(1))\n",
        "            else:\n",
        "                wait_time = min(wait_time * 2, 60)  # Exponential backoff\n",
        "            logger.info(f\"Rate limit exceeded. Waiting for {wait_time} seconds before retrying...\")\n",
        "            await asyncio.sleep(wait_time)\n",
        "            attempt += 1\n",
        "        except Exception as e:\n",
        "            logger.error({\n",
        "                \"event\": \"Error generating script_async\",\n",
        "                \"error\": str(e),\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            return \"Failed to generate script due to an unexpected error.\"\n",
        "\n",
        "    logger.error(\"Max retries exceeded for generate_script_for_topic_async. Returning failure message.\")\n",
        "    return \"Failed to generate script after multiple attempts due to rate limits.\"\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Main Function\n",
        "# ----------------------------\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to execute the script workflow.\n",
        "    \"\"\"\n",
        "    async def run():\n",
        "        # Function to display a changing message every 5 seconds with an additional note\n",
        "        def flashing_message(stop_event):\n",
        "            messages = [\n",
        "                \"🔍 Gathering the latest trends... (This could take up to 2 minutes. Please wait.)\",\n",
        "                \"⏳ Processing data, please wait... (This could take up to 2 minutes. Please wait.)\",\n",
        "                \"✨ Almost there, thank you for your patience! (This could take up to 2 minutes. Please wait.)\"\n",
        "            ]\n",
        "            idx = 0\n",
        "            while not stop_event.is_set():\n",
        "                message = messages[idx % len(messages)]\n",
        "                print(f\"\\r{message}   \", end='', flush=True)\n",
        "                for _ in range(5):\n",
        "                    if stop_event.is_set():\n",
        "                        break\n",
        "                    time.sleep(1)\n",
        "                idx += 1\n",
        "                print('\\r' + ' ' * len(message) + '   ', end='', flush=True)\n",
        "\n",
        "        while True:\n",
        "            print(\"Enter the country for trending topics data (e.g., United States or US):\")\n",
        "            country_input = input(\"Country: \").strip()\n",
        "            matching_country = get_matching_country(country_input, available_countries)\n",
        "            if not matching_country:\n",
        "                print(\"No matching countries found. Please try again.\")\n",
        "                # Optionally, display available countries\n",
        "                print(\"Available countries are:\")\n",
        "                for country in available_countries.keys():\n",
        "                    print(f\"- {country}\")\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        selected_country = matching_country\n",
        "        # Retrieve all possible codes for the selected country\n",
        "        selected_country_codes = available_countries[selected_country]\n",
        "        selected_country_code = selected_country_codes[0]  # Use the first code by default\n",
        "        print(f\"\\nYou selected: {selected_country}\")\n",
        "\n",
        "        print(\"\\nSelect the time frame for trending topics:\")\n",
        "        print(\"A. Last 4 hours\")\n",
        "        print(\"B. Last 24 hours\")\n",
        "        print(\"C. Last 7 days\")\n",
        "        time_range_selection = input(\"Enter the letter of the time frame you're interested in: \").strip().upper()\n",
        "        time_range_mapping = {'A': 4, 'B': 24, 'C': 168}\n",
        "        selected_timeframe_hours = time_range_mapping.get(time_range_selection)\n",
        "        if not selected_timeframe_hours:\n",
        "            print(\"Invalid selection. Exiting.\")\n",
        "            return\n",
        "\n",
        "        # Start flashing message while fetching all data\n",
        "        stop_event = threading.Event()\n",
        "        thread = threading.Thread(target=flashing_message, args=(stop_event,))\n",
        "        thread.start()\n",
        "\n",
        "        # Fetch all data asynchronously with robust error handling\n",
        "        try:\n",
        "            # Run the async fetch_and_aggregate_trending_data function\n",
        "            aggregated_trends = await fetch_and_aggregate_trending_data(selected_country_code, selected_timeframe_hours)\n",
        "        except Exception as e:\n",
        "            logger.error({\n",
        "                \"error\": str(e),\n",
        "                \"event\": \"fetch_data_error\",\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            aggregated_trends = []\n",
        "\n",
        "        # Stop flashing message\n",
        "        stop_event.set()\n",
        "        thread.join()\n",
        "        print()  # Move to the next line after flashing message\n",
        "\n",
        "        if not aggregated_trends:\n",
        "            logger.error({\n",
        "                \"event\": \"no_trending_topics_found\",\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            print(\"No trending topics found.\")\n",
        "            return\n",
        "\n",
        "        # Display trending topics with pagination\n",
        "        # Pagination variables\n",
        "        batch_size = 10\n",
        "        total_trends = len(aggregated_trends)\n",
        "        current_index = 0\n",
        "\n",
        "        while current_index < total_trends:\n",
        "            # Determine the end index for the current batch\n",
        "            end_index = min(current_index + batch_size, total_trends)\n",
        "            batch_trends = aggregated_trends[current_index:end_index]\n",
        "\n",
        "            # Display the consolidated list with sentiments\n",
        "            print(f\"\\nCurrently Trending in {selected_country} in the last {selected_timeframe_hours} hours (Showing {current_index + 1} to {end_index} of {total_trends}):\\n\")\n",
        "            table = PrettyTable()\n",
        "            table.field_names = [\"No.\", \"Topic\", \"Description\", \"Source\", \"Approx Traffic\", \"Sentiment\"]\n",
        "            table.hrules = HRuleStyle.ALL  # Use HRuleStyle.ALL\n",
        "            table.max_width = 40  # Suitable for phone screens\n",
        "            for idx, trend in enumerate(batch_trends, start=current_index + 1):\n",
        "                title = textwrap.fill(trend.title, width=40)\n",
        "                description = textwrap.fill(trend.description, width=40)\n",
        "                source = trend.source\n",
        "                approx_traffic = trend.approx_traffic\n",
        "                sentiment = trend.sentiment\n",
        "                table.add_row([idx, title, description, source, approx_traffic, sentiment])\n",
        "            print(table)\n",
        "\n",
        "            # Prepare to handle user input for pagination or selection\n",
        "            while True:\n",
        "                user_input = input(\"Type 'more' or '+' to view more results, enter the number of the topic to select it, or any other key to exit: \").strip().lower()\n",
        "                if user_input in ['more', '+']:\n",
        "                    current_index = end_index\n",
        "                    break  # Continue to the next batch\n",
        "                elif user_input.isdigit():\n",
        "                    selected_idx = int(user_input)\n",
        "                    if 1 <= selected_idx <= total_trends:\n",
        "                        selected_topic_data = aggregated_trends[selected_idx - 1]\n",
        "                        selected_topic = selected_topic_data.title\n",
        "                        # Apply text wrapping to the selected topic message\n",
        "                        selected_topic_display = textwrap.fill(f\"You selected: {selected_topic}\", width=40)\n",
        "                        print(f\"\\n{selected_topic_display}\")\n",
        "                        # Generate scripts for the selected topic\n",
        "                        await generate_and_display_script_async(selected_topic_data)\n",
        "                        return  # Exit after script generation\n",
        "                    else:\n",
        "                        print(\"Invalid selection. Please enter a valid topic number.\")\n",
        "                else:\n",
        "                    print(\"Exiting the script.\")\n",
        "                    return\n",
        "\n",
        "        print(\"====\\nNo more trending topics available.\\n\")\n",
        "\n",
        "        # After all batches are displayed, prompt the user to select a topic\n",
        "        while True:\n",
        "            try:\n",
        "                selected_idx = input(\"Enter the number of the topic you're interested in (or type 0 to exit): \").strip()\n",
        "                if selected_idx == '0':\n",
        "                    print(\"Exiting the script.\")\n",
        "                    return\n",
        "                if selected_idx.isdigit():\n",
        "                    selected_idx = int(selected_idx)\n",
        "                    if 1 <= selected_idx <= total_trends:\n",
        "                        selected_topic_data = aggregated_trends[selected_idx - 1]\n",
        "                        selected_topic = selected_topic_data.title\n",
        "                        # Apply text wrapping to the selected topic message\n",
        "                        selected_topic_display = textwrap.fill(f\"You selected: {selected_topic}\", width=40)\n",
        "                        print(f\"\\n{selected_topic_display}\")\n",
        "                        # Generate scripts for the selected topic\n",
        "                        await generate_and_display_script_async(selected_topic_data)\n",
        "                        return  # Exit after script generation\n",
        "                    else:\n",
        "                        print(\"Invalid selection. Please enter a valid topic number.\")\n",
        "                else:\n",
        "                    print(\"Invalid input. Please enter a number.\")\n",
        "            except ValueError:\n",
        "                print(\"Invalid input. Please enter a valid number.\")\n",
        "\n",
        "    asyncio.run(run())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1s-3u0czUndpxCkgirEnqndEDGpEPjpJ_",
      "authorship_tag": "ABX9TyOw1/hsXdZOLTDl5BoaR+Fr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}