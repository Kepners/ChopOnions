{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1s-3u0czUndpxCkgirEnqndEDGpEPjpJ_",
      "authorship_tag": "ABX9TyM5qpT8GbuT+8u4Ice4OR4s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kepners/ChopOnions/blob/main/ChoppingOnionS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Install Required Libraries\n",
        "\n",
        "!pip install --quiet openai==0.27.8 praw==7.8.1 feedparser==6.0.10 textblob==0.17.1 \\\n",
        "requests cachetools==5.3.1 prettytable==3.7.0 python-dotenv==0.21.0 ratelimit==2.2.1 \\\n",
        "nltk==3.8.1 fuzzywuzzy==0.18.0 python-Levenshtein==0.12.2\n"
      ],
      "metadata": {
        "id": "MAUVB0guTX2b",
        "collapsed": true
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 2: Mount Google Drive and Load Environment Variables\n",
        "from google.colab import drive\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to your .env file in Google Drive\n",
        "dotenv_path = '/content/drive/MyDrive/Secrets/.env'\n",
        "\n",
        "# Load the environment variables from the .env file\n",
        "load_dotenv(dotenv_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIud78FOZR3M",
        "outputId": "c1132acc-6af7-45fe-e66d-6972cffc3885"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 3: Configure Logging\n",
        "import logging\n",
        "import os\n",
        "\n",
        "# Configure Logging to write to a log file and suppress console output\n",
        "log_file = 'app.log'\n",
        "logging.basicConfig(\n",
        "    level=logging.ERROR,  # Only log ERROR and CRITICAL\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(log_file),\n",
        "        logging.StreamHandler(open(os.devnull, 'w'))  # Suppress console output\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "ujJviMETtuLY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 4: Download NLTK Data Silently\n",
        "import nltk\n",
        "import sys\n",
        "from contextlib import contextmanager\n",
        "import os\n",
        "\n",
        "@contextmanager\n",
        "def suppress_stdout():\n",
        "    with open(os.devnull, \"w\") as devnull:\n",
        "        old_stdout = sys.stdout\n",
        "        sys.stdout = devnull\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n",
        "\n",
        "with suppress_stdout():\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    nltk.download('brown')\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('movie_reviews')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGPK1IRBXfBm",
        "outputId": "ddff2292-795c-4c1f-9f57-d8dd37761211"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 5: Create Utils Directory\n",
        "import os\n",
        "\n",
        "utils_dir = 'utils'\n",
        "if not os.path.exists(utils_dir):\n",
        "    os.makedirs(utils_dir)\n"
      ],
      "metadata": {
        "id": "RPd0MOuGTaTM",
        "collapsed": true
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 6: Create data_processing.py\n",
        "\n",
        "data_processing_code = \"\"\"\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from textblob import TextBlob\n",
        "\n",
        "def extract_keywords(text, max_keywords=10):\n",
        "    \\\"\"\"\n",
        "    Extracts up to `max_keywords` nouns from the input `text`.\n",
        "    \\\"\"\"\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords and non-alphabetic tokens\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "    # Get part-of-speech tags\n",
        "    tagged_words = pos_tag(filtered_words)\n",
        "    # Keep nouns and proper nouns\n",
        "    keywords = [word for word, pos in tagged_words if pos.startswith('NN')]\n",
        "    # Limit the number of keywords\n",
        "    return keywords[:max_keywords]\n",
        "\n",
        "def analyze_sentiment(visit_count, url_count):\n",
        "    \\\"\"\"\n",
        "    Analyzes the sentiment based on visit_count and url_count.\n",
        "    Returns 'Positive', 'Negative', or 'Neutral'.\n",
        "    \\\"\"\"\n",
        "    # Simple heuristic: higher counts imply higher sentiment\n",
        "    if visit_count > 1000 and url_count > 50:\n",
        "        return 'Positive'\n",
        "    elif visit_count < 100 and url_count < 10:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join('utils', 'data_processing.py'), 'w') as file:\n",
        "    file.write(data_processing_code)\n"
      ],
      "metadata": {
        "id": "bNicEhLDr06j"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 7: Initialize API Clients and Configure Caches\n",
        "\n",
        "import sys\n",
        "sys.path.append('utils')  # Add the utils directory to the system path\n",
        "\n",
        "import praw\n",
        "import openai\n",
        "import logging\n",
        "import warnings\n",
        "from utils.data_processing import extract_keywords, analyze_sentiment\n",
        "from cachetools import TTLCache, cached\n",
        "import feedparser\n",
        "import requests\n",
        "import time\n",
        "import os\n",
        "from fuzzywuzzy import process\n",
        "\n",
        "# Suppress PRAW warnings about asynchronous environments\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='praw')\n",
        "logging.getLogger('praw').setLevel(logging.CRITICAL)\n",
        "\n",
        "# Initialize Reddit API using PRAW\n",
        "try:\n",
        "    reddit = praw.Reddit(\n",
        "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
        "        client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
        "        user_agent=os.getenv('REDDIT_USER_AGENT', 'script:TrendingTopicsScript:1.0 (by u/yourusername)')\n",
        "    )\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error initializing Reddit API: {e}\")\n",
        "\n",
        "# Initialize OpenAI API\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Initialize SerpAPI API key\n",
        "SERPAPI_API_KEY = os.getenv('SERPAPI_API_KEY')\n",
        "\n",
        "# Initialize NewsAPI.org API key\n",
        "NEWSAPI_API_KEY = os.getenv('NEWSAPI_API_KEY')\n",
        "\n",
        "# Define a list of countries with their codes, including common abbreviations\n",
        "available_countries = {\n",
        "    'United States': 'US',\n",
        "    'US': 'US',\n",
        "    'USA': 'US',\n",
        "    'India': 'IN',\n",
        "    'Canada': 'CA',\n",
        "    'United Kingdom': 'GB',\n",
        "    'UK': 'GB',\n",
        "    'Great Britain': 'GB',\n",
        "    'Australia': 'AU',\n",
        "    'Germany': 'DE',\n",
        "    'France': 'FR',\n",
        "    'Brazil': 'BR',\n",
        "    'Mexico': 'MX',\n",
        "    'Japan': 'JP',\n",
        "    'Russia': 'RU',\n",
        "    'South Korea': 'KR',\n",
        "    'Korea': 'KR',\n",
        "    'Italy': 'IT',\n",
        "    'Spain': 'ES',\n",
        "    'Netherlands': 'NL',\n",
        "    'Sweden': 'SE',\n",
        "    'Switzerland': 'CH',\n",
        "    'Austria': 'AT',\n",
        "    'Belgium': 'BE',\n",
        "    'New Zealand': 'NZ'\n",
        "}\n",
        "\n",
        "# Initialize caches\n",
        "post_relevance_cache = TTLCache(maxsize=1000, ttl=3600)  # Cache for 1 hour\n",
        "trend_reason_cache = TTLCache(maxsize=1000, ttl=86400)   # Cache for 24 hours\n"
      ],
      "metadata": {
        "id": "C5RZdBozr4hJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 8: Define Supporting Functions\n",
        "\n",
        "import logging\n",
        "import re\n",
        "import textwrap\n",
        "import requests\n",
        "from cachetools import TTLCache, cached\n",
        "import feedparser\n",
        "import openai\n",
        "from fuzzywuzzy import process\n",
        "import praw\n",
        "import time\n",
        "import os\n",
        "from textblob import TextBlob  # Ensure TextBlob is installed\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "\n",
        "# Initialize OpenAI API\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Initialize Reddit API using PRAW\n",
        "reddit = praw.Reddit(\n",
        "    client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
        "    client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
        "    user_agent=os.getenv('REDDIT_USER_AGENT')\n",
        ")\n",
        "\n",
        "# Initialize NewsAPI.org API key\n",
        "NEWSAPI_API_KEY = os.getenv('NEWSAPI_API_KEY')\n",
        "\n",
        "# Initialize SerpAPI API key\n",
        "SERPAPI_API_KEY = os.getenv('SERPAPI_API_KEY')\n",
        "\n",
        "# Initialize caches\n",
        "post_relevance_cache = TTLCache(maxsize=1000, ttl=3600)  # Cache for 1 hour\n",
        "trend_reason_cache = TTLCache(maxsize=1000, ttl=86400)   # Cache for 24 hours\n",
        "\n",
        "# SerpAPI rate limits: adjust based on your plan\n",
        "MAX_CALLS_PER_MINUTE = 5  # Example limit\n",
        "ONE_MINUTE = 60\n",
        "\n",
        "@sleep_and_retry\n",
        "@limits(calls=MAX_CALLS_PER_MINUTE, period=ONE_MINUTE)\n",
        "def fetch_trending_topics_serpapi(country_code='US', limit=10):\n",
        "    \"\"\"\n",
        "    Fetches trending topics using SerpAPI's Google Trends API with rate limiting.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        params = {\n",
        "            \"engine\": \"google_trends\",\n",
        "            \"geo\": country_code,\n",
        "            \"api_key\": SERPAPI_API_KEY\n",
        "        }\n",
        "        url = \"https://serpapi.com/search\"\n",
        "        response = requests.get(url, params=params)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        trending_topics = []\n",
        "        for trend in data.get('trending_searches', {}).get('searches', [])[:limit]:\n",
        "            title = trend.get('title')\n",
        "            visit_count = trend.get('formatted_visit_count', '0')\n",
        "            url_count = trend.get('formatted_url_count', '0')\n",
        "            # Convert visit_count and url_count to integers\n",
        "            try:\n",
        "                visit_count = int(re.sub(r'[^\\d]', '', visit_count))\n",
        "                url_count = int(re.sub(r'[^\\d]', '', url_count))\n",
        "            except:\n",
        "                visit_count = 0\n",
        "                url_count = 0\n",
        "            sentiment = analyze_sentiment(visit_count, url_count)\n",
        "            trending_topics.append({\n",
        "                'title': title,\n",
        "                'description': generate_trend_reason(title),\n",
        "                'sentiment': sentiment\n",
        "            })\n",
        "        return trending_topics\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        logging.error(f\"HTTP error occurred while fetching SerpAPI Trends: {http_err}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching trending topics from SerpAPI: {e}\")\n",
        "        return []\n",
        "\n",
        "def fetch_trending_topics_google_rss(geo='US'):\n",
        "    \"\"\"\n",
        "    Fetches trending topics from Google Trends RSS feed.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        rss_url = f'https://trends.google.com/trends/trendingsearches/daily/rss?geo={geo}'\n",
        "        feed = feedparser.parse(rss_url)\n",
        "        trending_topics = []\n",
        "        for entry in feed.entries:\n",
        "            title = entry.title\n",
        "            # Optionally, extract visit_count and url_count if available in RSS\n",
        "            # Since RSS might not provide these, we'll default to 0\n",
        "            visit_count = 0\n",
        "            url_count = 0\n",
        "            sentiment = analyze_sentiment(visit_count, url_count)\n",
        "            trending_topics.append({\n",
        "                'title': title,\n",
        "                'description': generate_trend_reason(title),\n",
        "                'sentiment': sentiment\n",
        "            })\n",
        "        return trending_topics\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching trending topics from Google Trends RSS: {e}\")\n",
        "        return []\n",
        "\n",
        "def fetch_trending_topics_newsapi(country='us', limit=10):\n",
        "    \"\"\"\n",
        "    Fetches trending topics using NewsAPI.org top headlines.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        url = f'https://newsapi.org/v2/top-headlines?country={country}&pageSize={limit}&apiKey={NEWSAPI_API_KEY}'\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        trending_topics = []\n",
        "        for article in data.get('articles', [])[:limit]:\n",
        "            title = article.get('title')\n",
        "            description = article.get('description', '')\n",
        "            # For visit_count and url_count, we'll use the number of times the URL appears\n",
        "            # This is a proxy and may not be accurate\n",
        "            url = article.get('url', '')\n",
        "            url_count = 1  # Each URL is unique in this context\n",
        "            visit_count = 0  # NewsAPI doesn't provide visit counts\n",
        "            sentiment = analyze_sentiment(visit_count, url_count)\n",
        "            trending_topics.append({\n",
        "                'title': title,\n",
        "                'description': generate_trend_reason(title),\n",
        "                'sentiment': sentiment\n",
        "            })\n",
        "        return trending_topics\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        logging.error(f\"HTTP error occurred while fetching NewsAPI: {http_err}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching trending topics from NewsAPI: {e}\")\n",
        "        return []\n",
        "\n",
        "@cached(trend_reason_cache)\n",
        "def generate_trend_reason(topic, retries=3, backoff_factor=2):\n",
        "    \"\"\"\n",
        "    Generates a brief reason why a topic is trending using OpenAI's GPT-3.5-turbo.\n",
        "    \"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"You are an assistant that provides brief reasons why a topic is trending. Your responses should be concise, in brackets, and two to four words long.\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": f\"Why is '{topic}' trending? Provide a brief reason in brackets, two to four words long.\"\n",
        "                    }\n",
        "                ],\n",
        "                max_tokens=20,\n",
        "                temperature=0.5,\n",
        "            )\n",
        "            reason = response.choices[0].message.content.strip()\n",
        "            return reason\n",
        "        except openai.error.OpenAIError as e:\n",
        "            logging.error(f\"Attempt {attempt + 1} - Error generating trend reason: {e}\")\n",
        "            if attempt < retries - 1:\n",
        "                wait_time = backoff_factor ** attempt\n",
        "                logging.info(f\"Retrying in {wait_time} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                logging.error(\"Max retries reached. Using default reason.\")\n",
        "                return \"[Trending Topic]\"\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Unexpected error generating trend reason: {e}\")\n",
        "            return \"[Trending Topic]\"\n",
        "\n",
        "def fetch_related_news(topic, limit=5):\n",
        "    \"\"\"\n",
        "    Fetches related news articles using NewsAPI.org.\n",
        "    Includes sentiment analysis for each article.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        url = f'https://newsapi.org/v2/everything?q={requests.utils.quote(topic)}&sortBy=publishedAt&pageSize={limit}&apiKey={NEWSAPI_API_KEY}'\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        related_stories = []\n",
        "        for article in data.get('articles', [])[:limit]:\n",
        "            title = article.get('title', '')\n",
        "            description = article.get('description', '')\n",
        "            url = article.get('url', '')\n",
        "            # For visit_count and url_count, we'll use the number of times the URL appears\n",
        "            # This is a proxy and may not be accurate\n",
        "            url_count = 1  # Each URL is unique in this context\n",
        "            visit_count = 0  # NewsAPI doesn't provide visit counts\n",
        "            sentiment = analyze_sentiment(visit_count, url_count)\n",
        "            # Extract website source from URL\n",
        "            website = re.findall(r'https?://(?:www\\.)?([^/]+)/', url)\n",
        "            website = website[0] if website else \"Unknown Source\"\n",
        "            # Expand topic by extracting headline from URL or similar\n",
        "            summary = generate_summary(description) if description else \"No description available.\"\n",
        "            related_stories.append({\n",
        "                'type': 'News Article',\n",
        "                'title': title,\n",
        "                'summary': summary,\n",
        "                'source': website,\n",
        "                'sentiment': sentiment\n",
        "            })\n",
        "        return related_stories\n",
        "    except requests.exceptions.HTTPError as http_err:\n",
        "        logging.error(f\"HTTP error occurred while fetching related news: {http_err}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching related news: {e}\")\n",
        "        return []\n",
        "\n",
        "@cached(post_relevance_cache)\n",
        "def is_post_relevant(post_title, topic):\n",
        "    \"\"\"\n",
        "    Determines if a Reddit post is relevant to the selected topic using OpenAI's GPT-3.5-turbo.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are a helpful assistant that determines if a Reddit post is relevant to a given topic. \"\n",
        "                        \"You should respond with 'Yes' or 'No'.\"\n",
        "                    )\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Is the following Reddit post relevant to the topic '{topic}'?\\n\\nPost Title: {post_title}\\n\\nRespond with 'Yes' or 'No'.\"\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=1,\n",
        "            temperature=0,\n",
        "        )\n",
        "        answer = response.choices[0].message.content.strip().lower()\n",
        "        return answer == 'yes'\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error checking post relevance: {e}\")\n",
        "        return False\n",
        "\n",
        "def extract_keywords_openai(text, max_keywords=5):\n",
        "    \"\"\"\n",
        "    Extracts keywords from the text using OpenAI's GPT-3.5-turbo.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are an assistant that extracts relevant keywords from a given text. Provide a list of up to five keywords separated by commas.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Extract up to {max_keywords} relevant keywords from the following text:\\n\\n{text}\"\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=60,\n",
        "            temperature=0.5,\n",
        "        )\n",
        "        keywords = response.choices[0].message.content.strip()\n",
        "        # Split the keywords by commas and strip whitespace\n",
        "        keyword_list = [kw.strip() for kw in keywords.split(',') if kw.strip()]\n",
        "        return keyword_list\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error extracting keywords with OpenAI: {e}\")\n",
        "        return []\n",
        "\n",
        "def find_related_subreddits(keywords, limit=5):\n",
        "    \"\"\"\n",
        "    Finds related subreddits based on a list of keywords using Reddit's API.\n",
        "    \"\"\"\n",
        "    related_subreddits = set()\n",
        "    for keyword in keywords:\n",
        "        try:\n",
        "            subreddits = reddit.subreddits.search(query=keyword, limit=limit)\n",
        "            for subreddit in subreddits:\n",
        "                # Filter out NSFW, private, and restricted subreddits\n",
        "                if subreddit.over18 or subreddit.subreddit_type in ['private', 'restricted']:\n",
        "                    continue\n",
        "                # Additional filters: minimum subscriber count\n",
        "                if subreddit.subscribers < 1000:\n",
        "                    continue\n",
        "                related_subreddits.add(subreddit.display_name)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error searching subreddits with keyword '{keyword}': {e}\")\n",
        "            continue\n",
        "    return list(related_subreddits)[:limit]\n",
        "\n",
        "def fetch_reddit_posts_from_subreddits(subreddits, topic, limit=10):\n",
        "    \"\"\"\n",
        "    Fetches top posts from specified subreddits that are relevant to the topic.\n",
        "    \"\"\"\n",
        "    reddit_posts = []\n",
        "    for subreddit_name in subreddits:\n",
        "        try:\n",
        "            subreddit = reddit.subreddit(subreddit_name)\n",
        "            search_results = subreddit.search(topic, limit=limit)\n",
        "            for post in search_results:\n",
        "                if post.over_18:\n",
        "                    continue\n",
        "                if is_post_relevant(post.title, topic):\n",
        "                    sentiment = analyze_sentiment(0, 1)  # Assuming each relevant post has 1 URL\n",
        "                    reddit_posts.append({\n",
        "                        'type': 'Reddit Post',\n",
        "                        'subreddit': subreddit_name,\n",
        "                        'title': post.title,\n",
        "                        'summary': f\"Score: {post.score}\",\n",
        "                        'source': 'Reddit',\n",
        "                        'sentiment': sentiment\n",
        "                    })\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error fetching posts from subreddit '{subreddit_name}': {e}\")\n",
        "            continue\n",
        "    return reddit_posts\n",
        "\n",
        "def extract_keywords_from_topic(topic, max_keywords=5):\n",
        "    \"\"\"\n",
        "    Extracts keywords from the topic using OpenAI.\n",
        "    \"\"\"\n",
        "    keywords = extract_keywords_openai(topic, max_keywords)\n",
        "    # Optional: map synonyms using a predefined dictionary or OpenAI\n",
        "    # For example, map 'football' to 'soccer' if relevant\n",
        "    synonym_mapping = {\n",
        "        'football': 'soccer',\n",
        "        'election': 'voting',\n",
        "        'tornadoes': 'tornado'\n",
        "        # Add more mappings as needed\n",
        "    }\n",
        "    mapped_keywords = [synonym_mapping.get(kw.lower(), kw) for kw in keywords]\n",
        "    return mapped_keywords\n",
        "\n",
        "def find_top_subreddits(keywords, limit=5):\n",
        "    \"\"\"\n",
        "    Finds top related subreddits based on extracted keywords.\n",
        "    \"\"\"\n",
        "    subreddits = find_related_subreddits(keywords, limit)\n",
        "    return subreddits\n",
        "\n",
        "def analyze_sentiment(visit_count, url_count):\n",
        "    \"\"\"\n",
        "    Analyzes the sentiment based on visit_count and url_count.\n",
        "    Returns 'Positive', 'Negative', or 'Neutral'.\n",
        "    \"\"\"\n",
        "    return analyze_sentiment(visit_count, url_count)\n",
        "\n",
        "def generate_summary(content):\n",
        "    \"\"\"\n",
        "    Generates a concise two-sentence summary of the provided content using OpenAI's GPT-3.5-turbo.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a concise summarizer. Provide a clear and brief two-sentence summary of the following content.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": content\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=150,\n",
        "            temperature=0.5,\n",
        "        )\n",
        "        summary = response.choices[0].message.content.strip()\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating summary: {e}\")\n",
        "        return \"No summary available.\"\n",
        "\n",
        "def generate_fan_sentiment_summary(title, sentiment):\n",
        "    \"\"\"\n",
        "    Generates a summary of fan sentiment from a Reddit post title using OpenAI's GPT-3.5-turbo.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are an assistant that summarizes fan sentiments from online discussions into a brief overview.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"The following Reddit post has a {sentiment} sentiment:\\n\\n'{title}'\\n\\nSummarize this sentiment for inclusion in a video script.\"\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=200,\n",
        "            temperature=0.5,\n",
        "        )\n",
        "        summary = response.choices[0].message.content.strip()\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating fan sentiment summary: {e}\")\n",
        "        return \"No summary available.\"\n",
        "\n",
        "def generate_script_for_topic(topic, content_summary, style=\"Normal Script\"):\n",
        "    \"\"\"\n",
        "    Generates a video script for the given topic and summary using OpenAI's GPT-3.5-turbo.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        style_prompts = {\n",
        "            \"Flashy Script\": \"Create a flashy, high-energy script with quick cuts, bold visuals, and impactful statements.\",\n",
        "            \"Expressive Script\": \"Compose an expressive script that evokes emotions, using descriptive language and a storytelling approach.\",\n",
        "            \"Normal Script\": \"Write a straightforward script that clearly presents the information in an engaging manner.\"\n",
        "        }\n",
        "        style_prompt = style_prompts.get(style, style_prompts[\"Normal Script\"])\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",  # Using 'gpt-3.5-turbo' for efficiency\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are a creative scriptwriter for short-form videos. \"\n",
        "                        \"Produce engaging, dynamic scripts that are visually compelling and can be represented \"\n",
        "                        \"using stock images and videos. The script should be suitable for a 60-second video, \"\n",
        "                        \"have a captivating introduction, a clear narrative flow, and a strong conclusion or call-to-action. \"\n",
        "                        \"Include specific details about the trend, such as key events, statistics, and other relevant information.\"\n",
        "                    )\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": (\n",
        "                        f\"{style_prompt}\\n\"\n",
        "                        f\"Using the following summary, write a detailed script suitable for a 60-second video:\\n\\n\"\n",
        "                        f\"Summary: {content_summary}\\n\\n\"\n",
        "                        \"Ensure the script includes vivid descriptions and is structured with a beginning, middle, and end. \"\n",
        "                        \"Use language that resonates with the target audience, and make sure it's adaptable with widely available stock images and videos.\"\n",
        "                    )\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=1500,  # Increased max_tokens for longer scripts\n",
        "            temperature=0.7,\n",
        "        )\n",
        "        script_content = response.choices[0].message.content.strip()\n",
        "        return script_content\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating script: {e}\")\n",
        "        return \"No script available.\"\n"
      ],
      "metadata": {
        "id": "-2yQIa03r7Ml"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 9: Define the Main Function\n",
        "\n",
        "import sys\n",
        "import threading\n",
        "import random\n",
        "import textwrap\n",
        "import time\n",
        "from IPython.display import display, Markdown\n",
        "from prettytable import PrettyTable\n",
        "import logging\n",
        "import prettytable\n",
        "\n",
        "def main():\n",
        "    # Function to display a changing message every 15 seconds\n",
        "    def flashing_message(stop_event):\n",
        "        messages = [\n",
        "            \"ðŸ” Gathering the latest trends...\",\n",
        "            \"â³ Processing data, please wait...\",\n",
        "            \"âœ¨ Almost there, thank you for your patience!\"\n",
        "        ]\n",
        "        idx = 0\n",
        "        while not stop_event.is_set():\n",
        "            message = messages[idx % len(messages)]\n",
        "            print(f\"\\r{message}   \", end='', flush=True)\n",
        "            for _ in range(15):\n",
        "                if stop_event.is_set():\n",
        "                    break\n",
        "                time.sleep(1)\n",
        "            idx += 1\n",
        "            print('\\r' + ' ' * len(message) + '   ', end='', flush=True)\n",
        "\n",
        "    # Step 1: User selects the country for trending topics\n",
        "    while True:\n",
        "        print(\"Enter the country for trending topics data (e.g., United States):\")\n",
        "        country_input = input(\"Country: \").strip()\n",
        "        matching_country = get_matching_country(country_input, available_countries)\n",
        "        if not matching_country:\n",
        "            print(\"No matching countries found. Please try again.\")\n",
        "            # Optionally, display available countries\n",
        "            print(\"Available countries are:\")\n",
        "            for country in available_countries.keys():\n",
        "                print(f\"- {country}\")\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    selected_country = matching_country\n",
        "    selected_country_code = available_countries[selected_country]\n",
        "    print(f\"You selected: {selected_country}\")\n",
        "\n",
        "    # Step 2: User selects the time frame for trending topics\n",
        "    print(\"\\nSelect the time frame for trending topics:\")\n",
        "    print(\"\\033[1mA.\\033[0m Last 4 hours\")\n",
        "    print(\"\\033[1mB.\\033[0m Last 24 hours\")\n",
        "    time_range_selection = input(\"Enter the letter of the time frame you're interested in: \").strip().upper()\n",
        "    time_range_mapping = {'A': '4h', 'B': '24h'}\n",
        "    time_range = time_range_mapping.get(time_range_selection)\n",
        "    if not time_range:\n",
        "        print(\"Invalid selection. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Start flashing message while fetching trending topics\n",
        "    stop_event = threading.Event()\n",
        "    thread = threading.Thread(target=flashing_message, args=(stop_event,))\n",
        "    thread.start()\n",
        "\n",
        "    # Fetch trending topics based on selected country and time frame\n",
        "    trending_topics_google = []\n",
        "    trending_topics_serpapi = []\n",
        "    trending_topics_newsapi = []\n",
        "\n",
        "    if time_range == '4h':\n",
        "        # For the last 4 hours, use SerpAPI and NewsAPI\n",
        "        trending_topics_serpapi = fetch_trending_topics_serpapi(country_code=selected_country_code, limit=10)\n",
        "        trending_topics_newsapi = fetch_trending_topics_newsapi(country=selected_country_code.lower(), limit=10)\n",
        "    else:\n",
        "        # For the last 24 hours, use Google Trends RSS feed\n",
        "        trending_topics_google = fetch_trending_topics_google_rss(geo=selected_country_code)\n",
        "\n",
        "    # Stop flashing message\n",
        "    stop_event.set()\n",
        "    thread.join()\n",
        "    print()  # Move to the next line after flashing message\n",
        "\n",
        "    # Combine trending topics from all sources\n",
        "    combined_trending_topics = trending_topics_google + trending_topics_serpapi + trending_topics_newsapi\n",
        "\n",
        "    if not combined_trending_topics:\n",
        "        logging.error(\"No trending topics found from any source.\")\n",
        "        print(\"No trending topics found.\")\n",
        "        return\n",
        "\n",
        "    # Remove duplicates based on title\n",
        "    seen_titles = set()\n",
        "    unique_trending_topics = []\n",
        "    for topic in combined_trending_topics:\n",
        "        title = topic['title']\n",
        "        if title not in seen_titles:\n",
        "            seen_titles.add(title)\n",
        "            unique_trending_topics.append(topic)\n",
        "\n",
        "    # Display the trending topics with additional information in a table\n",
        "    print(f\"\\nCurrent Trending Topics in {selected_country}:\\n\")\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [\"No.\", \"Topic\", \"Reason\", \"Sentiment\"]\n",
        "    table.hrules = prettytable.ALL  # Add horizontal lines between rows\n",
        "    table.max_width = 40  # Set maximum width for columns suitable for phone screens\n",
        "    for idx, topic in enumerate(unique_trending_topics, start=1):\n",
        "        title = textwrap.fill(topic['title'], width=40)\n",
        "        reason = textwrap.fill(topic['description'], width=40)\n",
        "        sentiment = topic.get('sentiment', 'Neutral')\n",
        "        table.add_row([idx, title, reason, sentiment])\n",
        "    print(table)\n",
        "\n",
        "    # Allow the user to select a topic\n",
        "    try:\n",
        "        selected_idx = int(input(\"Enter the number of the topic you're interested in: \"))\n",
        "        if 1 <= selected_idx <= len(unique_trending_topics):\n",
        "            selected_topic = unique_trending_topics[selected_idx - 1]['title']\n",
        "            # Apply text wrapping to the selected topic message\n",
        "            selected_topic_display = textwrap.fill(f\"You selected: {selected_topic}\", width=40)\n",
        "            print(f\"\\n{selected_topic_display}\")\n",
        "        else:\n",
        "            logging.error(\"Invalid selection. Exiting.\")\n",
        "            print(\"Invalid selection. Exiting.\")\n",
        "            return\n",
        "    except ValueError:\n",
        "        logging.error(\"Invalid input. Please enter a number. Exiting.\")\n",
        "        print(\"Invalid input. Please enter a number. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Step 3: Fetch related news articles\n",
        "    related_stories = fetch_related_news(selected_topic, limit=5)\n",
        "    if related_stories:\n",
        "        print(\"\\nRelated News Articles & Reddit Posts:\")\n",
        "        combined_related = []\n",
        "        for story in related_stories:\n",
        "            combined_related.append({\n",
        "                'type': 'News Article',\n",
        "                'title': story['title'],\n",
        "                'summary': story['summary'],\n",
        "                'source': story['source'],\n",
        "                'sentiment': story['sentiment']\n",
        "            })\n",
        "    else:\n",
        "        print(\"\\nNo related news articles found.\")\n",
        "        combined_related = []\n",
        "\n",
        "    # Step 4: Search Reddit directly using the topic as a query\n",
        "    print(\"\\nSearching Reddit for posts related to the topic...\")\n",
        "    all_posts = []\n",
        "    try:\n",
        "        subreddit = reddit.subreddit('all')\n",
        "        search_results = subreddit.search(selected_topic, limit=20)\n",
        "        for post in search_results:\n",
        "            if post.over_18:\n",
        "                continue\n",
        "            if is_post_relevant(post.title, selected_topic):\n",
        "                sentiment = analyze_sentiment(0, 1)  # Assuming each relevant post has 1 URL\n",
        "                # Extract subreddit name as source\n",
        "                subreddit_name = post.subreddit.display_name\n",
        "                all_posts.append({\n",
        "                    'type': 'Reddit Post',\n",
        "                    'title': post.title,\n",
        "                    'summary': f\"Score: {post.score}\",\n",
        "                    'source': subreddit_name,\n",
        "                    'sentiment': sentiment\n",
        "                })\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error searching Reddit: {e}\")\n",
        "\n",
        "    # If no posts found, attempt to find related subreddits and fetch posts from them\n",
        "    if not all_posts:\n",
        "        print(\"No direct Reddit posts found. Attempting to find related subreddits...\")\n",
        "        # Extract keywords from the topic\n",
        "        keywords = extract_keywords_from_topic(selected_topic, max_keywords=5)\n",
        "        if keywords:\n",
        "            # Find related subreddits based on keywords\n",
        "            related_subreddits = find_top_subreddits(keywords, limit=5)\n",
        "            if related_subreddits:\n",
        "                print(f\"Found related subreddits: {', '.join(related_subreddits)}\")\n",
        "                # Fetch posts from related subreddits\n",
        "                reddit_posts = fetch_reddit_posts_from_subreddits(related_subreddits, selected_topic, limit=10)\n",
        "                if reddit_posts:\n",
        "                    all_posts.extend(reddit_posts)\n",
        "                else:\n",
        "                    print(\"No relevant posts found in related subreddits.\")\n",
        "            else:\n",
        "                print(\"No related subreddits found based on the extracted keywords.\")\n",
        "        else:\n",
        "            print(\"No keywords extracted from the selected topic.\")\n",
        "\n",
        "    # Combine news articles and Reddit posts\n",
        "    combined_content = combined_related + all_posts\n",
        "\n",
        "    if not combined_content:\n",
        "        logging.error(\"No content available for script generation. Exiting.\")\n",
        "        print(\"\\nNo content available for script generation.\")\n",
        "        return\n",
        "\n",
        "    # Format the combined content\n",
        "    print(\"\\nRelated News Articles & Reddit Posts:\")\n",
        "    formatted_content = []\n",
        "    for content in combined_content:\n",
        "        title = textwrap.fill(content['title'], width=60)\n",
        "        summary = textwrap.fill(content['summary'], width=60)\n",
        "        source = content['source']\n",
        "        formatted_content.append({\n",
        "            'type': content['type'],\n",
        "            'title': title,\n",
        "            'summary': summary,\n",
        "            'source': source,\n",
        "            'sentiment': content['sentiment']\n",
        "        })\n",
        "\n",
        "    # Display the combined content in a table\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [\"No.\", \"Type\", \"Title\", \"Summary\", \"Source\", \"Sentiment\"]\n",
        "    table.hrules = prettytable.ALL  # Add horizontal lines between rows\n",
        "    table.max_width = 60  # Adjust width as needed\n",
        "    for idx, content in enumerate(formatted_content, start=1):\n",
        "        table.add_row([\n",
        "            idx,\n",
        "            content['type'],\n",
        "            content['title'],\n",
        "            content['summary'],\n",
        "            f\"[{content['source']}]\",  # Adding source in brackets\n",
        "            content['sentiment']\n",
        "        ])\n",
        "    print(table)\n",
        "\n",
        "    # Allow user to select content for script generation\n",
        "    print(\"\\nAvailable Content for Script Generation:\")\n",
        "    for idx, content in enumerate(formatted_content, start=1):\n",
        "        print(f\"{idx}. [{content['type']}] {content['title']} ({content['source']}) - Sentiment: {content['sentiment']}\")\n",
        "\n",
        "    # Shorten the input prompt and ensure it wraps properly\n",
        "    input_prompt = \"Enter the numbers of the items you want to generate scripts for (e.g., 1,3,5):\\n\"\n",
        "    try:\n",
        "        selected_content_input = input(input_prompt)\n",
        "        selected_indices = [int(i.strip()) - 1 for i in selected_content_input.split(',') if i.strip().isdigit()]\n",
        "        selected_items = [formatted_content[i] for i in selected_indices if 0 <= i < len(formatted_content)]\n",
        "        if not selected_items:\n",
        "            logging.error(\"No valid items selected. Exiting.\")\n",
        "            print(\"No valid items selected. Exiting.\")\n",
        "            return\n",
        "    except ValueError:\n",
        "        logging.error(\"Invalid input. Please enter numbers separated by commas. Exiting.\")\n",
        "        print(\"Invalid input. Please enter numbers separated by commas. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Fetch content and generate summaries for selected items\n",
        "    print(\"\\nFetching content and generating summaries for selected items...\")\n",
        "    for item in selected_items:\n",
        "        title = item['title']\n",
        "        sentiment = item['sentiment']\n",
        "        if item['type'] == 'Reddit Post':\n",
        "            item['summary'] = generate_fan_sentiment_summary(title, sentiment)\n",
        "        else:\n",
        "            if not item['summary']:\n",
        "                item['summary'] = generate_summary(item['summary'])\n",
        "\n",
        "    # Remove stock media availability check as per your instructions\n",
        "    print(\"\\nGenerating scripts for the selected items...\")\n",
        "    for item in selected_items:\n",
        "        title = item['title']\n",
        "        summary = item.get('summary', \"No summary available.\")\n",
        "        source = item['source']\n",
        "        sentiment = item['sentiment']\n",
        "        # Generate script without checking stock media\n",
        "        script = generate_script_for_topic(title, summary, style=\"Normal Script\")\n",
        "        # Display the script with enhanced formatting\n",
        "        script_wrapped = textwrap.fill(script, width=80)\n",
        "        display(Markdown(f\"### Generated Script for '{title}':\\n\\n{script_wrapped}\\n\\n**Source:** [{source}]\"))\n",
        "        print(\"====\\n\")\n",
        "\n",
        "    print(\"\\nScript execution completed.\")\n"
      ],
      "metadata": {
        "id": "O06ulUQJr9wx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 10: Run the Main Function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "0YOaDffLsA15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4b61c81c-bdfe-456a-a632-31e55bd4e4d4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the country for trending topics data (e.g., United States):\n",
            "Country: us\n",
            "You selected: US\n",
            "\n",
            "Select the time frame for trending topics:\n",
            "\u001b[1mA.\u001b[0m Last 4 hours\n",
            "\u001b[1mB.\u001b[0m Last 24 hours\n",
            "Enter the letter of the time frame you're interested in: b\n",
            "                                   \n",
            "\n",
            "Current Trending Topics in US:\n",
            "\n",
            "+-----+----------------------+-----------------------------------------+-----------+\n",
            "| No. |        Topic         |                  Reason                 | Sentiment |\n",
            "+-----+----------------------+-----------------------------------------+-----------+\n",
            "|  1  |  Election Day 2024   |       [Future political interest]       |  Neutral  |\n",
            "+-----+----------------------+-----------------------------------------+-----------+\n",
            "|  2  |     Quincy Jones     |         [Music legend birthday]         |  Neutral  |\n",
            "+-----+----------------------+-----------------------------------------+-----------+\n",
            "|  3  | When is Election Day |      [Upcoming election, confusion]     |  Neutral  |\n",
            "+-----+----------------------+-----------------------------------------+-----------+\n",
            "|  4  |   Where do I vote    |        (Election day approaching)       |  Neutral  |\n",
            "+-----+----------------------+-----------------------------------------+-----------+\n",
            "|  5  |     Dennis Allen     |          [New head coach hire]          |  Neutral  |\n",
            "+-----+----------------------+-----------------------------------------+-----------+\n",
            "|  6  |     Veterans Day     |           [Honoring veterans]           |  Neutral  |\n",
            "+-----+----------------------+-----------------------------------------+-----------+\n",
            "|  7  |      Provision       |          [New government bill]          |  Neutral  |\n",
            "+-----+----------------------+-----------------------------------------+-----------+\n",
            "|  8  |  NFL trade deadline  |         [Player trades, rumors]         |  Neutral  |\n",
            "+-----+----------------------+-----------------------------------------+-----------+\n",
            "|  9  |          ID          |           [New album release]           |  Neutral  |\n",
            "+-----+----------------------+-----------------------------------------+-----------+\n",
            "|  10 |  Voter registration  |  [Election season, high participation.] |  Neutral  |\n",
            "+-----+----------------------+-----------------------------------------+-----------+\n",
            "|  11 |        Voting        |           [Polls closing soon]          |  Neutral  |\n",
            "+-----+----------------------+-----------------------------------------+-----------+\n",
            "|  12 |    Pete Buttigieg    | [Transportation secretary confirmation] |  Neutral  |\n",
            "+-----+----------------------+-----------------------------------------+-----------+\n",
            "|  13 |   Voting Day 2024    |     [Upcoming election preparation]     |  Neutral  |\n",
            "+-----+----------------------+-----------------------------------------+-----------+\n",
            "|  14 |     Daniel Jones     |          [Injured during game]          |  Neutral  |\n",
            "+-----+----------------------+-----------------------------------------+-----------+\n",
            "|  15 |    Where to vote     |           [Upcoming elections]          |  Neutral  |\n",
            "+-----+----------------------+-----------------------------------------+-----------+\n",
            "|  16 |       Vikings        |           [New season release]          |  Neutral  |\n",
            "+-----+----------------------+-----------------------------------------+-----------+\n",
            "|  17 |       Cowboys        |          [Dallas Cowboys win.]          |  Neutral  |\n",
            "+-----+----------------------+-----------------------------------------+-----------+\n",
            "|  18 |    Michael Thomas    |     [Injury concerns, Saints player]    |  Neutral  |\n",
            "+-----+----------------------+-----------------------------------------+-----------+\n",
            "|  19 |    Chappell Roan     |           [New music release]           |  Neutral  |\n",
            "+-----+----------------------+-----------------------------------------+-----------+\n",
            "|  20 |   Lions vs Packers   |          [Close game, rivalry]          |  Neutral  |\n",
            "+-----+----------------------+-----------------------------------------+-----------+\n",
            "Enter the number of the topic you're interested in: 1\n",
            "\n",
            "You selected: Election Day 2024\n",
            "\n",
            "Related News Articles:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**1. Fed eyes another rate cut**\nNo one knows how Tuesday's presidential\nelection will turn out, but the Federal\nReserve's move two days later is much\neasier to predict: With inflation\ncontinuing to cool, the Fed is set to\ncut interest rates for a second time\nthis year.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**2. From Lime to Uber to Hertz: Free and\ndiscounted Election Day rides**\nElection Day is nearly here, and if you\nhavenâ€™t already mailed in your ballot or\ngone in for early voting, you might need\na ride come November 5. Hereâ€™s a roundup\nof all the freebies, discounts, and\ninformation on getting to the polls that\ntransportation compâ€¦\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**3. Trump Claims Pennsylvania Vote Fraud As\nHarris Stumps In Michigan**\nDonald Trump doubled down Sunday on his\nbaseless predictions of voter fraud in\nbattleground states as he and rival\nKamala Harris launched their frantic\nfinal 48 hours of campaigning to court\nthe last holdouts in a bitterly\ncontested US election.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**4. Hereâ€™s When Polls Close on Election Day\nin Battleground States**\nItâ€™s a tight swing state race to 270\nelectoral college votes as the\npresidential election comes to a close.\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**5. Le'Veon Bell issues threat to Kamala\nHarris two days before presidential\nelection**\nThe campaigns for the 2024 presidential\nelection are among the most divisive in\nrecent history. Supporters of both\ncandidates spare no effort to take\npoints away from their rival,\n"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Searching Reddit for posts related to the topic...\n",
            "\n",
            "Available Content for Script Generation:\n",
            "+-----+--------------+------------------------------------------+-----------+\n",
            "| No. |     Type     |                  Title                   | Sentiment |\n",
            "+-----+--------------+------------------------------------------+-----------+\n",
            "|  1  | News Article |        Fed eyes another rate cut         |  Neutral  |\n",
            "+-----+--------------+------------------------------------------+-----------+\n",
            "|  2  | News Article |   From Lime to Uber to Hertz: Free and   |  Positive |\n",
            "|     |              |      discounted Election Day rides       |           |\n",
            "+-----+--------------+------------------------------------------+-----------+\n",
            "|  3  | News Article | Trump Claims Pennsylvania Vote Fraud As  |  Neutral  |\n",
            "|     |              |        Harris Stumps In Michigan         |           |\n",
            "+-----+--------------+------------------------------------------+-----------+\n",
            "|  4  | News Article | Hereâ€™s When Polls Close on Election Day  |  Neutral  |\n",
            "|     |              |          in Battleground States          |           |\n",
            "+-----+--------------+------------------------------------------+-----------+\n",
            "|  5  | News Article |   Le'Veon Bell issues threat to Kamala   |  Neutral  |\n",
            "|     |              |   Harris two days before presidential    |           |\n",
            "|     |              |                 election                 |           |\n",
            "+-----+--------------+------------------------------------------+-----------+\n",
            "|  6  | Reddit Post  | Harrison Ford Endorses Kamala Harris-Tim |  Neutral  |\n",
            "|     |              |      Walz Days Before 2024 Election      |           |\n",
            "+-----+--------------+------------------------------------------+-----------+\n",
            "|  7  | Reddit Post  |  Discussion Thread: First Presidential   |  Positive |\n",
            "|     |              |   Debate of the 2024 General Election    |           |\n",
            "|     |              | Between Vice President Kamala Harris and |           |\n",
            "|     |              |      Former President Donald Trump       |           |\n",
            "+-----+--------------+------------------------------------------+-----------+\n",
            "|  8  | Reddit Post  |   Judge sets Trump DC federal election   |  Positive |\n",
            "|     |              | subversion trial for March 4, 2024 â€” one |           |\n",
            "|     |              |         day before Super Tuesday         |           |\n",
            "+-----+--------------+------------------------------------------+-----------+\n",
            "|  9  | Reddit Post  |   Beware: The Supreme Court Is Laying    |  Neutral  |\n",
            "|     |              | Groundwork to Pre-Rig the 2024 Election  |           |\n",
            "+-----+--------------+------------------------------------------+-----------+\n",
            "|  10 | Reddit Post  | Mormons are organizing for Harris â€” and  |  Neutral  |\n",
            "|     |              |    they could swing the 2024 election    |           |\n",
            "+-----+--------------+------------------------------------------+-----------+\n",
            "|  11 | Reddit Post  |    Why is the 2024 Election so close?    |  Neutral  |\n",
            "+-----+--------------+------------------------------------------+-----------+\n",
            "|  12 | Reddit Post  | Travis County Day 1 Early Voting exceeds |  Neutral  |\n",
            "|     |              |    previous 3 presidential elections.    |           |\n",
            "+-----+--------------+------------------------------------------+-----------+\n",
            "|  13 | Reddit Post  |   Bitcoin has never retraced below its   |  Neutral  |\n",
            "|     |              | election-day price after the results are |           |\n",
            "|     |              | in, Historically BTC explodes post-U.S.  |           |\n",
            "|     |              |    elections, often going parabolic.     |           |\n",
            "+-----+--------------+------------------------------------------+-----------+\n",
            "|  14 | Reddit Post  |   Megathread - 2024 General Election -   |  Neutral  |\n",
            "|     |              |                 Results                  |           |\n",
            "+-----+--------------+------------------------------------------+-----------+\n",
            "|  15 | Reddit Post  | Travis County: 2024 Day One Early Voter  |  Positive |\n",
            "|     |              | Turnout Is Higher Than Last 3 Elections  |           |\n",
            "|     |              |                    ðŸ—³ï¸                    |           |\n",
            "+-----+--------------+------------------------------------------+-----------+\n",
            "Enter the numbers of the items you want to generate scripts for (e.g., 1,3,5):\n",
            "15\n",
            "\n",
            "Fetching content and generating summaries for selected items...\n",
            "\n",
            "Do you want to check for stock media availability for the selected items? (yes/no): n\n",
            "\n",
            "Skipping stock media availability check.\n",
            "\n",
            "Summary: Fans are excited about the high early\n",
            "voter turnout in Travis County for the\n",
            "2024 election, which has surpassed the\n",
            "numbers from the last three elections.\n",
            "\n",
            "Select a script style:\n",
            "\u001b[1m1.\u001b[0m Flashy Script\n",
            "\u001b[1m2.\u001b[0m Expressive Script\n",
            "\u001b[1m3.\u001b[0m Normal Script\n",
            "Enter the number of the script style you prefer: 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Generated Expressive Script for 'Travis County: 2024 Day One Early Voter Turnout Is Higher Than Last 3 Elections ðŸ—³ï¸':\n\n[Opening shot of a bustling cityscape with vibrant energy]  Narrator: In the\nheart of Texas, a wave of excitement is sweeping through Travis County like a\nwildfire.  [Clips of diverse individuals of all ages lining up to vote, their\nfaces filled with determination and hope]  Narrator: The 2024 election season\nhas ignited a passion for change, drawing record numbers of early voters to the\npolls.  [Images of ballot boxes overflowing with votes, displaying signs of\ncivic engagement]  Narrator: Travis County's early voter turnout has shattered\nall expectations, surpassing the combined numbers of the last three elections.\n[Transition to a montage of enthusiastic supporters cheering and waving banners]\nNarrator: From the young to the elderly, from all walks of life, Texans are\ncoming together to make their voices heard.  [Close-up shots of first-time\nvoters casting their ballots, capturing their sense of empowerment]  Narrator:\nEvery vote counts, every voice matters, and in Travis County, democracy is alive\nand thriving.  [Final shot of a diverse group of people linking arms in unity]\nNarrator: Join the movement, be part of history. Let your voice shape the\nfuture. Together, we can make a difference.  [Closing frame with a call-to-\naction: \"Vote for change. Be the voice of tomorrow.\"]  [End with a fade-out as\ninspirational music swells]  Narrator: Travis County: Where every vote counts,\nand hope shines bright."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====\n",
            "\n",
            "\n",
            "Script execution completed.\n"
          ]
        }
      ]
    }
  ]
}