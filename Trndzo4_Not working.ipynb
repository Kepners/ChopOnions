{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kepners/ChopOnions/blob/main/Trndzo4_Not%20working.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Install Required Packages\n",
        "\n",
        "!pip install --upgrade openai==0.28.1 python-dotenv praw rake_nltk newsapi-python --upgrade lxml_html_clean bleach --upgrade newspaper3k feedparser aiohttp nest_asyncio structlog beautifulsoup4 cachetools fuzzywuzzy python-Levenshtein nltk pytrends ratelimit prettytable bs4 configparser structlog\n",
        "print(\"pip install completed\")\n",
        "import nest_asyncio\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "import lxml\n",
        "import newspaper\n",
        "\n",
        "print(f\"lxml version: {lxml.__version__}\")\n",
        "print(f\"newspaper version: {newspaper.__version__}\")\n",
        "\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "print(\"vader_lexicon downloaded\")\n",
        "nltk.download('stopwords')\n",
        "print(\"stopwords downloaded\")\n",
        "nltk.download('punkt')\n",
        "print(\"punkt downloaded\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "print(\"averaged_perceptron_tagger downloaded\")\n",
        "nltk.download('maxent_ne_chunker')\n",
        "print(\"maxent_ne_chunker downloaded\")\n",
        "nltk.download('words')\n",
        "print(\"words downloaded\")\n",
        "nltk.download('wordnet')\n",
        "print(\"wordnet downloaded\")\n",
        "nltk.download('punkt_tab')\n",
        "print('punkt_tab downloaded')\n",
        "\n",
        "\n",
        "print(\"Installation and NLTK data download completed successfully.\")"
      ],
      "metadata": {
        "id": "CGNrZ1kNMSXk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77c8560e-8ee7-4eb0-e57a-e8fed48ed0ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Processing data, please wait... (This could take up to 2 minutes. Please wait.)   Requirement already satisfied: openai==0.28.1 in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: praw in /usr/local/lib/python3.10/dist-packages (7.8.1)\n",
            "Requirement already satisfied: rake_nltk in /usr/local/lib/python3.10/dist-packages (1.0.6)\n",
            "Requirement already satisfied: newsapi-python in /usr/local/lib/python3.10/dist-packages (0.2.7)\n",
            "Requirement already satisfied: lxml_html_clean in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (6.2.0)\n",
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.10/dist-packages (0.2.8)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.10/dist-packages (6.0.11)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.11.1)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: structlog in /usr/local/lib/python3.10/dist-packages (24.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (5.5.0)\n",
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.10/dist-packages (0.26.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pytrends in /usr/local/lib/python3.10/dist-packages (4.9.2)\n",
            "Requirement already satisfied: ratelimit in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (3.12.0)\n",
            "‚ú® Almost there, thank you for your patience! (This could take up to 2 minutes. Please wait.)   Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: configparser in /usr/local/lib/python3.10/dist-packages (7.1.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (4.66.6)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in /usr/local/lib/python3.10/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from lxml_html_clean) (5.3.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach) (0.5.1)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (11.0.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.2)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (1.2.0)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (5.1.3)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser) (1.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: Levenshtein==0.26.1 in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein) (0.26.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein==0.26.1->python-Levenshtein) (3.10.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from pytrends) (2.2.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable) (0.2.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (1.26.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2024.8.30)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.16.1)\n",
            "pip install completed\n",
            "lxml version: 5.3.0\n",
            "newspaper version: 0.2.8\n",
            "vader_lexicon downloaded\n",
            "stopwords downloaded\n",
            "punkt downloaded\n",
            "averaged_perceptron_tagger downloaded\n",
            "maxent_ne_chunker downloaded\n",
            "words downloaded\n",
            "wordnet downloaded\n",
            "punkt_tab downloaded\n",
            "Installation and NLTK data download completed successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 2: Mount Google Drive and Load Configuration\n",
        "from google.colab import drive\n",
        "import configparser\n",
        "import os\n",
        "import openai  # import the openai module\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to your config.ini in Google Drive\n",
        "config_path = '/content/drive/MyDrive/Secrets/config.ini'  # Update this path as needed\n",
        "\n",
        "# Check if config.ini exists at the specified path\n",
        "if not os.path.exists(config_path):\n",
        "    raise FileNotFoundError(f\"config.ini not found at {config_path}\")\n",
        "\n",
        "# Load configuration using configparser\n",
        "config = configparser.ConfigParser()\n",
        "config.read(config_path)\n",
        "\n",
        "# Retrieve OpenAI API key\n",
        "try:\n",
        "    openai_api_key = config.get('openai', 'api_key', fallback=None)\n",
        "except KeyError:\n",
        "    raise ValueError(\"OpenAI API key not found or invalid format in config.ini under [openai] section.\")\n",
        "\n",
        "if not openai_api_key:\n",
        "    raise ValueError(\"OpenAI API key not found or invalid format in config.ini under [openai] section.\")\n",
        "\n",
        "# Set the OpenAI API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "# Retrieve NewsAPI key\n",
        "try:\n",
        "    newsapi_key = config.get('newsapi', 'api_key')\n",
        "except KeyError:\n",
        "    raise ValueError(\"NewsAPI key not found in config.ini under [newsapi] section.\")\n",
        "\n",
        "# Retrieve caching configurations\n",
        "rss_cache_ttl = config.getint('CACHING', 'RSS_CACHE_TTL', fallback=86400)\n",
        "trend_cache_ttl = config.getint('CACHING', 'TREND_CACHE_TTL', fallback=3600)\n",
        "openai_cache_ttl = config.getint('CACHING', 'OPENAI_CACHE_TTL', fallback=86400)\n",
        "\n",
        "# Retrieve rate limiting configurations\n",
        "rss_calls_per_day = config.getint('RATE_LIMITING', 'RSS_CALLS_PER_DAY', fallback=1000)\n",
        "\n",
        "print(\"Configuration loaded successfully.\")\n"
      ],
      "metadata": {
        "id": "BNlyvpVs_QsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49754401-3d75-46a0-bf9c-ce084f8b0b9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Gathering the latest trends... (This could take up to 2 minutes. Please wait.)   Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Configuration loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 3: Define Data Structures\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional\n",
        "\n",
        "@dataclass\n",
        "class GoogleTrend:\n",
        "    topic: str\n",
        "    interest: str\n",
        "    sentiment: str\n",
        "\n",
        "@dataclass\n",
        "class Trend:\n",
        "    title: str\n",
        "    description: str\n",
        "    source: str\n",
        "    approx_traffic: str\n",
        "    sentiment: str\n",
        "    google_trend: Optional[GoogleTrend] = None\n",
        "\n",
        "@dataclass\n",
        "class ScriptOptions:\n",
        "    style: str = \"Normal Script\"\n",
        "    tone: str = \"Informative\"\n",
        "    length: str = \"60 seconds\"\n",
        "\n",
        "print(\"Data structures defined successfully.\")\n"
      ],
      "metadata": {
        "id": "4wkdmt7yp0mR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bbb9143-1eee-40d2-8ffa-f86ca7d921b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data structures defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gIud78FOZR3M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "783dd43c-462e-4aae-cfda-06e4c30ec3c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration and initialization completed successfully.\n"
          ]
        }
      ],
      "source": [
        "# Block 4: Configuration and Initialization\n",
        "\n",
        "import configparser\n",
        "from pytrends.request import TrendReq # This import will now work\n",
        "from newsapi import NewsApiClient\n",
        "\n",
        "# Initialize OpenAI\n",
        "api_key = openai_api_key\n",
        "\n",
        "# Initialize NewsAPI\n",
        "newsapi = NewsApiClient(api_key=newsapi_key)\n",
        "\n",
        "# Initialize PyTrends with custom requests arguments\n",
        "pytrends = TrendReq(hl='en-US', tz=360, requests_args={'headers': {'User-Agent': 'Mozilla/5.0'}})\n",
        "\n",
        "print(\"Configuration and initialization completed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGPK1IRBXfBm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "991b2679-86f0-488a-e4de-8f68393f1df0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'__init__.py' already exists at 'utils/__init__.py'.\n",
            "Created 'utils/data_processing.py' successfully.\n",
            "\n",
            "Verifying the creation of 'data_processing.py' and '__init__.py':\n",
            "total 4\n",
            "-rw-r--r-- 1 root root 1534 Nov 14 19:05 data_processing.py\n",
            "-rw-r--r-- 1 root root    0 Nov 14 15:43 __init__.py\n"
          ]
        }
      ],
      "source": [
        "# Block 5: Create utils/data_processing.py and utils/__init__.py\n",
        "\n",
        "# Define the directory where helper functions will reside\n",
        "utils_dir = 'utils'\n",
        "\n",
        "# Create the 'utils' directory if it doesn't exist\n",
        "os.makedirs(utils_dir, exist_ok=True)\n",
        "\n",
        "# Define the path for the __init__.py file to make 'utils' a package\n",
        "init_path = os.path.join(utils_dir, '__init__.py')\n",
        "\n",
        "# Create an empty __init__.py file if it doesn't exist\n",
        "if not os.path.exists(init_path):\n",
        "    with open(init_path, 'w') as file:\n",
        "        pass  # Creating an empty __init__.py\n",
        "    print(f\"Created empty '__init__.py' at '{init_path}' to make 'utils' a package.\")\n",
        "else:\n",
        "    print(f\"'__init__.py' already exists at '{init_path}'.\")\n",
        "\n",
        "# Define the path for the data_processing.py file\n",
        "data_processing_path = os.path.join(utils_dir, 'data_processing.py')\n",
        "\n",
        "# Define the content for data_processing.py\n",
        "data_processing_code = \"\"\"\n",
        "# data_processing.py\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ne_chunk, pos_tag\n",
        "from nltk.tree import Tree\n",
        "\n",
        "def clean_text(text):\n",
        "    '''\n",
        "    Cleans the input text by removing URLs, special characters, and stopwords.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to clean.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned text.\n",
        "    '''\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\\\S+', '', text)\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^A-Za-z\\\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    # Join the words back into a single string\n",
        "    cleaned_text = ' '.join(filtered_words)\n",
        "    return cleaned_text\n",
        "\n",
        "def extract_entities(text):\n",
        "    '''\n",
        "    Extracts named entities from the input text.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to extract entities from.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of named entities.\n",
        "    '''\n",
        "    def get_entities(tree):\n",
        "        entities = []\n",
        "        for subtree in tree:\n",
        "            if isinstance(subtree, Tree):\n",
        "                entity = \" \".join([token for token, pos in subtree.leaves()])\n",
        "                entities.append(entity)\n",
        "        return entities\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = pos_tag(tokens)\n",
        "    chunked = ne_chunk(tagged)\n",
        "    entities = get_entities(chunked)\n",
        "    return entities\n",
        "\"\"\"\n",
        "\n",
        "# Write the data_processing.py file\n",
        "with open(data_processing_path, 'w') as file:\n",
        "    file.write(data_processing_code)\n",
        "\n",
        "print(f\"Created '{data_processing_path}' successfully.\")\n",
        "\n",
        "# Optional: Verify the creation by listing the 'utils' directory\n",
        "print(\"\\nVerifying the creation of 'data_processing.py' and '__init__.py':\")\n",
        "!ls -l utils/\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(f\"Current Working Directory: {os.getcwd()}\")\n"
      ],
      "metadata": {
        "id": "vLVzZkCSYp0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e874caa-81d1-4c80-d201-2c0bfb618e43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5RZdBozr4hJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "cad06862-d487-48b5-80f7-ee4a01d3b883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rss_feeds' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-1a8ac5d9988f>\u001b[0m in \u001b[0;36m<cell line: 649>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m rss_feeds = [\n\u001b[0;32m--> 650\u001b[0;31m     \u001b[0murl\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrss_feeds\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvalidate_rss_feed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m ]\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rss_feeds' is not defined"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import asyncio # For asynchronous programming\n",
        "import aiohttp # For asynchronous HTTP requests\n",
        "import structlog\n",
        "from prettytable import PrettyTable, HRuleStyle  # Import HRuleStyle for hrules\n",
        "from cachetools import TTLCache, cached # For caching\n",
        "from fuzzywuzzy import process\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from rake_nltk import Rake\n",
        "import feedparser\n",
        "import time\n",
        "import re\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "import openai\n",
        "import openai.error  # Add this line\n",
        "import textwrap  # For wrapping text in the table\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from newsapi import NewsApiClient\n",
        "from bs4 import BeautifulSoup\n",
        "import difflib\n",
        "import nltk\n",
        "import threading\n",
        "import configparser  # Import configparser\n",
        "import random\n",
        "import requests\n",
        "from newspaper import Article\n",
        "from pytrends.request import TrendReq\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "import pandas as pd\n",
        "pd.set_option('future.no_silent_downcasting', True)\n",
        "from random import uniform\n",
        "import time\n",
        "import logging\n",
        "from contextlib import redirect_stdout\n",
        "from urllib.parse import urlparse, parse_qs\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "\n",
        "openai_semaphore = asyncio.Semaphore(1)  # Limit to 1 concurrent requests\n",
        "\n",
        "import random\n",
        "\n",
        "async def retry_with_backoff(func, *args, max_retries=5, backoff_factor=2, **kwargs):\n",
        "    \"\"\"\n",
        "    Retries an async function with exponential backoff and random jitter.\n",
        "    \"\"\"\n",
        "    wait_time = 1\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            return await func(*args, **kwargs)\n",
        "        except openai.error.RateLimitError:\n",
        "            jitter = random.uniform(0, 1)  # Add randomness to retry wait time\n",
        "            logger.warning(f\"Rate limit hit. Retrying in {wait_time + jitter:.2f} seconds... (Attempt {attempt + 1})\")\n",
        "            await asyncio.sleep(wait_time + jitter)\n",
        "            wait_time *= backoff_factor\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Unexpected error during {func.__name__}: {e}\", exc_info=True)\n",
        "            break\n",
        "    logger.error(f\"Failed to complete {func.__name__} after {max_retries} retries.\")\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Suppress NLTK logging messages\n",
        "# ----------------------------\n",
        "logging.getLogger('nltk').setLevel(logging.CRITICAL)\n",
        "\n",
        "# Suppress stdout for NLTK downloads\n",
        "with open(os.devnull, 'w') as f:\n",
        "    with redirect_stdout(f):\n",
        "        try:\n",
        "            nltk.data.find('tokenizers/punkt')\n",
        "            nltk.data.find('corpora/stopwords')\n",
        "            nltk.data.find('sentiment/vader_lexicon.zip')\n",
        "        except LookupError:\n",
        "            nltk.download('vader_lexicon')\n",
        "            nltk.download('stopwords')\n",
        "            nltk.download('punkt')\n",
        "import structlog\n",
        "import logging\n",
        "\n",
        "# ----------------------------\n",
        "# Structured Logging with structlog\n",
        "# ----------------------------\n",
        "\n",
        "structlog.configure(\n",
        "    processors=[\n",
        "        structlog.processors.TimeStamper(fmt=\"iso\"),\n",
        "        structlog.processors.JSONRenderer()\n",
        "    ],\n",
        "    context_class=dict,\n",
        "    logger_factory=structlog.stdlib.LoggerFactory(),\n",
        "    wrapper_class=structlog.stdlib.BoundLogger,\n",
        "    cache_logger_on_first_use=True,\n",
        ")\n",
        "\n",
        "logger = structlog.get_logger()\n",
        "\n",
        "# Additionally, set the logging level to DEBUG\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Read API Keys from config.ini\n",
        "# ----------------------------\n",
        "\n",
        "# Initialize ConfigParser\n",
        "config = configparser.ConfigParser()\n",
        "config.read('/content/drive/MyDrive/Secrets/config.ini')  # Ensure this path is correct\n",
        "\n",
        "# Handle missing API keys gracefully\n",
        "try:\n",
        "    openai_api_key = config.get('openai', 'api_key')\n",
        "    newsapi_api_key = config.get('newsapi', 'api_key')\n",
        "except (configparser.NoSectionError, configparser.NoOptionError) as e:\n",
        "    logger.error(f\"Error reading API keys from config.ini: {e}\", exc_info=True)\n",
        "    sys.exit(\"Failed to read API keys from config.ini. Please ensure the file exists and contains the necessary keys.\")\n",
        "\n",
        "# Set your OpenAI API key from config.ini\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "# Initialize NewsApiClient with your API key from config.ini\n",
        "newsapi = NewsApiClient(api_key=newsapi_api_key)\n",
        "\n",
        "# ----------------------------\n",
        "# Caching\n",
        "# ----------------------------\n",
        "from cachetools import TTLCache\n",
        "# Define caching constants\n",
        "TREND_CACHE_TTL = 3600       # 1 hour in seconds\n",
        "RSS_CACHE_TTL = 3600         # 1 hour in seconds\n",
        "OPENAI_CACHE_TTL = 86400     # 1 day in seconds\n",
        "\n",
        "# Initialize caches with defined TTLs\n",
        "trends_cache = TTLCache(maxsize=100, ttl=TREND_CACHE_TTL)\n",
        "openai_cache = TTLCache(maxsize=1000, ttl=OPENAI_CACHE_TTL)\n",
        "rss_cache = TTLCache(maxsize=10, ttl=RSS_CACHE_TTL)  # Added RSS cache\n",
        "\n",
        "# ----------------------------\n",
        "# Rate Limiting for RSS Feeds\n",
        "# ----------------------------\n",
        "\n",
        "ONE_DAY = 86400               # Seconds in one day\n",
        "RSS_CALLS_PER_DAY = 10      # Maximum number of RSS feed calls per day\n",
        "\n",
        "@sleep_and_retry\n",
        "@limits(calls=RSS_CALLS_PER_DAY, period=ONE_DAY)\n",
        "def fetch_rss_feed_sync(rss_url):\n",
        "    # Placeholder for synchronous RSS feed fetching if needed\n",
        "    pass\n",
        "\n",
        "# ----------------------------\n",
        "# Initialize Sentiment Analyzer and Keyword Extractor\n",
        "# ----------------------------\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize Sentiment Analyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "rake = Rake()\n",
        "\n",
        "# ----------------------------\n",
        "# OpenAI API Key and Semaphore\n",
        "# ----------------------------\n",
        "\n",
        "# Define a semaphore to limit concurrent requests\n",
        "google_trends_semaphore = asyncio.Semaphore(1)  # Reduce concurrency to 2\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Helper Functions\n",
        "# ----------------------------\n",
        "\n",
        "async def fetch_and_cache_rss_feed(rss_url, limit):\n",
        "    \"\"\"\n",
        "    Fetches RSS feed data and caches it for reuse.\n",
        "\n",
        "    Parameters:\n",
        "        rss_url (str): The URL of the RSS feed.\n",
        "        limit (int): Number of entries to fetch.\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: Parsed RSS entries.\n",
        "    \"\"\"\n",
        "    # Check if the feed is already cached\n",
        "    if rss_url in rss_cache:\n",
        "        logger.info(f\"Using cached RSS feed for {rss_url}\")\n",
        "        return rss_cache[rss_url]\n",
        "\n",
        "    # Fetch the RSS feed\n",
        "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "    try:\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            async with session.get(rss_url, headers=headers, timeout=10) as response:\n",
        "                if response.status == 200:\n",
        "                    content = await response.text()\n",
        "                    feed = feedparser.parse(content)\n",
        "                    # Cache the result\n",
        "                    rss_cache[rss_url] = feed.entries[:limit]\n",
        "                    logger.info(f\"Fetched and cached RSS feed for {rss_url}\")\n",
        "                    return feed.entries[:limit]\n",
        "                else:\n",
        "                    logger.error(f\"Failed to fetch RSS feed {rss_url}: Status {response.status}\")\n",
        "                    return []\n",
        "    except aiohttp.ClientError as e:\n",
        "        logger.error(f\"Aiohttp error fetching RSS feed {rss_url}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Available countries with multiple codes\n",
        "available_countries = {\n",
        "    'United States': ['US', 'USA'],\n",
        "    'Canada': ['CA'],\n",
        "    'United Kingdom': ['GB', 'UK'],\n",
        "    'Australia': ['AU'],\n",
        "    'India': ['IN'],\n",
        "    'Germany': ['DE'],\n",
        "    'France': ['FR'],\n",
        "    'Japan': ['JP'],\n",
        "    'Brazil': ['BR'],\n",
        "    'South Korea': ['KR'],\n",
        "    'Argentina': ['AR'],\n",
        "    'Mexico': ['MX'],\n",
        "    'Singapore': ['SG'],\n",
        "    'Spain': ['ES'],\n",
        "    'Italy': ['IT'],\n",
        "    'Netherlands': ['NL'],\n",
        "    'Poland': ['PL'],\n",
        "    'Sweden': ['SE'],\n",
        "    'Switzerland': ['CH'],\n",
        "    # Add more countries and their codes as needed\n",
        "}\n",
        "\n",
        "def get_matching_country(input_country, available_countries):\n",
        "    \"\"\"\n",
        "    Matches user input to available countries.\n",
        "\n",
        "    Parameters:\n",
        "        input_country (str): The country input by the user.\n",
        "        available_countries (dict): Dictionary mapping country names to lists of codes.\n",
        "\n",
        "    Returns:\n",
        "        str or None: The matched country name or None if no match.\n",
        "    \"\"\"\n",
        "    normalized_input = input_country.lower()\n",
        "    for country, codes in available_countries.items():\n",
        "        # Check if input matches the country name\n",
        "        if normalized_input == country.lower():\n",
        "            return country\n",
        "        # Check if input matches any of the country codes\n",
        "        if normalized_input in [code.lower() for code in codes]:\n",
        "            return country\n",
        "    # If no direct match, attempt fuzzy matching\n",
        "    country_names = list(available_countries.keys())\n",
        "    closest_matches = difflib.get_close_matches(input_country, country_names, n=1, cutoff=0.8)\n",
        "    if closest_matches:\n",
        "        return closest_matches[0]\n",
        "    return None\n",
        "\n",
        "def sanitize_topic(topic):\n",
        "    \"\"\"\n",
        "    Sanitizes the topic string by removing unwanted characters.\n",
        "\n",
        "    Parameters:\n",
        "        topic (str): The topic to sanitize.\n",
        "\n",
        "    Returns:\n",
        "        str: The sanitized topic.\n",
        "    \"\"\"\n",
        "    return re.sub(r'[^\\w\\s]', '', topic)\n",
        "\n",
        "def clean_description(description):\n",
        "    \"\"\"\n",
        "    Cleans and truncates descriptions to ensure proper formatting.\n",
        "    Parameters:\n",
        "        description (str): The original description.\n",
        "    Returns:\n",
        "        str: Cleaned and truncated description.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(description, 'html.parser')  # Remove HTML tags\n",
        "    clean_text = soup.get_text(separator=' ', strip=True)  # Extract clean text\n",
        "    clean_text = re.sub(r'\\s+', ' ', clean_text)  # Normalize whitespace\n",
        "    return (clean_text[:200] + '...') if len(clean_text) > 200 else clean_text\n",
        "\n",
        "def extract_source(url):\n",
        "    \"\"\"\n",
        "    Extracts the main domain name from the URL to identify the source.\n",
        "\n",
        "    Parameters:\n",
        "        url (str): The URL of the news article.\n",
        "\n",
        "    Returns:\n",
        "        str: The name of the source.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        domain = urlparse(url).netloc\n",
        "        # Remove subdomains (e.g., \"www\" or \"edition.cnn.com\")\n",
        "        domain_parts = domain.split('.')\n",
        "        if len(domain_parts) > 2:\n",
        "            domain = '.'.join(domain_parts[-2:])\n",
        "        domain_mapping = {\n",
        "            'cbsnews.com': 'CBS News',\n",
        "            'cnn.com': 'CNN',\n",
        "            'foxnews.com': 'Fox News',\n",
        "            'abcnews.go.com': 'ABC News',\n",
        "            'bbc.co.uk': 'BBC',\n",
        "            'google.com': 'Google News',\n",
        "            'news.google.com': 'Google News',\n",
        "            'reuters.com': 'Reuters',\n",
        "            'theguardian.com': 'The Guardian',\n",
        "            'nytimes.com': 'The New York Times',\n",
        "            'usatoday.com': 'USA Today',\n",
        "            'fortworthstar.com': 'Fort Worth Star-Telegram',\n",
        "            'wcnc.com': 'WCNC',\n",
        "            'apnews.com': 'AP News',\n",
        "            'floridatoday.com': 'Florida Today',\n",
        "            'msnbc.com': 'MSNBC News',\n",
        "            # Add more mappings as needed\n",
        "        }\n",
        "        # Handle subdomains (e.g., edition.cnn.com)\n",
        "        domain_parts = domain.split('.')\n",
        "        if len(domain_parts) > 2:\n",
        "            domain = '.'.join(domain_parts[-2:])\n",
        "        return domain_mapping.get(domain, domain.capitalize())\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting source from URL '{url}': {e}\", exc_info=True)\n",
        "        return \"Unknown Source\"\n",
        "\n",
        "\n",
        "\n",
        "async def fetch_google_trends_async(topic, timeframe='now 7-d', max_retries=5):\n",
        "    \"\"\"\n",
        "    Asynchronously fetches Google Trends data with caching.\n",
        "    \"\"\"\n",
        "    if topic in trends_cache:\n",
        "        logger.info(f\"Using cached Google Trends data for '{topic}'\")\n",
        "        return trends_cache[topic]\n",
        "\n",
        "    # Fetch the data in a separate thread to prevent blocking\n",
        "    result = await asyncio.to_thread(fetch_google_trends, topic, timeframe, max_retries)\n",
        "\n",
        "    if result:\n",
        "        trends_cache[topic] = result  # Cache only successful results\n",
        "\n",
        "    return result\n",
        "\n",
        "def broaden_query(query):\n",
        "    \"\"\"\n",
        "    Broadens the query by adding synonyms or related terms.\n",
        "\n",
        "    Parameters:\n",
        "        query (str): The original query.\n",
        "\n",
        "    Returns:\n",
        "        str: The broadened query.\n",
        "    \"\"\"\n",
        "    # Placeholder for query broadening logic\n",
        "    return query\n",
        "\n",
        "@cached(trends_cache)\n",
        "def fetch_google_trends(topic, timeframe='now 7-d', max_retries=5):\n",
        "    \"\"\"\n",
        "    Fetches Google Trends data for the given topic with retry logic.\n",
        "    \"\"\"\n",
        "    pytrends = TrendReq(hl='en-US', tz=360)\n",
        "    attempt = 0\n",
        "    wait_time = 2  # Initial wait time in seconds\n",
        "\n",
        "    while attempt < max_retries:\n",
        "        try:\n",
        "            pytrends.build_payload([topic], timeframe=timeframe)\n",
        "            interest_over_time = pytrends.interest_over_time()\n",
        "            if interest_over_time is not None and not interest_over_time.empty:\n",
        "                avg_interest = interest_over_time[topic].mean()\n",
        "                sentiment = 'Neutral'\n",
        "                return {'topic': topic, 'interest': avg_interest, 'sentiment': sentiment}\n",
        "            else:\n",
        "                logger.error(f\"No data returned for topic '{topic}' from Google Trends.\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            # Check if the exception is a rate limit error\n",
        "            if \"429\" in str(e):\n",
        "                logger.warning(f\"Rate limit hit for topic '{topic}'. Retrying after a delay...\")\n",
        "            else:\n",
        "                logger.error(f\"Error fetching Google Trends data for '{topic}': {e}\", exc_info=True)\n",
        "\n",
        "            # Implement backoff with random jitter\n",
        "            jitter = random.uniform(0, 1)\n",
        "            sleep_time = wait_time + jitter\n",
        "            logger.info(f\"Sleeping for {sleep_time:.2f} seconds before retrying...\")\n",
        "            time.sleep(sleep_time)  # Keep as time.sleep since it's in a separate thread\n",
        "            wait_time = min(wait_time * 2, 60)  # Exponential backoff up to 60 seconds\n",
        "            attempt += 1\n",
        "\n",
        "    logger.error(f\"Max retries exceeded for Google Trends data fetch for '{topic}'.\")\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "async def fetch_google_trends_async(topic, timeframe='now 7-d', max_retries=5):\n",
        "    \"\"\"\n",
        "    Asynchronously fetches Google Trends data with caching.\n",
        "    \"\"\"\n",
        "    if topic in trends_cache:\n",
        "        logger.info(f\"Using cached Google Trends data for {topic}\")\n",
        "        return trends_cache[topic]\n",
        "\n",
        "    # Fetch the data in a separate thread to prevent blocking\n",
        "    result = await asyncio.to_thread(fetch_google_trends, topic, timeframe, max_retries)\n",
        "\n",
        "    if result:\n",
        "        trends_cache[topic] = result  # Cache only successful results\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "# Handle rate limit error\n",
        "async def generate_summary_async(content):\n",
        "    \"\"\"\n",
        "    Generates a summary of the content using OpenAI's GPT with retry logic.\n",
        "    \"\"\"\n",
        "    if not content or content == \"Description not available.\":\n",
        "        logger.warning(\"Content is unavailable for summarization.\")\n",
        "        return \"Content unavailable for summarization.\"\n",
        "\n",
        "    # Ensure content length is reasonable for OpenAI API\n",
        "    max_length = 4000  # Approximate token limit for GPT models\n",
        "    if len(content) > max_length:\n",
        "        logger.warning(\"Content too long for summarization. Truncating content.\")\n",
        "        content = content[:max_length]\n",
        "\n",
        "    async def call_openai_api():\n",
        "        async with openai_semaphore:\n",
        "            response = await openai.ChatCompletion.acreate(\n",
        "                model=\"gpt-4\",\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes text.\"},\n",
        "                    {\"role\": \"user\", \"content\": f\"Summarize the following text: {content}\"}\n",
        "                ],\n",
        "                max_tokens=500,\n",
        "                temperature=0.5,\n",
        "            )\n",
        "            return response.choices[0].message['content'].strip()\n",
        "\n",
        "    # Retry mechanism with backoff\n",
        "    summary = await retry_with_backoff(call_openai_api, max_retries=5, backoff_factor=2)\n",
        "\n",
        "    if not summary or len(summary) < 50:\n",
        "        return \"Failed to generate a meaningful summary.\"\n",
        "    return summary\n",
        "\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    \"\"\"\n",
        "    Analyzes the sentiment of the given text using NLTK's VADER.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to analyze.\n",
        "\n",
        "    Returns:\n",
        "        str: 'Positive', 'Neutral', or 'Negative'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not text:\n",
        "            return 'Neutral'\n",
        "        text = str(text)  # Ensure text is a string\n",
        "        scores = sid.polarity_scores(text)\n",
        "        compound_score = scores['compound']\n",
        "        if compound_score >= 0.05:\n",
        "            return 'Positive'\n",
        "        elif compound_score <= -0.05:\n",
        "            return 'Negative'\n",
        "        else:\n",
        "            return 'Neutral'\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in analyze_sentiment: {e}\", exc_info=True)\n",
        "        return 'Neutral'\n",
        "        logger.debug(f\"Analyzing sentiment for text: {text}\")\n",
        "\n",
        "\n",
        "def map_topic_to_trends_query(topic_title):\n",
        "    \"\"\"\n",
        "    Maps a topic title to a Google Trends query.\n",
        "\n",
        "    Parameters:\n",
        "        topic_title (str): The title of the topic.\n",
        "\n",
        "    Returns:\n",
        "        str: The mapped query.\n",
        "    \"\"\"\n",
        "    # Use RAKE or predefined mappings for broader queries\n",
        "    if len(topic_title.split()) > 5:\n",
        "        logger.info(\"Broadening topic title for Google Trends query.\")\n",
        "        keywords = extract_keywords(topic_title)\n",
        "        return ' '.join(keywords[:3])  # Use top 3 keywords\n",
        "    return topic_title\n",
        "\n",
        "    # Placeholder for mapping logic\n",
        "    return topic_title\n",
        "\n",
        "def extract_keywords(topic_title):\n",
        "    \"\"\"\n",
        "    Extracts keywords from the topic title using RAKE.\n",
        "\n",
        "    Parameters:\n",
        "        topic_title (str): The title of the topic.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of extracted keywords.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        rake.extract_keywords_from_text(topic_title)\n",
        "        keywords = rake.get_ranked_phrases()\n",
        "        if not keywords:\n",
        "            logger.warning(f\"No keywords extracted from topic: {topic_title}\")\n",
        "            return topic_title.split()[:3]  # Fallback to splitting\n",
        "        return keywords\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting keywords: {e}\", exc_info=True)\n",
        "        return topic_title.split()[:3]  # Fallback to splitting\n",
        "\n",
        "\n",
        "def fetch_newsapi_articles(topic, page_size=5, language='en', sort_by='relevancy'):\n",
        "    \"\"\"\n",
        "    Fetches articles from NewsAPI for a given topic.\n",
        "\n",
        "    Parameters:\n",
        "        topic (str): The topic to search articles for.\n",
        "        page_size (int): Number of articles to fetch.\n",
        "        language (str): Language of the articles.\n",
        "        sort_by (str): Sorting criterion (e.g., relevancy, popularity).\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: A list of articles.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.info(f\"Fetching articles from NewsAPI for topic: {topic}\")\n",
        "        all_articles = newsapi.get_everything(\n",
        "            q=topic,\n",
        "            language=language,\n",
        "            sort_by=sort_by,\n",
        "            page_size=page_size\n",
        "        )\n",
        "        if all_articles.get('status') != 'ok':\n",
        "            raise Exception(f\"NewsAPI Error: {all_articles.get('message', 'Unknown error')}\")\n",
        "        return all_articles.get('articles', [])\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching articles from NewsAPI: {e}\", exc_info=True)\n",
        "        return []\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Trend Data Classes\n",
        "# ----------------------------\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional\n",
        "\n",
        "@dataclass\n",
        "class GoogleTrend:\n",
        "    topic: str\n",
        "    interest: float\n",
        "    sentiment: str\n",
        "\n",
        "@dataclass\n",
        "class Trend:\n",
        "    title: str\n",
        "    description: str\n",
        "    source: str\n",
        "    approx_traffic: str\n",
        "    sentiment: str\n",
        "    google_trend: Optional[GoogleTrend] = None\n",
        "\n",
        "@dataclass\n",
        "class ScriptOptions:\n",
        "    style: str = \"Normal\"\n",
        "    tone: str = \"Informative\"\n",
        "    length: str = \"60 seconds\"\n",
        "\n",
        "# ----------------------------\n",
        "# Fetch Trending Topics Function\n",
        "# ----------------------------\n",
        "\n",
        "def fetch_full_article_content(url):\n",
        "    \"\"\"\n",
        "    Fetches the full article content from the given URL.\n",
        "\n",
        "    Parameters:\n",
        "        url (str): The URL of the article.\n",
        "\n",
        "    Returns:\n",
        "        str: The full article content or a clear message if unavailable.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        logger.debug(f\"Attempting to fetch article content from URL: {url}\")\n",
        "\n",
        "        # Handle redirected URLs (e.g., Google News, Yahoo News)\n",
        "        resolved_url = resolve_actual_article_url(url)\n",
        "        logger.debug(f\"Resolved URL: {resolved_url}\")\n",
        "\n",
        "        article = Article(resolved_url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "\n",
        "        # Check for sufficient content\n",
        "        if not article.text or len(article.text) < 100:\n",
        "            logger.warning({\n",
        "                \"event\": \"Insufficient content in article\",\n",
        "                \"url\": resolved_url,\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            return \"Content unavailable due to an error.\"\n",
        "\n",
        "        return article.text\n",
        "    except Exception as e:\n",
        "        logger.error({\n",
        "            \"event\": \"Failed to fetch article content\",\n",
        "            \"url\": url,\n",
        "            \"error\": str(e),\n",
        "            \"timestamp\": datetime.utcnow().isoformat()\n",
        "        })\n",
        "        return \"Content unavailable due to an error.\"\n",
        "def validate_rss_feed(rss_url):\n",
        "    \"\"\"\n",
        "    Validates the RSS feed URL.\n",
        "\n",
        "    Parameters:\n",
        "        rss_url (str): The RSS feed URL to validate.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the feed is valid, False otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(rss_url, timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            return True\n",
        "        logger.warning(f\"Invalid RSS feed URL: {rss_url} - Status Code: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error validating RSS feed URL {rss_url}: {e}\", exc_info=True)\n",
        "    return False\n",
        "\n",
        "rss_feeds = [\n",
        "    url for url in rss_feeds if validate_rss_feed(url)\n",
        "]\n",
        "\n",
        "async def fetch_trending_topics_rss_async(geo='US', limit=10):\n",
        "    \"\"\"\n",
        "    Fetches trending topics from multiple RSS feeds asynchronously with caching.\n",
        "\n",
        "    Parameters:\n",
        "        geo (str): Geographical region for fetching RSS feeds.\n",
        "        limit (int): Number of entries to fetch per RSS feed.\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: List of parsed RSS feed entries.\n",
        "\n",
        "    \"\"\"\n",
        "    rss_feeds = [\n",
        "        f\"https://trends.google.com/trends/trendingsearches/daily/rss?geo={geo}\",\n",
        "        f\"https://news.google.com/rss?hl=en-{geo}&gl={geo}&ceid={geo}:en\",\n",
        "        \"http://rss.cnn.com/rss/edition.rss\",\n",
        "        \"http://news.yahoo.com/rss/\",\n",
        "        \"https://www.theguardian.com/uk/rss\",\n",
        "        \"https://news.un.org/feed/subscribe/en/news/all/rss.xml\"\n",
        "    ]\n",
        "\n",
        "    trending_topics = []  # Initialize the list to store trending topics\n",
        "\n",
        "    # Asynchronously fetch all feeds with caching\n",
        "    tasks = [fetch_and_cache_rss_feed(rss_url, limit) for rss_url in rss_feeds]\n",
        "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "    for rss_url, result in zip(rss_feeds, results):\n",
        "        if isinstance(result, Exception):\n",
        "            logger.error(f\"Error fetching RSS feed {rss_url}: {result}\")\n",
        "            continue  # Skip feeds that failed to fetch\n",
        "\n",
        "for entry in result:\n",
        "    try:\n",
        "        title = entry.get('title', 'No Title')\n",
        "        link = entry.get('link', '')\n",
        "\n",
        "        # Resolve and fetch article content\n",
        "        resolved_link = resolve_actual_article_url(link)\n",
        "        full_content = await asyncio.to_thread(fetch_full_article_content, resolved_link)\n",
        "\n",
        "        # Use the full content or fallback to the summary\n",
        "        description = (\n",
        "            full_content if full_content != \"Content unavailable due to an error.\"\n",
        "            else clean_description(entry.get('summary', 'No description available.'))\n",
        "        )\n",
        "\n",
        "        # Append parsed data to trending_topics\n",
        "        trending_topics.append({\n",
        "            'title': title,\n",
        "            'description': description,\n",
        "            'source': extract_source(link),\n",
        "            'link': link,\n",
        "            'approx_traffic': entry.get('ht:approx_traffic', 'N/A'),\n",
        "            'sentiment': 'Neutral',\n",
        "        })\n",
        "    except Exception as e:\n",
        "        logger.error({\n",
        "            \"event\": \"Error processing entry\",\n",
        "            \"rss_url\": rss_url,\n",
        "            \"error\": str(e),\n",
        "            \"timestamp\": datetime.utcnow().isoformat()\n",
        "        })\n",
        "        continue  # Skip this entry\n",
        "\n",
        "\n",
        "    return trending_topics\n",
        "\n",
        "\n",
        "\n",
        "def resolve_actual_article_url(url):\n",
        "    \"\"\"\n",
        "    Resolves the actual article URL if the given URL is a redirect or contains a URL parameter.\n",
        "\n",
        "    Parameters:\n",
        "        url (str): The URL to resolve.\n",
        "\n",
        "    Returns:\n",
        "        str: The resolved actual article URL.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if url is a redirect link (e.g., Google News, Yahoo News)\n",
        "        if 'news.google.com' in url or 'news.url.google.com' in url or 'news.yahoo.com' in url:\n",
        "            parsed_url = urlparse(url)\n",
        "            query_params = parse_qs(parsed_url.query)\n",
        "            if 'url' in query_params:\n",
        "                actual_url = query_params['url'][0]\n",
        "                logger.debug(f\"Resolved actual article URL: {actual_url}\")\n",
        "                return actual_url\n",
        "        return url\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error resolving actual article URL from '{url}': {e}\", exc_info=True)\n",
        "        return url\n",
        "\n",
        "\n",
        "\n",
        "# Remove or comment out the old fetch_rss_feed_async function\n",
        "# async def fetch_rss_feed_async(rss_url, limit):\n",
        "#     # ... (old code)\n",
        "PAYWALLED_DOMAINS = {\n",
        "    'nytimes.com',\n",
        "    'washingtonpost.com',\n",
        "    'wsj.com',\n",
        "    'financialtimes.com',\n",
        "    # Add more domains as needed\n",
        "}\n",
        "\n",
        "def fetch_full_article_content(url):\n",
        "    \"\"\"\n",
        "    Fetches the full article content from the given URL, with paywall detection.\n",
        "\n",
        "    Parameters:\n",
        "        url (str): The URL of the article.\n",
        "\n",
        "    Returns:\n",
        "        str: The full article content or a clear message if unavailable.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        domain = urlparse(url).netloc\n",
        "        if domain in PAYWALLED_DOMAINS:\n",
        "            logger.warning(f\"Paywalled article detected: {url}\")\n",
        "            return \"Content unavailable due to paywall restrictions.\"\n",
        "\n",
        "        article = Article(url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        if not article.text or len(article.text) < 100:\n",
        "            logger.warning(f\"Insufficient content in article at {url}\")\n",
        "            return \"Content unavailable due to an error.\"\n",
        "        return article.text\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to fetch article content from {url}: {e}\", exc_info=True)\n",
        "        return \"Content unavailable due to an error.\"\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Aggregate Trends Data Function\n",
        "# ----------------------------\n",
        "\n",
        "async def aggregate_trends_data(rss_trends, selected_timeframe_hours):\n",
        "    \"\"\"\n",
        "    Aggregates data for each trend by fetching NewsAPI articles and Google Trends data.\n",
        "    \"\"\"\n",
        "    aggregated_trends = []\n",
        "    now = datetime.now(timezone.utc)\n",
        "\n",
        "    for trend in rss_trends:\n",
        "        time_diff = now - trend['published']\n",
        "        if time_diff.total_seconds() > selected_timeframe_hours * 3600:\n",
        "            logger.debug(f\"Skipping topic '{trend['title']}' due to timeframe constraints.\")\n",
        "            continue  # Skip trends outside the time frame\n",
        "\n",
        "        topic_title = trend['title']\n",
        "        link = trend['link']\n",
        "        source = trend['source']\n",
        "\n",
        "        logger.debug(f\"Processing topic: {topic_title}\")\n",
        "\n",
        "        # Resolve the actual article URL\n",
        "        resolved_link = resolve_actual_article_url(link)\n",
        "        logger.debug(f\"Resolved link for topic '{topic_title}': {resolved_link}\")\n",
        "\n",
        "        # Fetch full article content\n",
        "        full_content = await asyncio.to_thread(fetch_full_article_content, resolved_link)\n",
        "\n",
        "        # Replace the description with the full content if available\n",
        "        if full_content and full_content != \"Description not available.\":\n",
        "            description = full_content\n",
        "            logger.debug(f\"Replaced description for '{topic_title}' with full article content.\")\n",
        "        else:\n",
        "            # Use a clear fallback message instead of generic RSS summary\n",
        "            description = \"Description not available.\"\n",
        "            logger.debug(f\"Set description for '{topic_title}' as 'Description not available.'\")\n",
        "\n",
        "        # Fetch Google Trends data with semaphore to limit concurrency\n",
        "        async with google_trends_semaphore:\n",
        "            google_trend_data = await fetch_google_trends_async(topic_title, timeframe='now 7-d')\n",
        "\n",
        "        # Aggregate data\n",
        "        google_trend = GoogleTrend(**google_trend_data) if google_trend_data else None\n",
        "        approx_traffic = str(google_trend.interest) if google_trend else 'Unavailable'\n",
        "\n",
        "        # Generate summary using OpenAI\n",
        "        summary = await generate_summary_async(description)\n",
        "        sentiment = analyze_sentiment(summary)\n",
        "\n",
        "        # Append the aggregated trend\n",
        "        aggregated_trends.append(Trend(\n",
        "            title=topic_title,\n",
        "            description=summary,\n",
        "            source=source,\n",
        "            approx_traffic=approx_traffic,\n",
        "            sentiment=sentiment,\n",
        "            google_trend=google_trend\n",
        "        ))\n",
        "\n",
        "        logger.debug(f\"Aggregated data for topic '{topic_title}'.\")\n",
        "\n",
        "        if len(aggregated_trends) >= 10:\n",
        "            logger.debug(\"Reached maximum number of aggregated trends (10).\")\n",
        "            break  # Limit to 10 trends\n",
        "\n",
        "    logger.info(f\"Aggregated {len(aggregated_trends)} trending topics.\")\n",
        "    return aggregated_trends\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Define fetch_and_aggregate_trending_data Function\n",
        "# ----------------------------\n",
        "\n",
        "async def fetch_and_aggregate_trending_data(country_code, timeframe_hours):\n",
        "    \"\"\"\n",
        "    Fetches trending topics and aggregates data.\n",
        "\n",
        "    Parameters:\n",
        "        country_code (str): The country code for fetching trends.\n",
        "        timeframe_hours (int): The timeframe in hours to filter trends.\n",
        "\n",
        "    Returns:\n",
        "        List[Trend]: A list of aggregated Trend objects.\n",
        "    \"\"\"\n",
        "    rss_trends = await fetch_trending_topics_rss_async(geo=country_code, limit=20)\n",
        "    aggregated_trends = await aggregate_trends_data(rss_trends, timeframe_hours)\n",
        "    return aggregated_trends\n",
        "\n",
        "# ----------------------------\n",
        "# Generate and Display Script Function\n",
        "# ----------------------------\n",
        "\n",
        "async def generate_and_display_script_async(selected_topic_data: Trend):\n",
        "    \"\"\"\n",
        "    Prompts the user for script customization options, generates the script,\n",
        "    and displays it.\n",
        "\n",
        "    Parameters:\n",
        "        selected_topic_data (Trend): The data associated with the selected topic.\n",
        "    \"\"\"\n",
        "    print(\"\\nYou can customize the script generation. If you wish to skip any option, just press Enter.\")\n",
        "\n",
        "    # Select script style\n",
        "    style_input = input(\"Select script style (Flashy / Expressive / Normal): \").strip()\n",
        "    style = style_input if style_input else \"Normal\"\n",
        "\n",
        "    # Select script tone\n",
        "    tone_input = input(\"Select script tone (e.g., Informative, Persuasive, Emotional): \").strip()\n",
        "    tone = tone_input if tone_input else \"Informative\"\n",
        "\n",
        "    # Enter script length\n",
        "    length_input = input(\"Enter script length in seconds (e.g., 60, 120): \").strip()\n",
        "    length = f\"{length_input} seconds\" if length_input.isdigit() else \"60 seconds\"\n",
        "\n",
        "    # Create ScriptOptions instance\n",
        "    options = ScriptOptions(\n",
        "        style=style,\n",
        "        tone=tone,\n",
        "        length=length\n",
        "    )\n",
        "\n",
        "    print(\"\\nSelected Options - Style: {0}, Tone: {1}, Length: {2}\".format(options.style, options.tone, options.length))\n",
        "\n",
        "    print(\"\\nGenerating script for the selected topic...\")\n",
        "    try:\n",
        "        script = await generate_script_for_topic_async(selected_topic_data.title, selected_topic_data, options=options)\n",
        "        print(\"Script generated successfully.\")\n",
        "    except Exception as e:\n",
        "        logger.error({\n",
        "            \"error\": str(e),\n",
        "            \"event\": \"generate_script_error\",\n",
        "            \"timestamp\": datetime.utcnow().isoformat()\n",
        "        })\n",
        "        script = \"Failed to generate script.\"\n",
        "\n",
        "    # Display the script\n",
        "    print(f\"\\n### Generated Script for '{selected_topic_data.title}':\\n\")\n",
        "    print(script)\n",
        "    print(\"\\n**Source:**\", selected_topic_data.source)\n",
        "    print(\"\\n====\\n\")\n",
        "\n",
        "    print(\"Script generation completed.\")\n",
        "\n",
        "async def generate_script_for_topic_async(title, trend_data, options: ScriptOptions):\n",
        "    \"\"\"\n",
        "    Generates a script based on the topic and options using OpenAI's GPT.\n",
        "\n",
        "    Parameters:\n",
        "        title (str): The title of the topic.\n",
        "        trend_data (Trend): The trend data.\n",
        "        options (ScriptOptions): User-selected script options.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated script.\n",
        "    \"\"\"\n",
        "    # Calculate an approximate max token count based on script length\n",
        "    words_per_minute = 150\n",
        "    desired_word_count = int(words_per_minute * (int(options.length.split()[0]) / 60))\n",
        "    max_tokens = max(int(desired_word_count * 1.5), 500)  # Ensure minimum of 500 tokens\n",
        "\n",
        "    # Construct the prompt for OpenAI\n",
        "    prompt = (\n",
        "        f\"Generate a {options.length} script for the topic '{title}' with a {options.tone} tone \"\n",
        "        f\"and a {options.style} style. The script should include the following key details extracted from the article:\\n\"\n",
        "        f\"{trend_data.description}\\n\\n\"\n",
        "        f\"Ensure the script is engaging and suitable for a {options.length} duration.\"\n",
        "    )\n",
        "\n",
        "    attempt = 0\n",
        "    max_retries = 5\n",
        "    wait_time = 1  # Initial wait time in seconds\n",
        "\n",
        "    while attempt < max_retries:\n",
        "        try:\n",
        "            async with openai_semaphore:\n",
        "                response = await openai.ChatCompletion.acreate(\n",
        "                    model=\"gpt-4\",\n",
        "                    messages=[\n",
        "                        {\n",
        "                            \"role\": \"system\",\n",
        "                            \"content\": \"You are a creative scriptwriter that generates scripts based on provided data.\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": prompt\n",
        "                        }\n",
        "                    ],\n",
        "                    max_tokens=max_tokens,  # Dynamically set max tokens\n",
        "                    temperature=0.7,\n",
        "                )\n",
        "            script = response.choices[0].message['content'].strip()\n",
        "            return script\n",
        "        except openai.error.RateLimitError as e:\n",
        "            # Handle rate limit error\n",
        "            logger.error({\n",
        "                \"event\": \"RateLimitError in generate_script_for_topic_async\",\n",
        "                \"error\": str(e),\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            match = re.search(r\"Please try again in ([\\d\\.]+)s\", str(e))\n",
        "            if match:\n",
        "                wait_time = float(match.group(1))\n",
        "            else:\n",
        "                wait_time = min(wait_time * 2, 60)  # Exponential backoff\n",
        "            logger.info(f\"Rate limit exceeded. Waiting for {wait_time} seconds before retrying...\")\n",
        "            await asyncio.sleep(wait_time)\n",
        "            attempt += 1\n",
        "        except Exception as e:\n",
        "            logger.error({\n",
        "                \"event\": \"Error generating script_async\",\n",
        "                \"error\": str(e),\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            return \"Failed to generate script due to an unexpected error.\"\n",
        "\n",
        "    logger.error(\"Max retries exceeded for generate_script_for_topic_async. Returning failure message.\")\n",
        "    return \"Failed to generate script after multiple attempts due to rate limits.\"\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Main Function\n",
        "# ----------------------------\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to execute the script workflow.\n",
        "    \"\"\"\n",
        "    async def run():\n",
        "        # Function to display a changing message every 5 seconds with an additional note\n",
        "        def flashing_message(stop_event):\n",
        "            messages = [\n",
        "                \"üîç Gathering the latest trends... (This could take up to 2 minutes. Please wait.)\",\n",
        "                \"‚è≥ Processing data, please wait... (This could take up to 2 minutes. Please wait.)\",\n",
        "                \"‚ú® Almost there, thank you for your patience! (This could take up to 2 minutes. Please wait.)\"\n",
        "            ]\n",
        "            idx = 0\n",
        "            while not stop_event.is_set():\n",
        "                message = messages[idx % len(messages)]\n",
        "                print(f\"\\r{message}   \", end='', flush=True)\n",
        "                for _ in range(5):\n",
        "                    if stop_event.is_set():\n",
        "                        break\n",
        "                    time.sleep(1)\n",
        "                idx += 1\n",
        "                print('\\r' + ' ' * len(message) + '   ', end='', flush=True)\n",
        "\n",
        "        while True:\n",
        "            print(\"Enter the country for trending topics data (e.g., United States or US):\")\n",
        "            country_input = input(\"Country: \").strip()\n",
        "            matching_country = get_matching_country(country_input, available_countries)\n",
        "            if not matching_country:\n",
        "                print(\"No matching countries found. Please try again.\")\n",
        "                # Optionally, display available countries\n",
        "                print(\"Available countries are:\")\n",
        "                for country in available_countries.keys():\n",
        "                    print(f\"- {country}\")\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        selected_country = matching_country\n",
        "        # Retrieve all possible codes for the selected country\n",
        "        selected_country_codes = available_countries[selected_country]\n",
        "        selected_country_code = selected_country_codes[0]  # Use the first code by default\n",
        "        print(f\"\\nYou selected: {selected_country}\")\n",
        "\n",
        "        print(\"\\nSelect the time frame for trending topics:\")\n",
        "        print(\"A. Last 4 hours\")\n",
        "        print(\"B. Last 24 hours\")\n",
        "        print(\"C. Last 7 days\")\n",
        "        time_range_selection = input(\"Enter the letter of the time frame you're interested in: \").strip().upper()\n",
        "        time_range_mapping = {'A': 4, 'B': 24, 'C': 168}\n",
        "        selected_timeframe_hours = time_range_mapping.get(time_range_selection)\n",
        "        if not selected_timeframe_hours:\n",
        "            print(\"Invalid selection. Exiting.\")\n",
        "            return\n",
        "\n",
        "        # Start flashing message while fetching all data\n",
        "        stop_event = threading.Event()\n",
        "        thread = threading.Thread(target=flashing_message, args=(stop_event,))\n",
        "        thread.start()\n",
        "\n",
        "        # Fetch all data asynchronously with robust error handling\n",
        "        try:\n",
        "            # Use the updated fetch_trending_topics_rss_async function\n",
        "            rss_trends = await fetch_trending_topics_rss_async(geo=selected_country_code, limit=20)\n",
        "            aggregated_trends = await aggregate_trends_data(rss_trends, selected_timeframe_hours)\n",
        "        except Exception as e:\n",
        "            logger.error({\n",
        "                \"error\": str(e),\n",
        "                \"event\": \"fetch_data_error\",\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            aggregated_trends = []\n",
        "\n",
        "        # Stop flashing message\n",
        "        stop_event.set()\n",
        "        thread.join()\n",
        "        print()  # Move to the next line after flashing message\n",
        "\n",
        "        if not aggregated_trends:\n",
        "            logger.error({\n",
        "                \"event\": \"no_trending_topics_found\",\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            print(\"No trending topics found.\")\n",
        "            return\n",
        "\n",
        "        # Display trending topics with pagination\n",
        "        # Pagination variables\n",
        "        batch_size = 10\n",
        "        total_trends = len(aggregated_trends)\n",
        "        current_index = 0\n",
        "\n",
        "        while current_index < total_trends:\n",
        "            # Determine the end index for the current batch\n",
        "            end_index = min(current_index + batch_size, total_trends)\n",
        "            batch_trends = aggregated_trends[current_index:end_index]\n",
        "\n",
        "            # Display the consolidated list with sentiments\n",
        "            print(f\"\\nCurrently Trending in {selected_country} in the last {selected_timeframe_hours} hours (Showing {current_index + 1} to {end_index} of {total_trends}):\\n\")\n",
        "            table = PrettyTable()\n",
        "            table.field_names = [\"No.\", \"Topic\", \"Description\", \"Source\", \"Approx Traffic\", \"Sentiment\"]\n",
        "            table.hrules = HRuleStyle.ALL  # Use HRuleStyle.ALL\n",
        "            table.max_width = 40  # Suitable for phone screens\n",
        "            for idx, trend in enumerate(batch_trends, start=current_index + 1):\n",
        "                title = textwrap.fill(trend.title, width=40)\n",
        "                description = textwrap.fill(trend.description, width=40)\n",
        "                source = trend.source\n",
        "                approx_traffic = trend.approx_traffic\n",
        "                sentiment = trend.sentiment\n",
        "                table.add_row([idx, title, description, source, approx_traffic, sentiment])\n",
        "            print(table)\n",
        "\n",
        "            # Prepare to handle user input for pagination or selection\n",
        "            while True:\n",
        "                user_input = input(\"Type 'more' or '+' to view more results, enter the number of the topic to select it, or any other key to exit: \").strip().lower()\n",
        "                if user_input in ['more', '+']:\n",
        "                    current_index = end_index\n",
        "                    break  # Continue to the next batch\n",
        "                elif user_input.isdigit():\n",
        "                    selected_idx = int(user_input)\n",
        "                    if 1 <= selected_idx <= total_trends:\n",
        "                        selected_topic_data = aggregated_trends[selected_idx - 1]\n",
        "                        selected_topic = selected_topic_data.title\n",
        "                        # Apply text wrapping to the selected topic message\n",
        "                        selected_topic_display = textwrap.fill(f\"You selected: {selected_topic}\", width=40)\n",
        "                        print(f\"\\n{selected_topic_display}\")\n",
        "                        # Generate scripts for the selected topic\n",
        "                        await generate_and_display_script_async(selected_topic_data)\n",
        "                        return  # Exit after script generation\n",
        "                    else:\n",
        "                        print(\"Invalid selection. Please enter a valid topic number.\")\n",
        "                else:\n",
        "                    print(\"Exiting the script.\")\n",
        "                    return\n",
        "\n",
        "        print(\"====\\nNo more trending topics available.\\n\")\n",
        "\n",
        "        # After all batches are displayed, prompt the user to select a topic\n",
        "        while True:\n",
        "            try:\n",
        "                selected_idx = input(\"Enter the number of the topic you're interested in (or type 0 to exit): \").strip()\n",
        "                if selected_idx == '0':\n",
        "                    print(\"Exiting the script.\")\n",
        "                    return\n",
        "                if selected_idx.isdigit():\n",
        "                    selected_idx = int(selected_idx)\n",
        "                    if 1 <= selected_idx <= total_trends:\n",
        "                        selected_topic_data = aggregated_trends[selected_idx - 1]\n",
        "                        selected_topic = selected_topic_data.title\n",
        "                        # Apply text wrapping to the selected topic message\n",
        "                        selected_topic_display = textwrap.fill(f\"You selected: {selected_topic}\", width=40)\n",
        "                        print(f\"\\n{selected_topic_display}\")\n",
        "                        # Generate scripts for the selected topic\n",
        "                        await generate_and_display_script_async(selected_topic_data)\n",
        "                        return  # Exit after script generation\n",
        "                    else:\n",
        "                        print(\"Invalid selection. Please enter a valid topic number.\")\n",
        "                else:\n",
        "                    print(\"Invalid input. Please enter a number.\")\n",
        "            except ValueError:\n",
        "                print(\"Invalid input. Please enter a valid number.\")\n",
        "\n",
        "    asyncio.run(run())\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1s-3u0czUndpxCkgirEnqndEDGpEPjpJ_",
      "authorship_tag": "ABX9TyPGJlE+rRUDLeI6iV/5CJJD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}