{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kepners/ChopOnions/blob/main/ChoppingOnions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Install Required Packages\n",
        "\n",
        "!pip install --upgrade openai aiohttp nest_asyncio structlog cachetools fuzzywuzzy python-Levenshtein nltk pytrends ratelimit prettytable\n",
        "\n",
        "# Import necessary packages after installation\n",
        "import openai\n",
        "import aiohttp\n",
        "import nest_asyncio\n",
        "import structlog\n",
        "import cachetools\n",
        "import fuzzywuzzy\n",
        "import nltk\n",
        "import pytrends\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "from prettytable import PrettyTable, ALL\n",
        "\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "RWAbuL6NbeeL",
        "outputId": "8d84c6d7-b6f7-46f8-e537-69bcb446ddc4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.52.2)\n",
            "Collecting openai\n",
            "  Downloading openai-1.54.3-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.10.10)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: structlog in /usr/local/lib/python3.10/dist-packages (24.4.0)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (5.5.0)\n",
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.10/dist-packages (0.26.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: pytrends in /usr/local/lib/python3.10/dist-packages (4.9.2)\n",
            "Requirement already satisfied: ratelimit in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (3.11.0)\n",
            "Collecting prettytable\n",
            "  Downloading prettytable-3.12.0-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (4.0.3)\n",
            "Requirement already satisfied: Levenshtein==0.26.1 in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein) (0.26.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein==0.26.1->python-Levenshtein) (3.10.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.10/dist-packages (from pytrends) (2.32.3)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from pytrends) (2.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pytrends) (5.3.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable) (0.2.13)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (2.2.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp) (0.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25->pytrends) (1.16.0)\n",
            "Downloading openai-1.54.3-py3-none-any.whl (389 kB)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prettytable-3.12.0-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: prettytable, nltk, openai\n",
            "  Attempting uninstall: prettytable\n",
            "    Found existing installation: prettytable 3.11.0\n",
            "    Uninstalling prettytable-3.11.0:\n",
            "      Successfully uninstalled prettytable-3.11.0\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.52.2\n",
            "    Uninstalling openai-1.52.2:\n",
            "      Successfully uninstalled openai-1.52.2\n",
            "Successfully installed nltk-3.9.1 openai-1.54.3 prettytable-3.12.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk"
                ]
              },
              "id": "c6da2c2b67ff40b19012bd6c14f822e8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MAUVB0guTX2b",
        "outputId": "0080d6d5-3ba2-4ff6-bfb5-338eb60027f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please restart the runtime to apply package installations and config changes.\n"
          ]
        }
      ],
      "source": [
        "# Block 2: Restart Runtime\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "print(\"Please restart the runtime to apply package installations and config changes.\")\n",
        "\n",
        "# Function to restart the runtime\n",
        "def restart_runtime():\n",
        "    os.kill(os.getpid(), 9)\n",
        "\n",
        "# Uncomment the following line to restart the runtime automatically\n",
        "# restart_runtime()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIud78FOZR3M",
        "outputId": "0f682aa1-9c2d-4e4d-eeae-f9f531d0dc43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Google Drive mounted successfully.\n"
          ]
        }
      ],
      "source": [
        "# Block 3: Mount Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive to access files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"Google Drive mounted successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ujJviMETtuLY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e27340ae-4a58-4b77-85d3-b59e500c5fdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK data downloaded successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Block 4: Download NLTK Data\n",
        "\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt')          # Required by rake_nltk\n",
        "nltk.download('stopwords')      # Required by rake_nltk\n",
        "\n",
        "print(\"NLTK data downloaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wGPK1IRBXfBm"
      },
      "outputs": [],
      "source": [
        "# Block 5: Define Data Structures\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional\n",
        "\n",
        "@dataclass\n",
        "class GoogleTrend:\n",
        "    topic: str\n",
        "    interest: str\n",
        "    sentiment: str\n",
        "\n",
        "@dataclass\n",
        "class RedditPost:\n",
        "    type: str\n",
        "    title: str\n",
        "    summary: str\n",
        "    source: str\n",
        "    approx_traffic: str\n",
        "    sentiment: str\n",
        "\n",
        "@dataclass\n",
        "class Trend:\n",
        "    title: str\n",
        "    description: str\n",
        "    source: str\n",
        "    approx_traffic: str\n",
        "    sentiment: str\n",
        "    reddit_posts: List[RedditPost] = field(default_factory=list)\n",
        "    google_trend: Optional[GoogleTrend] = None\n",
        "\n",
        "@dataclass\n",
        "class ScriptOptions:\n",
        "    style: str = \"Normal Script\"\n",
        "    tone: str = \"Informative\"\n",
        "    length: str = \"60 seconds\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": true,
        "id": "RPd0MOuGTaTM"
      },
      "outputs": [],
      "source": [
        "# Block 6: Configuration and Constants\n",
        "\n",
        "import configparser\n",
        "import os\n",
        "\n",
        "# Define caching constants\n",
        "TREND_CACHE_TTL = 3600       # 1 hour in seconds\n",
        "OPENAI_CACHE_TTL = 86400     # 1 day in seconds\n",
        "\n",
        "# Define rate limiting constants\n",
        "RSS_CALLS_PER_DAY = 100       # Maximum number of RSS feed calls per day\n",
        "ONE_DAY = 86400               # Seconds in one day\n",
        "\n",
        "# Initialize External Clients and API Keys\n",
        "\n",
        "# Path to config.ini in Google Drive\n",
        "config_path = '/content/drive/MyDrive/Secrets/config.ini'  # Adjust the path as needed\n",
        "\n",
        "# Check if config.ini exists\n",
        "if not os.path.exists(config_path):\n",
        "    print(f\"Configuration file not found at {config_path}. Please create it with the following format:\")\n",
        "    print(\"\"\"\n",
        "[openai]\n",
        "api_key=YOUR_OPENAI_API_KEY\n",
        "\n",
        "[reddit]\n",
        "client_id=YOUR_REDDIT_CLIENT_ID\n",
        "client_secret=YOUR_REDDIT_CLIENT_SECRET\n",
        "user_agent=YOUR_REDDIT_USER_AGENT\n",
        "\"\"\")\n",
        "    raise FileNotFoundError(f\"No config.ini found at {config_path}\")\n",
        "\n",
        "# Load configuration from config.ini\n",
        "config = configparser.ConfigParser()\n",
        "config.read(config_path)\n",
        "\n",
        "# Assign API keys and credentials\n",
        "try:\n",
        "    OPENAI_API_KEY = config.get('openai', 'api_key')\n",
        "    REDDIT_CLIENT_ID = config.get('reddit', 'client_id')\n",
        "    REDDIT_CLIENT_SECRET = config.get('reddit', 'client_secret')\n",
        "    REDDIT_USER_AGENT = config.get('reddit', 'user_agent')\n",
        "except configparser.NoSectionError as e:\n",
        "    print(f\"Configuration error: {e}\")\n",
        "    raise\n",
        "except configparser.NoOptionError as e:\n",
        "    print(f\"Configuration error: {e}\")\n",
        "    raise\n",
        "\n",
        "import openai\n",
        "import praw\n",
        "from pytrends.request import TrendReq\n",
        "\n",
        "# Initialize OpenAI\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# Initialize Reddit\n",
        "reddit = praw.Reddit(\n",
        "    client_id=REDDIT_CLIENT_ID,\n",
        "    client_secret=REDDIT_CLIENT_SECRET,\n",
        "    user_agent=REDDIT_USER_AGENT,\n",
        ")\n",
        "\n",
        "# Initialize pytrends\n",
        "pytrends = TrendReq(hl='en-US', tz=360)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(f\"Current Working Directory: {os.getcwd()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLVzZkCSYp0c",
        "outputId": "70fb9ddc-84e6-4cef-e870-535a0d1119b2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "C5RZdBozr4hJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5799f18e-290f-461d-bdfe-030f4a5cfe35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'__init__.py' already exists at 'utils/__init__.py'.\n",
            "Created 'utils/data_processing.py' successfully.\n",
            "\n",
            "Verifying the creation of 'data_processing.py' and '__init__.py':\n",
            "total 8\n",
            "-rw-r--r-- 1 root root 1598 Nov  7 11:42 data_processing.py\n",
            "-rw-r--r-- 1 root root    0 Nov  7 10:41 __init__.py\n",
            "drwxr-xr-x 2 root root 4096 Nov  7 10:42 __pycache__\n"
          ]
        }
      ],
      "source": [
        "# Block 8: Create utils/data_processing.py and utils/__init__.py\n",
        "\n",
        "import os\n",
        "\n",
        "# Define the directory where helper functions will reside\n",
        "utils_dir = 'utils'\n",
        "\n",
        "# Create the 'utils' directory if it doesn't exist\n",
        "os.makedirs(utils_dir, exist_ok=True)\n",
        "\n",
        "# Define the path for the __init__.py file to make 'utils' a package\n",
        "init_path = os.path.join(utils_dir, '__init__.py')\n",
        "\n",
        "# Create an empty __init__.py file if it doesn't exist\n",
        "if not os.path.exists(init_path):\n",
        "    with open(init_path, 'w') as file:\n",
        "        pass  # Creating an empty __init__.py\n",
        "    print(f\"Created empty '__init__.py' at '{init_path}' to make 'utils' a package.\")\n",
        "else:\n",
        "    print(f\"'__init__.py' already exists at '{init_path}'.\")\n",
        "\n",
        "# Define the path for the data_processing.py file\n",
        "data_processing_path = os.path.join(utils_dir, 'data_processing.py')\n",
        "\n",
        "# Define the content for data_processing.py\n",
        "data_processing_code = \"\"\"\n",
        "# data_processing.py\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def clean_text(text):\n",
        "    '''\n",
        "    Cleans the input text by removing URLs, special characters, and stopwords.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to clean.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned text.\n",
        "    '''\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\\\S+', '', text)\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^A-Za-z\\\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    # Join the words back into a single string\n",
        "    cleaned_text = ' '.join(filtered_words)\n",
        "    return cleaned_text\n",
        "\n",
        "def extract_entities(text):\n",
        "    '''\n",
        "    Extracts named entities from the input text.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to extract entities from.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of named entities.\n",
        "    '''\n",
        "    import nltk\n",
        "    from nltk import ne_chunk, pos_tag, word_tokenize\n",
        "    from nltk.tree import Tree\n",
        "\n",
        "    def get_entities(tree):\n",
        "        entities = []\n",
        "        for subtree in tree:\n",
        "            if isinstance(subtree, Tree):\n",
        "                entity = \" \".join([token for token, pos in subtree.leaves()])\n",
        "                entities.append(entity)\n",
        "        return entities\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = pos_tag(tokens)\n",
        "    chunked = ne_chunk(tagged)\n",
        "    entities = get_entities(chunked)\n",
        "    return entities\n",
        "\"\"\"\n",
        "\n",
        "# Write the data_processing.py file\n",
        "with open(data_processing_path, 'w') as file:\n",
        "    file.write(data_processing_code)\n",
        "\n",
        "print(f\"Created '{data_processing_path}' successfully.\")\n",
        "\n",
        "# Optional: Verify the creation by listing the 'utils' directory\n",
        "print(\"\\nVerifying the creation of 'data_processing.py' and '__init__.py':\")\n",
        "!ls -l utils/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bNicEhLDr06j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "23309e54-501c-43bc-d015-7d13c904ba5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully imported 'clean_text' and 'extract_entities' from 'utils.data_processing'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-4aeba86fc0d4>:14: DeprecationWarning: the 'ALL' constant is deprecated, use the 'HRuleStyle' and 'VRuleStyle' enums instead\n",
            "  from prettytable import PrettyTable, ALL\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sleep_and_retry' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4aeba86fc0d4>\u001b[0m in \u001b[0;36m<cell line: 66>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mRSS_CALLS_PER_DAY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m       \u001b[0;31m# Maximum number of RSS feed calls per day\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0msleep_and_retry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mlimits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRSS_CALLS_PER_DAY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mONE_DAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfetch_rss_feed_sync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrss_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sleep_and_retry' is not defined"
          ]
        }
      ],
      "source": [
        "# Block 7: Define Helper Functions\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import structlog\n",
        "from cachetools import TTLCache, cached\n",
        "from fuzzywuzzy import process\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from rake_nltk import Rake\n",
        "from textblob import TextBlob\n",
        "import feedparser\n",
        "from prettytable import PrettyTable, ALL\n",
        "import time\n",
        "import re\n",
        "import openai  # Ensure openai is imported\n",
        "\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Import helper functions from data_processing.py\n",
        "try:\n",
        "    from utils.data_processing import clean_text, extract_entities\n",
        "    print(\"Successfully imported 'clean_text' and 'extract_entities' from 'utils.data_processing'.\")\n",
        "except ModuleNotFoundError as e:\n",
        "    print(f\"ModuleNotFoundError: {e}. Ensure that 'utils/data_processing.py' exists and '__init__.py' is present in 'utils'.\")\n",
        "    raise\n",
        "\n",
        "# ----------------------------\n",
        "# Structured Logging with structlog\n",
        "# ----------------------------\n",
        "\n",
        "structlog.configure(\n",
        "    processors=[\n",
        "        structlog.processors.TimeStamper(fmt=\"iso\"),\n",
        "        structlog.processors.JSONRenderer()\n",
        "    ],\n",
        "    context_class=dict,\n",
        "    logger_factory=structlog.stdlib.LoggerFactory(),\n",
        "    wrapper_class=structlog.stdlib.BoundLogger,\n",
        "    cache_logger_on_first_use=True,\n",
        ")\n",
        "\n",
        "logger = structlog.get_logger()\n",
        "\n",
        "# ----------------------------\n",
        "# Caching\n",
        "# ----------------------------\n",
        "\n",
        "# Define caching constants\n",
        "TREND_CACHE_TTL = 3600       # 1 hour in seconds\n",
        "OPENAI_CACHE_TTL = 86400     # 1 day in seconds\n",
        "\n",
        "# Initialize caches with defined TTLs\n",
        "trends_cache = TTLCache(maxsize=100, ttl=TREND_CACHE_TTL)\n",
        "openai_cache = TTLCache(maxsize=1000, ttl=OPENAI_CACHE_TTL)\n",
        "\n",
        "# ----------------------------\n",
        "# Rate Limiting for RSS Feeds\n",
        "# ----------------------------\n",
        "\n",
        "ONE_DAY = 86400               # Seconds in one day\n",
        "RSS_CALLS_PER_DAY = 100       # Maximum number of RSS feed calls per day\n",
        "\n",
        "@sleep_and_retry\n",
        "@limits(calls=RSS_CALLS_PER_DAY, period=ONE_DAY)\n",
        "def fetch_rss_feed_sync(rss_url):\n",
        "    \"\"\"\n",
        "    Fetches and parses the RSS feed with rate limiting.\n",
        "\n",
        "    Parameters:\n",
        "        rss_url (str): The URL of the RSS feed.\n",
        "\n",
        "    Returns:\n",
        "        feedparser.FeedParserDict or None: Parsed RSS feed or None if fetching fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        feed = feedparser.parse(rss_url)\n",
        "        return feed\n",
        "    except Exception as e:\n",
        "        logger.error(\"rss_fetch_error\", rss_url=rss_url, error=str(e))\n",
        "        return None\n",
        "\n",
        "# ----------------------------\n",
        "# Initialize Sentiment Analyzer and Keyword Extractor\n",
        "# ----------------------------\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "rake = Rake()\n",
        "\n",
        "# ----------------------------\n",
        "# Helper Functions\n",
        "# ----------------------------\n",
        "\n",
        "def get_matching_country(input_country, available_countries):\n",
        "    \"\"\"\n",
        "    Uses fuzzy matching to find the best matching country from the available_countries.\n",
        "    Supports both country names and country codes.\n",
        "\n",
        "    Parameters:\n",
        "        input_country (str): User input for the country.\n",
        "        available_countries (dict): Dictionary of available countries and their codes.\n",
        "\n",
        "    Returns:\n",
        "        str or None: The matched country name or None if no match is found.\n",
        "    \"\"\"\n",
        "    # Combine country names and codes\n",
        "    country_list = list(available_countries.keys()) + list(available_countries.values())\n",
        "    # Use fuzzy matching to find the best match\n",
        "    match, score = process.extractOne(input_country, country_list)\n",
        "    if score >= 80:  # Threshold can be adjusted\n",
        "        # Determine if the match is a country name or code\n",
        "        if match.upper() in available_countries.values():\n",
        "            # Find the country name corresponding to the code\n",
        "            for name, code in available_countries.items():\n",
        "                if code.upper() == match.upper():\n",
        "                    return name\n",
        "        else:\n",
        "            return match\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def sanitize_topic(topic):\n",
        "    \"\"\"\n",
        "    Simplifies and sanitizes the topic string to make it suitable for Google Trends queries.\n",
        "\n",
        "    Parameters:\n",
        "        topic (str): The topic to sanitize.\n",
        "\n",
        "    Returns:\n",
        "        str: Sanitized topic.\n",
        "    \"\"\"\n",
        "    # Remove URLs, special characters, and excessive whitespace\n",
        "    topic = re.sub(r'http\\S+', '', topic)  # Remove URLs\n",
        "    topic = re.sub(r'[^A-Za-z0-9\\s]', '', topic)  # Remove special characters\n",
        "    topic = re.sub(r'\\s+', ' ', topic)  # Replace multiple spaces with single space\n",
        "    topic = topic.strip()\n",
        "    # Optionally, shorten the topic if it's too long\n",
        "    if len(topic) > 100:\n",
        "        topic = topic[:100]\n",
        "    return topic\n",
        "\n",
        "def extract_source(url):\n",
        "    \"\"\"\n",
        "    Extracts the main domain name from the URL to identify the source.\n",
        "\n",
        "    Parameters:\n",
        "        url (str): The URL of the news article.\n",
        "\n",
        "    Returns:\n",
        "        str: The name of the source.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        match = re.findall(r'https?://(?:www\\.)?([^/]+)/', url)\n",
        "        domain = match[0] if match else \"Unknown Source\"\n",
        "        domain_mapping = {\n",
        "            'cbsnews.com': 'CBS News',\n",
        "            'cnn.com': 'CNN',\n",
        "            'foxnews.com': 'Fox News',\n",
        "            'abcnews.go.com': 'ABC News',\n",
        "            'bbc.co.uk': 'BBC',\n",
        "            'google.com': 'Google News',\n",
        "            'news.google.com': 'Google News',\n",
        "            'reuters.com': 'Reuters',\n",
        "            'theguardian.com': 'The Guardian',\n",
        "            'nytimes.com': 'The New York Times',\n",
        "            'usatoday.com': 'USA Today',\n",
        "            'fortworthstar.com': 'Fort Worth Star-Telegram',\n",
        "            'wcnc.com': 'WCNC',\n",
        "            'apnews.com': 'AP News',\n",
        "            'floridatoday.com': 'Florida Today',\n",
        "            'msnbc.com': 'MSNBC News',\n",
        "            # Add more mappings as needed\n",
        "        }\n",
        "        return domain_mapping.get(domain.lower(), domain.capitalize())\n",
        "    except Exception as e:\n",
        "        logger.error(\"extract_source_error\", url=url, error=str(e))\n",
        "        return \"Unknown Source\"\n",
        "\n",
        "def broaden_query(query):\n",
        "    \"\"\"\n",
        "    Broadens the query by removing specific terms to increase the likelihood of data retrieval.\n",
        "\n",
        "    Parameters:\n",
        "        query (str): The original query.\n",
        "\n",
        "    Returns:\n",
        "        str: A broadened query.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        remove_terms = ['and', 'or', 'the', 'of', 'in', 'to', 'for']\n",
        "        words = query.split()\n",
        "        broadened_words = [word for word in words if word.lower() not in remove_terms]\n",
        "        broadened_query = ' '.join(broadened_words)\n",
        "        return broadened_query\n",
        "    except Exception as e:\n",
        "        logger.error(\"broaden_query_error\", original_query=query, error=str(e))\n",
        "        return query\n",
        "\n",
        "@cached(trends_cache)\n",
        "def fetch_google_trends_cached(topic, timeframe='now 7-d'):\n",
        "    \"\"\"\n",
        "    Fetches Google Trends data for a given topic with caching.\n",
        "\n",
        "    Parameters:\n",
        "        topic (str): The topic to fetch trends for.\n",
        "        timeframe (str): The time frame for the trends data.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: Trends data or None if fetching fails.\n",
        "    \"\"\"\n",
        "    return fetch_google_trends(topic, timeframe)\n",
        "\n",
        "def fetch_google_trends(topic, timeframe='now 7-d', retries=3, backoff_factor=2):\n",
        "    \"\"\"\n",
        "    Fetches Google Trends data with enhanced error handling and structured logging.\n",
        "\n",
        "    Parameters:\n",
        "        topic (str): The topic to fetch trends for.\n",
        "        timeframe (str): The time frame for the trends data.\n",
        "        retries (int): Number of retry attempts.\n",
        "        backoff_factor (int): Factor for exponential backoff.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: Trends data or None if fetching fails.\n",
        "    \"\"\"\n",
        "    refined_query = map_topic_to_trends_query(topic)\n",
        "\n",
        "    # Log the refined query\n",
        "    logger.info(\"refined_google_trends_query\", refined_query=refined_query, original_topic=topic)\n",
        "    print(f\"Refined Google Trends Query: '{refined_query}' for Topic: '{topic}'\")\n",
        "\n",
        "    for attempt in range(1, retries + 1):\n",
        "        try:\n",
        "            pytrends.build_payload([refined_query], timeframe=timeframe, geo='US')\n",
        "            interest_over_time = pytrends.interest_over_time()\n",
        "            if not interest_over_time.empty:\n",
        "                latest_value = interest_over_time[refined_query].iloc[-1]\n",
        "                approx_traffic = str(latest_value)\n",
        "                sentiment = analyze_sentiment(approx_traffic)\n",
        "                return {\n",
        "                    'topic': refined_query,\n",
        "                    'interest': approx_traffic,\n",
        "                    'sentiment': sentiment\n",
        "                }\n",
        "            else:\n",
        "                logger.warning(\"no_google_trends_data\", query=refined_query, attempt=attempt)\n",
        "                if attempt < retries:\n",
        "                    refined_query = broaden_query(refined_query)\n",
        "                    logger.info(\"broaden_query_retry\", refined_query=refined_query, attempt=attempt)\n",
        "                    print(f\"Broadening query to '{refined_query}' and retrying.\")\n",
        "        except openai.error.OpenAIError as e:\n",
        "            logger.error(\"google_trends_openai_error\", query=refined_query, attempt=attempt, error=str(e))\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            logger.error(\"google_trends_error\", query=refined_query, attempt=attempt, error=str(e))\n",
        "            if attempt < retries:\n",
        "                sleep_time = backoff_factor ** attempt\n",
        "                logger.info(\"retrying_google_trends\", sleep_time=sleep_time)\n",
        "                print(f\"Retrying in {sleep_time} seconds...\")\n",
        "                time.sleep(sleep_time)\n",
        "            else:\n",
        "                logger.error(\"google_trends_failed\", query=refined_query, error=str(e))\n",
        "                return None\n",
        "    return None\n",
        "\n",
        "@cached(openai_cache)\n",
        "def generate_summary_cached(content):\n",
        "    \"\"\"\n",
        "    Generates a summary for the given content using cached OpenAI responses.\n",
        "\n",
        "    Parameters:\n",
        "        content (str): The content to summarize.\n",
        "\n",
        "    Returns:\n",
        "        str: A two-sentence summary.\n",
        "    \"\"\"\n",
        "    return generate_summary(content)\n",
        "\n",
        "def generate_summary(content):\n",
        "    \"\"\"\n",
        "    Generates a concise two-sentence summary using OpenAI's GPT-3.5-turbo.\n",
        "\n",
        "    Parameters:\n",
        "        content (str): The content to summarize.\n",
        "\n",
        "    Returns:\n",
        "        str: A two-sentence summary.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a concise summarizer. Provide a clear and brief two-sentence summary of the following content.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": content\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=150,\n",
        "            temperature=0.5,\n",
        "        )\n",
        "        summary = response.choices[0].message.content.strip()\n",
        "        return summary\n",
        "    except openai.error.OpenAIError as e:\n",
        "        logger.error(\"generate_summary_error\", error=str(e))\n",
        "        return \"No summary available.\"\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    \"\"\"\n",
        "    Analyzes sentiment using VADER and returns 'Positive', 'Negative', or 'Neutral'.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to analyze.\n",
        "\n",
        "    Returns:\n",
        "        str: Sentiment category.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if text == 'N/A':\n",
        "            # Default to Neutral if no applicable text is available\n",
        "            return 'Neutral'\n",
        "        scores = sid.polarity_scores(text)\n",
        "        compound = scores['compound']\n",
        "        if compound >= 0.05:\n",
        "            return 'Positive'\n",
        "        elif compound <= -0.05:\n",
        "            return 'Negative'\n",
        "        else:\n",
        "            return 'Neutral'\n",
        "    except Exception as e:\n",
        "        logger.error(\"sentiment_analysis_error\", text=text, error=str(e))\n",
        "        return 'Neutral'\n",
        "\n",
        "def map_topic_to_trends_query(topic_title):\n",
        "    \"\"\"\n",
        "    Maps topic titles to Google Trends queries using keyword extraction.\n",
        "\n",
        "    Parameters:\n",
        "        topic_title (str): The title of the topic.\n",
        "\n",
        "    Returns:\n",
        "        str: A refined Google Trends query.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        keywords = extract_keywords(topic_title)\n",
        "        return keywords\n",
        "    except Exception as e:\n",
        "        logger.error(\"map_topic_to_trends_query_error\", topic=topic_title, error=str(e))\n",
        "        return topic_title\n",
        "\n",
        "def extract_keywords(topic_title):\n",
        "    \"\"\"\n",
        "    Extracts keywords from the topic title using RAKE.\n",
        "\n",
        "    Parameters:\n",
        "        topic_title (str): The title of the topic.\n",
        "\n",
        "    Returns:\n",
        "        str: A string of top keywords.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        rake.extract_keywords_from_text(topic_title)\n",
        "        keywords = rake.get_ranked_phrases()\n",
        "        return ' '.join(keywords[:3])  # Top 3 keywords as a single string\n",
        "    except Exception as e:\n",
        "        logger.error(\"keyword_extraction_error\", topic=topic_title, error=str(e))\n",
        "        return topic_title\n",
        "\n",
        "def is_post_relevant(post_title, topic):\n",
        "    \"\"\"\n",
        "    Determines if a Reddit post is relevant to the topic using OpenAI's GPT-3.5-turbo.\n",
        "\n",
        "    Parameters:\n",
        "        post_title (str): The title of the Reddit post.\n",
        "        topic (str): The topic to compare against.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if relevant, False otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are a helpful assistant that determines if a Reddit post is relevant to a given topic. \"\n",
        "                        \"Respond with 'Yes' or 'No'.\"\n",
        "                    )\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Is the following Reddit post relevant to the topic '{topic}'?\\n\\nPost Title: {post_title}\\n\\nRespond with 'Yes' or 'No'.\"\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=1,\n",
        "            temperature=0,\n",
        "        )\n",
        "        answer = response.choices[0].message.content.strip().lower()\n",
        "        return answer == 'yes'\n",
        "    except openai.error.OpenAIError as e:\n",
        "        logger.error(\"is_post_relevant_error\", post_title=post_title, topic=topic, error=str(e))\n",
        "        return False\n",
        "\n",
        "def fetch_reddit_posts(topic, limit=10):\n",
        "    \"\"\"\n",
        "    Fetches Reddit posts related to the topic.\n",
        "\n",
        "    Parameters:\n",
        "        topic (str): The topic to search for.\n",
        "        limit (int): Number of posts to fetch.\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: A list of Reddit post data.\n",
        "    \"\"\"\n",
        "    reddit_posts = []\n",
        "    try:\n",
        "        subreddit = reddit.subreddit('all')\n",
        "        search_results = subreddit.search(topic, limit=limit)\n",
        "        for post in search_results:\n",
        "            if post.over_18:\n",
        "                continue\n",
        "            if is_post_relevant(post.title, topic):\n",
        "                # Analyze sentiment based on the post's title\n",
        "                sentiment = analyze_sentiment(post.title)\n",
        "                reddit_posts.append({\n",
        "                    'type': 'Reddit Post',\n",
        "                    'title': post.title,\n",
        "                    'summary': f\"Score: {post.score}\",\n",
        "                    'source': post.subreddit.display_name,\n",
        "                    'approx_traffic': 'N/A',\n",
        "                    'sentiment': sentiment\n",
        "                })\n",
        "    except Exception as e:\n",
        "        logger.error(\"fetch_reddit_posts_error\", topic=topic, error=str(e))\n",
        "    return reddit_posts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O06ulUQJr9wx"
      },
      "outputs": [],
      "source": [
        "# Block 9: Fetch and Aggregate Trending Data\n",
        "\n",
        "import asyncio\n",
        "\n",
        "async def fetch_and_aggregate_trending_data():\n",
        "    \"\"\"\n",
        "    Fetches and aggregates trending data from RSS feeds, Reddit, and Google Trends.\n",
        "\n",
        "    Returns:\n",
        "        List[Trend]: A list of aggregated trending data.\n",
        "    \"\"\"\n",
        "    rss_trends = await fetch_trending_topics_rss_async(geo='US', limit=10)\n",
        "    aggregated_trends = aggregate_trends_data(rss_trends)\n",
        "    return aggregated_trends\n",
        "\n",
        "async def fetch_trending_topics_rss_async(geo='US', limit=10):\n",
        "    \"\"\"\n",
        "    Asynchronously fetches trending topics from multiple RSS feeds with rate limiting and caching.\n",
        "\n",
        "    Parameters:\n",
        "        geo (str): Geographic location code.\n",
        "        limit (int): Number of entries per RSS feed.\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: A list of trending topics.\n",
        "    \"\"\"\n",
        "    rss_feeds = [\n",
        "        \"https://news.google.com/rss?geo=US\",\n",
        "        \"https://news.google.com/rss/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRGRqTVdZU0FtVnVLQUFQAQ?hl=en-US&gl=US&ceid=US%3Aen\",\n",
        "        \"https://news.google.com/rss/search?q=technology&hl=en-US&gl=US&ceid=US%3Aen\",\n",
        "        \"https://news.google.com/rss/search?q=health&hl=en-US&gl=US&ceid=US%3Aen\",\n",
        "        \"https://news.google.com/rss/search?q=business&hl=en-US&gl=US&ceid=US%3Aen\",\n",
        "        # Add more RSS feed URLs as needed\n",
        "    ]\n",
        "    trending_topics = []\n",
        "\n",
        "    tasks = []\n",
        "    for rss_url in rss_feeds:\n",
        "        tasks.append(fetch_rss_feed_async(rss_url, limit))\n",
        "\n",
        "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "    for result in results:\n",
        "        if isinstance(result, Exception):\n",
        "            logger.error(\"fetch_trending_topics_rss_async_exception\", error=str(result))\n",
        "            continue\n",
        "        for entry in result:\n",
        "            title = entry.get('title', 'No Title')\n",
        "            link = entry.get('link', '')\n",
        "            summary = entry.get('summary', \"No description available.\")\n",
        "            source = extract_source(link)\n",
        "            approx_traffic = entry.get('ht_approx_traffic', 'N/A')\n",
        "            if approx_traffic != 'N/A':\n",
        "                approx_traffic = approx_traffic.strip()\n",
        "                sentiment = analyze_sentiment(approx_traffic)\n",
        "            else:\n",
        "                google_trend_data = fetch_google_trends_cached(title, timeframe='now 7-d')\n",
        "                if google_trend_data:\n",
        "                    approx_traffic = google_trend_data.get('interest', 'N/A')\n",
        "                    sentiment = google_trend_data.get('sentiment', 'Neutral')\n",
        "                else:\n",
        "                    approx_traffic = 'N/A'\n",
        "                    sentiment = 'Neutral'\n",
        "            summary = generate_summary_cached(summary)\n",
        "            trending_topics.append({\n",
        "                'title': title,\n",
        "                'description': summary,\n",
        "                'source': source,\n",
        "                'approx_traffic': approx_traffic,\n",
        "                'sentiment': sentiment\n",
        "            })\n",
        "    return trending_topics\n",
        "\n",
        "async def fetch_rss_feed_async(rss_url, limit):\n",
        "    \"\"\"\n",
        "    Asynchronously fetches and parses an RSS feed.\n",
        "\n",
        "    Parameters:\n",
        "        rss_url (str): The URL of the RSS feed.\n",
        "        limit (int): Number of entries to fetch.\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: A list of RSS feed entries.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            async with session.get(rss_url, timeout=10) as response:\n",
        "                if response.status != 200:\n",
        "                    logger.error(\"rss_fetch_async_error\", rss_url=rss_url, status=response.status)\n",
        "                    return []\n",
        "                content = await response.text()\n",
        "                feed = feedparser.parse(content)\n",
        "                entries = feed.entries[:limit]\n",
        "                return entries\n",
        "    except asyncio.TimeoutError:\n",
        "        logger.error(\"rss_fetch_async_timeout\", rss_url=rss_url)\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logger.error(\"rss_fetch_async_exception\", rss_url=rss_url, error=str(e))\n",
        "        return []\n",
        "\n",
        "def aggregate_trends_data(rss_trends):\n",
        "    \"\"\"\n",
        "    Aggregates data for each trend by fetching Reddit posts and Google Trends data.\n",
        "\n",
        "    Parameters:\n",
        "        rss_trends (List[dict]): List of trends from RSS feeds.\n",
        "\n",
        "    Returns:\n",
        "        List[Trend]: List of aggregated trends.\n",
        "    \"\"\"\n",
        "    aggregated_trends = []\n",
        "    for trend in rss_trends:\n",
        "        topic_title = trend['title']\n",
        "        reddit_posts_data = fetch_reddit_posts(topic_title, limit=5)\n",
        "        reddit_posts = [RedditPost(**post) for post in reddit_posts_data]\n",
        "        # Fetch Google Trends data\n",
        "        google_trend_data = fetch_google_trends_cached(topic_title, timeframe='now 7-d')\n",
        "        if google_trend_data:\n",
        "            google_trend = GoogleTrend(**google_trend_data)\n",
        "            sentiment = google_trend.sentiment\n",
        "        else:\n",
        "            google_trend = None\n",
        "            sentiment = 'Neutral'\n",
        "        aggregated_trends.append(Trend(\n",
        "            title=topic_title,\n",
        "            description=trend['description'],\n",
        "            source=trend['source'],\n",
        "            approx_traffic=trend['approx_traffic'],\n",
        "            sentiment=sentiment,\n",
        "            reddit_posts=reddit_posts,\n",
        "            google_trend=google_trend\n",
        "        ))\n",
        "    return aggregated_trends\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 10: Fetch and Aggregate Trending Data\n",
        "\n",
        "import asyncio\n",
        "from prettytable import PrettyTable\n",
        "import openai  # Ensure openai is imported\n",
        "from pytrends.request import TrendReq  # Assuming pytrends is used\n",
        "import json\n",
        "\n",
        "# Initialize PyTrends\n",
        "pytrends = TrendReq(hl='en-US', tz=360)\n",
        "\n",
        "async def fetch_and_aggregate_trending_data():\n",
        "    \"\"\"\n",
        "    Fetches and aggregates trending data from RSS feeds, Reddit, and Google Trends.\n",
        "\n",
        "    Returns:\n",
        "        List[Trend]: A list of aggregated trending data.\n",
        "    \"\"\"\n",
        "    rss_trends = await fetch_trending_topics_rss_async(geo='US', limit=10)\n",
        "    aggregated_trends = aggregate_trends_data(rss_trends)\n",
        "    return aggregated_trends\n",
        "\n",
        "async def fetch_trending_topics_rss_async(geo='US', limit=10):\n",
        "    \"\"\"\n",
        "    Asynchronously fetches trending topics from multiple RSS feeds with rate limiting and caching.\n",
        "\n",
        "    Parameters:\n",
        "        geo (str): Geographic location code.\n",
        "        limit (int): Number of entries per RSS feed.\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: A list of trending topics.\n",
        "    \"\"\"\n",
        "    rss_feeds = [\n",
        "        \"https://news.google.com/rss?geo=US\",\n",
        "        \"https://news.google.com/rss/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRGRqTVdZU0FtVnVLQUFQAQ?hl=en-US&gl=US&ceid=US%3Aen\",\n",
        "        \"https://news.google.com/rss/search?q=technology&hl=en-US&gl=US&ceid=US%3Aen\",\n",
        "        \"https://news.google.com/rss/search?q=health&hl=en-US&gl=US&ceid=US%3Aen\",\n",
        "        \"https://news.google.com/rss/search?q=business&hl=en-US&gl=US&ceid=US%3Aen\",\n",
        "        # Add more RSS feed URLs as needed\n",
        "    ]\n",
        "    trending_topics = []\n",
        "\n",
        "    tasks = []\n",
        "    for rss_url in rss_feeds:\n",
        "        tasks.append(fetch_rss_feed_async(rss_url, limit))\n",
        "\n",
        "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "    for result in results:\n",
        "        if isinstance(result, Exception):\n",
        "            logger.error(\"fetch_trending_topics_rss_async_exception\", error=str(result))\n",
        "            continue\n",
        "        for entry in result:\n",
        "            title = entry.get('title', 'No Title')\n",
        "            link = entry.get('link', '')\n",
        "            summary = entry.get('summary', \"No description available.\")\n",
        "            source = extract_source(link)\n",
        "            approx_traffic = entry.get('ht_approx_traffic', 'N/A')\n",
        "            if approx_traffic != 'N/A':\n",
        "                approx_traffic = approx_traffic.strip()\n",
        "                sentiment = analyze_sentiment(approx_traffic)\n",
        "            else:\n",
        "                google_trend_data = fetch_google_trends_cached(title, timeframe='now 7-d')\n",
        "                if google_trend_data:\n",
        "                    approx_traffic = google_trend_data.get('interest', 'N/A')\n",
        "                    sentiment = google_trend_data.get('sentiment', 'Neutral')\n",
        "                else:\n",
        "                    approx_traffic = 'N/A'\n",
        "                    sentiment = 'Neutral'\n",
        "            summary = generate_summary_cached(summary)\n",
        "            trending_topics.append({\n",
        "                'title': title,\n",
        "                'description': summary,\n",
        "                'source': source,\n",
        "                'approx_traffic': approx_traffic,\n",
        "                'sentiment': sentiment\n",
        "            })\n",
        "    return trending_topics\n",
        "\n",
        "async def fetch_rss_feed_async(rss_url, limit):\n",
        "    \"\"\"\n",
        "    Asynchronously fetches and parses an RSS feed.\n",
        "\n",
        "    Parameters:\n",
        "        rss_url (str): The URL of the RSS feed.\n",
        "        limit (int): Number of entries to fetch.\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: A list of RSS feed entries.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            async with session.get(rss_url, timeout=10) as response:\n",
        "                if response.status != 200:\n",
        "                    logger.error(\"rss_fetch_async_error\", rss_url=rss_url, status=response.status)\n",
        "                    return []\n",
        "                content = await response.text()\n",
        "                feed = feedparser.parse(content)\n",
        "                entries = feed.entries[:limit]\n",
        "                return entries\n",
        "    except asyncio.TimeoutError:\n",
        "        logger.error(\"rss_fetch_async_timeout\", rss_url=rss_url)\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logger.error(\"rss_fetch_async_exception\", rss_url=rss_url, error=str(e))\n",
        "        return []\n",
        "\n",
        "def aggregate_trends_data(rss_trends):\n",
        "    \"\"\"\n",
        "    Aggregates data for each trend by fetching Reddit posts and Google Trends data.\n",
        "\n",
        "    Parameters:\n",
        "        rss_trends (List[dict]): List of trends from RSS feeds.\n",
        "\n",
        "    Returns:\n",
        "        List[Trend]: List of aggregated trends.\n",
        "    \"\"\"\n",
        "    aggregated_trends = []\n",
        "    for trend in rss_trends:\n",
        "        topic_title = trend['title']\n",
        "        reddit_posts_data = fetch_reddit_posts(topic_title, limit=5)\n",
        "        reddit_posts = [RedditPost(**post) for post in reddit_posts_data]\n",
        "        # Fetch Google Trends data\n",
        "        google_trend_data = fetch_google_trends_cached(topic_title, timeframe='now 7-d')\n",
        "        if google_trend_data:\n",
        "            google_trend = GoogleTrend(**google_trend_data)\n",
        "            sentiment = google_trend.sentiment\n",
        "        else:\n",
        "            google_trend = None\n",
        "            sentiment = 'Neutral'\n",
        "        aggregated_trends.append(Trend(\n",
        "            title=topic_title,\n",
        "            description=trend['description'],\n",
        "            source=trend['source'],\n",
        "            approx_traffic=trend['approx_traffic'],\n",
        "            sentiment=sentiment,\n",
        "            reddit_posts=reddit_posts,\n",
        "            google_trend=google_trend\n",
        "        ))\n",
        "    return aggregated_trends\n",
        "\n",
        "# Define dataclasses if not already defined\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional\n",
        "\n",
        "@dataclass\n",
        "class RedditPost:\n",
        "    type: str\n",
        "    title: str\n",
        "    summary: str\n",
        "    source: str\n",
        "    approx_traffic: str\n",
        "    sentiment: str\n",
        "\n",
        "@dataclass\n",
        "class GoogleTrend:\n",
        "    topic: str\n",
        "    interest: str\n",
        "    sentiment: str\n",
        "\n",
        "@dataclass\n",
        "class Trend:\n",
        "    title: str\n",
        "    description: str\n",
        "    source: str\n",
        "    approx_traffic: str\n",
        "    sentiment: str\n",
        "    reddit_posts: List[RedditPost]\n",
        "    google_trend: Optional[GoogleTrend]\n"
      ],
      "metadata": {
        "id": "awC_mVXsPsbZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Gq2Z-jEhsjqO"
      },
      "outputs": [],
      "source": [
        "# Block 10: Define the Main Function (Block 10)\n",
        "\n",
        "import sys\n",
        "import threading\n",
        "import random\n",
        "import textwrap\n",
        "import time\n",
        "from IPython.display import display, Markdown\n",
        "from prettytable import PrettyTable, ALL  # Ensure ALL is imported\n",
        "\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "\n",
        "# Apply the nest_asyncio patch to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to execute the script workflow.\n",
        "    \"\"\"\n",
        "    # Function to display a changing message every 15 seconds with an additional note\n",
        "    def flashing_message(stop_event):\n",
        "        messages = [\n",
        "            \"🔍 Gathering the latest trends... (This could take up to 2 minutes. Please wait.)\",\n",
        "            \"⏳ Processing data, please wait... (This could take up to 2 minutes. Please wait.)\",\n",
        "            \"✨ Almost there, thank you for your patience! (This could take up to 2 minutes. Please wait.)\"\n",
        "        ]\n",
        "        idx = 0\n",
        "        while not stop_event.is_set():\n",
        "            message = messages[idx % len(messages)]\n",
        "            print(f\"\\r{message}   \", end='', flush=True)\n",
        "            for _ in range(15):\n",
        "                if stop_event.is_set():\n",
        "                    break\n",
        "                time.sleep(1)\n",
        "            idx += 1\n",
        "            print('\\r' + ' ' * len(message) + '   ', end='', flush=True)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Step 1: User selects the country for trending topics\n",
        "    # ----------------------------\n",
        "    available_countries = {\n",
        "        'United States': 'US',\n",
        "        'Canada': 'CA',\n",
        "        'United Kingdom': 'GB',\n",
        "        'Australia': 'AU',\n",
        "        'India': 'IN',\n",
        "        'Germany': 'DE',\n",
        "        'France': 'FR',\n",
        "        'Japan': 'JP',\n",
        "        'Brazil': 'BR',\n",
        "        'South Korea': 'KR',\n",
        "        # Add more countries as needed\n",
        "    }\n",
        "\n",
        "    while True:\n",
        "        print(\"Enter the country for trending topics data (e.g., United States or US):\")\n",
        "        country_input = input(\"Country: \").strip()\n",
        "        matching_country = get_matching_country(country_input, available_countries)\n",
        "        if not matching_country:\n",
        "            print(\"No matching countries found. Please try again.\")\n",
        "            # Optionally, display available countries\n",
        "            print(\"Available countries are:\")\n",
        "            for country in available_countries.keys():\n",
        "                print(f\"- {country}\")\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    selected_country = matching_country\n",
        "    selected_country_code = available_countries[selected_country]\n",
        "    print(f\"You selected: {selected_country}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # Step 2: User selects the time frame for trending topics\n",
        "    # ----------------------------\n",
        "    print(\"\\nSelect the time frame for trending topics:\")\n",
        "    print(\"\\033[1mA.\\033[0m Last 4 hours\")\n",
        "    print(\"\\033[1mB.\\033[0m Last 24 hours\")\n",
        "    time_range_selection = input(\"Enter the letter of the time frame you're interested in: \").strip().upper()\n",
        "    time_range_mapping = {'A': 'Last 4 hours', 'B': 'Last 24 hours'}\n",
        "    time_range = time_range_mapping.get(time_range_selection)\n",
        "    if not time_range:\n",
        "        print(\"Invalid selection. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # ----------------------------\n",
        "    # Step 3: Start flashing message while fetching all data\n",
        "    # ----------------------------\n",
        "    stop_event = threading.Event()\n",
        "    thread = threading.Thread(target=flashing_message, args=(stop_event,))\n",
        "    thread.start()\n",
        "\n",
        "    # ----------------------------\n",
        "    # Step 4: Fetch all data asynchronously with robust error handling\n",
        "    # ----------------------------\n",
        "    async def fetch_data():\n",
        "        try:\n",
        "            aggregated_trends = await fetch_and_aggregate_trending_data()\n",
        "            return aggregated_trends\n",
        "        except Exception as e:\n",
        "            logger.error(\"fetch_data_error\", error=str(e))\n",
        "            return []\n",
        "\n",
        "    loop = asyncio.get_event_loop()\n",
        "    aggregated_trends = loop.create_task(fetch_data())\n",
        "\n",
        "    try:\n",
        "        # Wait for the data to be fetched\n",
        "        loop.run_until_complete(aggregated_trends)\n",
        "        aggregated_trends = aggregated_trends.result()\n",
        "    except Exception as e:\n",
        "        logger.error(\"asyncio_run_until_complete_error\", error=str(e))\n",
        "        aggregated_trends = []\n",
        "\n",
        "    # Stop flashing message\n",
        "    stop_event.set()\n",
        "    thread.join()\n",
        "    print()  # Move to the next line after flashing message\n",
        "\n",
        "    if not aggregated_trends:\n",
        "        logger.error(\"no_trending_topics_found\")\n",
        "        print(\"No trending topics found.\")\n",
        "        return\n",
        "\n",
        "    # ----------------------------\n",
        "    # Step 5: Display trending topics with pagination\n",
        "    # ----------------------------\n",
        "    # Pagination variables\n",
        "    batch_size = 10\n",
        "    total_trends = len(aggregated_trends)\n",
        "    current_index = 0\n",
        "\n",
        "    while current_index < total_trends:\n",
        "        # Determine the end index for the current batch\n",
        "        end_index = min(current_index + batch_size, total_trends)\n",
        "        batch_trends = aggregated_trends[current_index:end_index]\n",
        "\n",
        "        # Display the consolidated list with sentiments\n",
        "        print(f\"\\nCurrently Trending in {selected_country} in the last {time_range} (Showing {current_index + 1} to {end_index} of {total_trends}):\\n\")\n",
        "        table = PrettyTable()\n",
        "        table.field_names = [\"No.\", \"Topic\", \"Description\", \"Source\", \"Approx Traffic\", \"Sentiment\"]\n",
        "        table.hrules = ALL  # Use ALL for horizontal rules\n",
        "        table.max_width = 40  # Suitable for phone screens\n",
        "        for idx, trend in enumerate(batch_trends, start=current_index + 1):\n",
        "            title = textwrap.fill(trend.title, width=40)\n",
        "            description = textwrap.fill(trend.description, width=40)\n",
        "            source = trend.source\n",
        "            approx_traffic = trend.approx_traffic\n",
        "            sentiment = trend.sentiment\n",
        "            table.add_row([idx, title, description, source, approx_traffic, sentiment])\n",
        "        print(table)\n",
        "\n",
        "        current_index = end_index\n",
        "\n",
        "        if current_index >= total_trends:\n",
        "            print(\"====\\nNo more trending topics available.\\n\")\n",
        "            break\n",
        "\n",
        "        # Prompt the user to view more results\n",
        "        while True:\n",
        "            user_input = input(\"Type 'more' or '+' to view more results, or any other key to exit: \").strip().lower()\n",
        "            if user_input in ['more', '+']:\n",
        "                break  # Continue to the next batch\n",
        "            else:\n",
        "                print(\"Exiting the script.\")\n",
        "                return\n",
        "\n",
        "    # ----------------------------\n",
        "    # Step 6: User selects a topic to generate scripts\n",
        "    # ----------------------------\n",
        "    try:\n",
        "        selected_idx = input(\"Enter the number of the topic you're interested in (or type 0 to exit): \").strip()\n",
        "        if selected_idx.lower() == '0':\n",
        "            print(\"Exiting the script.\")\n",
        "            return\n",
        "        selected_idx = int(selected_idx)\n",
        "        if 1 <= selected_idx <= len(aggregated_trends):\n",
        "            selected_topic_data = aggregated_trends[selected_idx - 1]\n",
        "            selected_topic = selected_topic_data.title\n",
        "            # Apply text wrapping to the selected topic message\n",
        "            selected_topic_display = textwrap.fill(f\"You selected: {selected_topic}\", width=40)\n",
        "            print(f\"\\n{selected_topic_display}\")\n",
        "        else:\n",
        "            print(\"Invalid selection. Exiting.\")\n",
        "            return\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter a number. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # ----------------------------\n",
        "    # Step 7: Generate scripts for the selected topic\n",
        "    # ----------------------------\n",
        "    print(\"\\nChoose script customization options:\")\n",
        "    style = input(\"Select script style (Flashy Script / Expressive Script / Normal Script): \").strip().title()\n",
        "    tone = input(\"Select script tone (e.g., Informative, Persuasive, Emotional): \").strip().capitalize()\n",
        "    length = input(\"Enter script length (e.g., 60 seconds, 120 seconds): \").strip()\n",
        "\n",
        "    options = ScriptOptions(\n",
        "        style=style if style in [\"Flashy Script\", \"Expressive Script\", \"Normal Script\"] else \"Normal Script\",\n",
        "        tone=tone if tone else \"Informative\",\n",
        "        length=length if length else \"60 seconds\"\n",
        "    )\n",
        "\n",
        "    print(\"\\nGenerating script for the selected topic...\")\n",
        "    try:\n",
        "        script = generate_script_for_topic_cached(selected_topic, selected_topic_data, options=options)\n",
        "    except Exception as e:\n",
        "        logger.error(\"generate_script_error\", error=str(e))\n",
        "        script = \"Failed to generate script.\"\n",
        "\n",
        "    # Display the script with enhanced formatting as a code block to preserve structure\n",
        "    script_formatted = f\"```plaintext\\n{script}\\n```\"\n",
        "    display(Markdown(f\"### Generated Script for '{selected_topic}':\\n\\n{script_formatted}\\n\\n**Source:** [{selected_topic_data.source}]\"))\n",
        "    print(\"====\\n\")\n",
        "\n",
        "    print(\"Script generation completed.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1s-3u0czUndpxCkgirEnqndEDGpEPjpJ_",
      "authorship_tag": "ABX9TyMcNSoBNZUuhQG3EREMzEYR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}