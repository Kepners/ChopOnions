{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kepners/ChopOnions/blob/main/ChoppingOnions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Install Required Packages\n",
        "\n",
        "!pip install --upgrade openai python-dotenv praw rake_nltk feedparser aiohttp nest_asyncio structlog cachetools fuzzywuzzy python-Levenshtein nltk pytrends ratelimit prettytable\n",
        "print(\"pip install completed\")\n",
        "import nest_asyncio\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "print(\"vader_lexicon downloaded\")\n",
        "nltk.download('stopwords')\n",
        "print(\"stopwords downloaded\")\n",
        "nltk.download('punkt')\n",
        "print(\"punkt downloaded\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "print(\"averaged_perceptron_tagger downloaded\")\n",
        "nltk.download('maxent_ne_chunker')\n",
        "print(\"maxent_ne_chunker downloaded\")\n",
        "nltk.download('words')\n",
        "print(\"words downloaded\")\n",
        "nltk.download('wordnet')\n",
        "print(\"wordnet downloaded\")\n",
        "\n",
        "print(\"Installation and NLTK data download completed successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RWAbuL6NbeeL",
        "outputId": "e6b6a3aa-68a6-4cd9-893c-f03316d409cb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.52.2)\n",
            "Collecting openai\n",
            "  Downloading openai-1.54.3-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting praw\n",
            "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting rake_nltk\n",
            "  Downloading rake_nltk-1.0.6-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting feedparser\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.10.10)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Collecting structlog\n",
            "  Downloading structlog-24.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (5.5.0)\n",
            "Collecting fuzzywuzzy\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting python-Levenshtein\n",
            "  Downloading python_Levenshtein-0.26.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting pytrends\n",
            "  Downloading pytrends-4.9.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting ratelimit\n",
            "  Downloading ratelimit-2.2.1.tar.gz (5.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (3.11.0)\n",
            "Collecting prettytable\n",
            "  Downloading prettytable-3.12.0-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.2)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.9.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Collecting prawcore<3,>=2.4 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update_checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Collecting sgmllib3k (from feedparser)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (4.0.3)\n",
            "Collecting Levenshtein==0.26.1 (from python-Levenshtein)\n",
            "  Downloading levenshtein-0.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.26.1->python-Levenshtein)\n",
            "  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.10/dist-packages (from pytrends) (2.32.3)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from pytrends) (2.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pytrends) (5.3.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable) (0.2.13)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (2.2.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp) (0.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25->pytrends) (1.16.0)\n",
            "Downloading openai-1.54.3-py3-none-any.whl (389 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m389.6/389.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.3/189.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading structlog-24.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.2/67.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading python_Levenshtein-0.26.1-py3-none-any.whl (9.4 kB)\n",
            "Downloading levenshtein-0.26.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytrends-4.9.2-py3-none-any.whl (15 kB)\n",
            "Downloading prettytable-3.12.0-py3-none-any.whl (31 kB)\n",
            "Downloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: ratelimit, sgmllib3k\n",
            "  Building wheel for ratelimit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ratelimit: filename=ratelimit-2.2.1-py3-none-any.whl size=5895 sha256=0a4d2905d64001e6fb174905eb5d0a9b7859b7e95c4635f902ccbb34311b1c6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/5f/ba/e972a56dcbf5de9f2b7d2b2a710113970bd173c4dcd3d2c902\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=a66b8cd51252ffde45ed82c427fcbd079e89b0e28f3171de985e03f8d1f02784\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built ratelimit sgmllib3k\n",
            "Installing collected packages: sgmllib3k, ratelimit, fuzzywuzzy, structlog, rapidfuzz, python-dotenv, prettytable, nltk, feedparser, update_checker, rake_nltk, prawcore, Levenshtein, pytrends, python-Levenshtein, praw, openai\n",
            "  Attempting uninstall: prettytable\n",
            "    Found existing installation: prettytable 3.11.0\n",
            "    Uninstalling prettytable-3.11.0:\n",
            "      Successfully uninstalled prettytable-3.11.0\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.52.2\n",
            "    Uninstalling openai-1.52.2:\n",
            "      Successfully uninstalled openai-1.52.2\n",
            "Successfully installed Levenshtein-0.26.1 feedparser-6.0.11 fuzzywuzzy-0.18.0 nltk-3.9.1 openai-1.54.3 praw-7.8.1 prawcore-2.4.0 prettytable-3.12.0 python-Levenshtein-0.26.1 python-dotenv-1.0.1 pytrends-4.9.2 rake_nltk-1.0.6 rapidfuzz-3.10.1 ratelimit-2.2.1 sgmllib3k-1.0.0 structlog-24.4.0 update_checker-0.18.0\n",
            "pip install completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vader_lexicon downloaded\n",
            "stopwords downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "punkt downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "averaged_perceptron_tagger downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "maxent_ne_chunker downloaded\n",
            "words downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wordnet downloaded\n",
            "Installation and NLTK data download completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 2: Mount Google Drive and Load Environment Variables\n",
        "from google.colab import drive\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to your .env file in Google Drive\n",
        "dotenv_path = '/content/drive/MyDrive/Secrets/.env'\n",
        "\n",
        "# Load the environment variables from the .env file\n",
        "load_dotenv(dotenv_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wkdmt7yp0mR",
        "outputId": "030a8100-00d4-4d24-c6ce-c967b9031e90"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIud78FOZR3M",
        "outputId": "da26ec02-e554-4c47-9def-20b4c51792a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data structures defined successfully.\n"
          ]
        }
      ],
      "source": [
        "# Block 3: Define Data Structures\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional\n",
        "\n",
        "@dataclass\n",
        "class GoogleTrend:\n",
        "    topic: str\n",
        "    interest: str\n",
        "    sentiment: str\n",
        "\n",
        "@dataclass\n",
        "class RedditPost:\n",
        "    type: str\n",
        "    title: str\n",
        "    summary: str\n",
        "    source: str\n",
        "    approx_traffic: str\n",
        "    sentiment: str\n",
        "\n",
        "@dataclass\n",
        "class Trend:\n",
        "    title: str\n",
        "    description: str\n",
        "    source: str\n",
        "    approx_traffic: str\n",
        "    sentiment: str\n",
        "    reddit_posts: List[RedditPost] = field(default_factory=list)\n",
        "    google_trend: Optional[GoogleTrend] = None\n",
        "\n",
        "@dataclass\n",
        "class ScriptOptions:\n",
        "    style: str = \"Normal Script\"\n",
        "    tone: str = \"Informative\"\n",
        "    length: str = \"60 seconds\"\n",
        "\n",
        "print(\"Data structures defined successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ujJviMETtuLY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "426c7ff9-5c2f-44df-d95f-d94c4a79cb19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration and initialization completed successfully.\n"
          ]
        }
      ],
      "source": [
        "# Block 4: Configuration and Initialization\n",
        "\n",
        "import configparser\n",
        "import praw\n",
        "from pytrends.request import TrendReq\n",
        "\n",
        "# Path to config.ini in Google Drive\n",
        "config_path = '/content/drive/MyDrive/Secrets/config.ini'  # Adjust the path as needed\n",
        "\n",
        "# Check if config.ini exists\n",
        "if not os.path.exists(config_path):\n",
        "    print(f\"Configuration file not found at {config_path}. Please create it with the following format:\")\n",
        "    print(\"\"\"\n",
        "[openai]\n",
        "api_key=YOUR_OPENAI_API_KEY\n",
        "\n",
        "[reddit]\n",
        "client_id=YOUR_REDDIT_CLIENT_ID\n",
        "client_secret=YOUR_REDDIT_CLIENT_SECRET\n",
        "user_agent=YOUR_REDDIT_USER_AGENT\n",
        "\"\"\")\n",
        "    raise FileNotFoundError(f\"No config.ini found at {config_path}\")\n",
        "\n",
        "# Load configuration from config.ini\n",
        "config = configparser.ConfigParser()\n",
        "config.read(config_path)\n",
        "\n",
        "# Assign API keys and credentials\n",
        "try:\n",
        "    OPENAI_API_KEY = config.get('openai', 'api_key')\n",
        "    REDDIT_CLIENT_ID = config.get('reddit', 'client_id')\n",
        "    REDDIT_CLIENT_SECRET = config.get('reddit', 'client_secret')\n",
        "    REDDIT_USER_AGENT = config.get('reddit', 'user_agent')\n",
        "except configparser.NoSectionError as e:\n",
        "    print(f\"Configuration error: {e}\")\n",
        "    raise\n",
        "except configparser.NoOptionError as e:\n",
        "    print(f\"Configuration error: {e}\")\n",
        "    raise\n",
        "\n",
        "# Initialize OpenAI\n",
        "api_key = OPENAI_API_KEY\n",
        "\n",
        "# Initialize Reddit\n",
        "reddit = praw.Reddit(\n",
        "    client_id=REDDIT_CLIENT_ID,\n",
        "    client_secret=REDDIT_CLIENT_SECRET,\n",
        "    user_agent=REDDIT_USER_AGENT,\n",
        ")\n",
        "\n",
        "# Initialize PyTrends\n",
        "pytrends = TrendReq(hl='en-US', tz=360)\n",
        "\n",
        "print(\"Configuration and initialization completed successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGPK1IRBXfBm",
        "outputId": "3c487d35-a539-499d-dacf-75150d5786e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created empty '__init__.py' at 'utils/__init__.py' to make 'utils' a package.\n",
            "Created 'utils/data_processing.py' successfully.\n",
            "\n",
            "Verifying the creation of 'data_processing.py' and '__init__.py':\n",
            "total 4\n",
            "-rw-r--r-- 1 root root 1558 Nov  7 14:09 data_processing.py\n",
            "-rw-r--r-- 1 root root    0 Nov  7 14:09 __init__.py\n"
          ]
        }
      ],
      "source": [
        "# Block 5: Create utils/data_processing.py and utils/__init__.py\n",
        "\n",
        "# Define the directory where helper functions will reside\n",
        "utils_dir = 'utils'\n",
        "\n",
        "# Create the 'utils' directory if it doesn't exist\n",
        "os.makedirs(utils_dir, exist_ok=True)\n",
        "\n",
        "# Define the path for the __init__.py file to make 'utils' a package\n",
        "init_path = os.path.join(utils_dir, '__init__.py')\n",
        "\n",
        "# Create an empty __init__.py file if it doesn't exist\n",
        "if not os.path.exists(init_path):\n",
        "    with open(init_path, 'w') as file:\n",
        "        pass  # Creating an empty __init__.py\n",
        "    print(f\"Created empty '__init__.py' at '{init_path}' to make 'utils' a package.\")\n",
        "else:\n",
        "    print(f\"'__init__.py' already exists at '{init_path}'.\")\n",
        "\n",
        "# Define the path for the data_processing.py file\n",
        "data_processing_path = os.path.join(utils_dir, 'data_processing.py')\n",
        "\n",
        "# Define the content for data_processing.py\n",
        "data_processing_code = \"\"\"\n",
        "# data_processing.py\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ne_chunk, pos_tag\n",
        "from nltk.tree import Tree\n",
        "\n",
        "def clean_text(text):\n",
        "    '''\n",
        "    Cleans the input text by removing URLs, special characters, and stopwords.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to clean.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned text.\n",
        "    '''\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\\\S+', '', text)\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^A-Za-z\\\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    # Join the words back into a single string\n",
        "    cleaned_text = ' '.join(filtered_words)\n",
        "    return cleaned_text\n",
        "\n",
        "def extract_entities(text):\n",
        "    '''\n",
        "    Extracts named entities from the input text.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to extract entities from.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of named entities.\n",
        "    '''\n",
        "    def get_entities(tree):\n",
        "        entities = []\n",
        "        for subtree in tree:\n",
        "            if isinstance(subtree, Tree):\n",
        "                entity = \" \".join([token for token, pos in subtree.leaves()])\n",
        "                entities.append(entity)\n",
        "        return entities\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = pos_tag(tokens)\n",
        "    chunked = ne_chunk(tagged)\n",
        "    entities = get_entities(chunked)\n",
        "    return entities\n",
        "\"\"\"\n",
        "\n",
        "# Write the data_processing.py file\n",
        "with open(data_processing_path, 'w') as file:\n",
        "    file.write(data_processing_code)\n",
        "\n",
        "print(f\"Created '{data_processing_path}' successfully.\")\n",
        "\n",
        "# Optional: Verify the creation by listing the 'utils' directory\n",
        "print(\"\\nVerifying the creation of 'data_processing.py' and '__init__.py':\")\n",
        "!ls -l utils/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "id": "RPd0MOuGTaTM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "b110b526-b61a-41ca-e340-a3d2f9b65a9a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 47 (<ipython-input-11-75f3cb2fefad>, line 48)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-75f3cb2fefad>\"\u001b[0;36m, line \u001b[0;32m48\u001b[0m\n\u001b[0;31m    table = PrettyTable() # Create the table instance here\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 47\n"
          ]
        }
      ],
      "source": [
        "# Block 6: Define Helper Functions\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Get the absolute path to the notebook's directory\n",
        "notebook_dir = os.path.abspath('')\n",
        "\n",
        "# Add the 'utils' directory to the system path\n",
        "utils_dir = os.path.join(notebook_dir, 'utils')\n",
        "sys.path.append(utils_dir)\n",
        "\n",
        "# Now import from utils\n",
        "from utils.data_processing import clean_text, extract_entities\n",
        "\n",
        "import openai\n",
        "from openai import OpenAIError  # Import OpenAIError directly\n",
        "from prettytable import PrettyTable, HRuleStyle, VRuleStyle  # Import HRuleStyle, VRuleStyle\n",
        "# ... (rest of your code in Block 6)\n",
        "\n",
        "# ... (later, when using PrettyTable)\n",
        "table.hrules = HRuleStyle.ALL  # Use HRuleStyle.ALL\n",
        "\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import structlog\n",
        "import praw\n",
        "from prettytable import PrettyTable, HRuleStyle, VRuleStyle\n",
        "\n",
        "from utils.data_processing import clean_text, extract_entities\n",
        "from cachetools import TTLCache, cached\n",
        "from fuzzywuzzy import process\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from rake_nltk import Rake\n",
        "from textblob import TextBlob\n",
        "import feedparser\n",
        "from prettytable import PrettyTable, ALL\n",
        "import time\n",
        "import re\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "from utils.data_processing import clean_text, extract_entities\n",
        "from openai.error import OpenAIError  # Import OpenAIError correctly\n",
        "\n",
        "def my_function_using_prettytable():\n",
        "table = PrettyTable() # Create the table instance here\n",
        "table.field_names = [\"Column 1\", \"Column 2\"]\n",
        "table.hrules = HRuleStyle.ALL\n",
        "\n",
        "  # Set hrules after creating the table\n",
        "table = PrettyTable()  # Create a PrettyTable instance\n",
        "table.field_names = [\"No.\", \"Topic\", \"Description\", \"Source\", \"Approx Traffic\", \"Sentiment\"]\n",
        "table.hrules = ALL  # Use ALL for horizontal rules\n",
        "table.max_width = 40  # Suitable for phone screens\n",
        "\n",
        "  print(table)\n",
        "# ----------------------------\n",
        "# Structured Logging with structlog\n",
        "# ----------------------------\n",
        "\n",
        "structlog.configure(\n",
        "    processors=[\n",
        "        structlog.processors.TimeStamper(fmt=\"iso\"),\n",
        "        structlog.processors.JSONRenderer()\n",
        "    ],\n",
        "    context_class=dict,\n",
        "    logger_factory=structlog.stdlib.LoggerFactory(),\n",
        "    wrapper_class=structlog.stdlib.BoundLogger,\n",
        "    cache_logger_on_first_use=True,\n",
        ")\n",
        "\n",
        "logger = structlog.get_logger()\n",
        "\n",
        "# ----------------------------\n",
        "# Caching\n",
        "# ----------------------------\n",
        "\n",
        "# Define caching constants\n",
        "TREND_CACHE_TTL = 3600       # 1 hour in seconds\n",
        "OPENAI_CACHE_TTL = 86400     # 1 day in seconds\n",
        "\n",
        "# Initialize caches with defined TTLs\n",
        "trends_cache = TTLCache(maxsize=100, ttl=TREND_CACHE_TTL)\n",
        "openai_cache = TTLCache(maxsize=1000, ttl=OPENAI_CACHE_TTL)\n",
        "\n",
        "# ----------------------------\n",
        "# Rate Limiting for RSS Feeds\n",
        "# ----------------------------\n",
        "\n",
        "ONE_DAY = 86400               # Seconds in one day\n",
        "RSS_CALLS_PER_DAY = 100       # Maximum number of RSS feed calls per day\n",
        "\n",
        "@sleep_and_retry\n",
        "@limits(calls=RSS_CALLS_PER_DAY, period=ONE_DAY)\n",
        "def fetch_rss_feed_sync(rss_url):\n",
        "    \"\"\"\n",
        "    Fetches and parses the RSS feed with rate limiting.\n",
        "\n",
        "    Parameters:\n",
        "        rss_url (str): The URL of the RSS feed.\n",
        "\n",
        "    Returns:\n",
        "        feedparser.FeedParserDict or None: Parsed RSS feed or None if fetching fails.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        feed = feedparser.parse(rss_url)\n",
        "        return feed\n",
        "    except Exception as e:\n",
        "        logger.error(\"rss_fetch_error\", rss_url=rss_url, error=str(e))\n",
        "        return None\n",
        "\n",
        "# ----------------------------\n",
        "# Initialize Sentiment Analyzer and Keyword Extractor\n",
        "# ----------------------------\n",
        "\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "rake = Rake()\n",
        "\n",
        "# ----------------------------\n",
        "# Helper Functions\n",
        "# ----------------------------\n",
        "\n",
        "def get_matching_country(input_country, available_countries):\n",
        "    \"\"\"\n",
        "    Uses fuzzy matching to find the best matching country from the available_countries.\n",
        "    Supports both country names and country codes.\n",
        "\n",
        "    Parameters:\n",
        "        input_country (str): User input for the country.\n",
        "        available_countries (dict): Dictionary of available countries and their codes.\n",
        "\n",
        "    Returns:\n",
        "        str or None: The matched country name or None if no match is found.\n",
        "    \"\"\"\n",
        "    # Combine country names and codes\n",
        "    country_list = list(available_countries.keys()) + list(available_countries.values())\n",
        "    # Use fuzzy matching to find the best match\n",
        "    match, score = process.extractOne(input_country, country_list)\n",
        "    if score >= 80:  # Threshold can be adjusted\n",
        "        # Determine if the match is a country name or code\n",
        "        if match.upper() in available_countries.values():\n",
        "            # Find the country name corresponding to the code\n",
        "            for name, code in available_countries.items():\n",
        "                if code.upper() == match.upper():\n",
        "                    return name\n",
        "        else:\n",
        "            return match\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def sanitize_topic(topic):\n",
        "    \"\"\"\n",
        "    Simplifies and sanitizes the topic string to make it suitable for Google Trends queries.\n",
        "\n",
        "    Parameters:\n",
        "        topic (str): The topic to sanitize.\n",
        "\n",
        "    Returns:\n",
        "        str: Sanitized topic.\n",
        "    \"\"\"\n",
        "    # Remove URLs, special characters, and excessive whitespace\n",
        "    topic = re.sub(r'http\\S+', '', topic)  # Remove URLs\n",
        "    topic = re.sub(r'[^A-Za-z0-9\\s]', '', topic)  # Remove special characters\n",
        "    topic = re.sub(r'\\s+', ' ', topic)  # Replace multiple spaces with single space\n",
        "    topic = topic.strip()\n",
        "    # Optionally, shorten the topic if it's too long\n",
        "    if len(topic) > 100:\n",
        "        topic = topic[:100]\n",
        "    return topic\n",
        "\n",
        "def extract_source(url):\n",
        "    \"\"\"\n",
        "    Extracts the main domain name from the URL to identify the source.\n",
        "\n",
        "    Parameters:\n",
        "        url (str): The URL of the news article.\n",
        "\n",
        "    Returns:\n",
        "        str: The name of the source.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        match = re.findall(r'https?://(?:www\\.)?([^/]+)/', url)\n",
        "        domain = match[0] if match else \"Unknown Source\"\n",
        "        domain_mapping = {\n",
        "            'cbsnews.com': 'CBS News',\n",
        "            'cnn.com': 'CNN',\n",
        "            'foxnews.com': 'Fox News',\n",
        "            'abcnews.go.com': 'ABC News',\n",
        "            'bbc.co.uk': 'BBC',\n",
        "            'google.com': 'Google News',\n",
        "            'news.google.com': 'Google News',\n",
        "            'reuters.com': 'Reuters',\n",
        "            'theguardian.com': 'The Guardian',\n",
        "            'nytimes.com': 'The New York Times',\n",
        "            'usatoday.com': 'USA Today',\n",
        "            'fortworthstar.com': 'Fort Worth Star-Telegram',\n",
        "            'wcnc.com': 'WCNC',\n",
        "            'apnews.com': 'AP News',\n",
        "            'floridatoday.com': 'Florida Today',\n",
        "            'msnbc.com': 'MSNBC News',\n",
        "            # Add more mappings as needed\n",
        "        }\n",
        "        return domain_mapping.get(domain.lower(), domain.capitalize())\n",
        "    except Exception as e:\n",
        "        logger.error(\"extract_source_error\", url=url, error=str(e))\n",
        "        return \"Unknown Source\"\n",
        "\n",
        "def broaden_query(query):\n",
        "    \"\"\"\n",
        "    Broadens the query by removing specific terms to increase the likelihood of data retrieval.\n",
        "\n",
        "    Parameters:\n",
        "        query (str): The original query.\n",
        "\n",
        "    Returns:\n",
        "        str: A broadened query.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        remove_terms = ['and', 'or', 'the', 'of', 'in', 'to', 'for']\n",
        "        words = query.split()\n",
        "        broadened_words = [word for word in words if word.lower() not in remove_terms]\n",
        "        broadened_query = ' '.join(broadened_words)\n",
        "        return broadened_query\n",
        "    except Exception as e:\n",
        "        logger.error(\"broaden_query_error\", original_query=query, error=str(e))\n",
        "        return query\n",
        "\n",
        "@cached(trends_cache)\n",
        "def fetch_google_trends_cached(topic, timeframe='now 7-d'):\n",
        "    \"\"\"\n",
        "    Fetches Google Trends data for a given topic with caching.\n",
        "\n",
        "    Parameters:\n",
        "        topic (str): The topic to fetch trends for.\n",
        "        timeframe (str): The time frame for the trends data.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: Trends data or None if fetching fails.\n",
        "    \"\"\"\n",
        "    return fetch_google_trends(topic, timeframe)\n",
        "\n",
        "def fetch_google_trends(topic, timeframe='now 7-d', retries=3, backoff_factor=2):\n",
        "    \"\"\"\n",
        "    Fetches Google Trends data with enhanced error handling and structured logging.\n",
        "\n",
        "    Parameters:\n",
        "        topic (str): The topic to fetch trends for.\n",
        "        timeframe (str): The time frame for the trends data.\n",
        "        retries (int): Number of retry attempts.\n",
        "        backoff_factor (int): Factor for exponential backoff.\n",
        "\n",
        "    Returns:\n",
        "        dict or None: Trends data or None if fetching fails.\n",
        "    \"\"\"\n",
        "    refined_query = map_topic_to_trends_query(topic)\n",
        "\n",
        "    # Log the refined query\n",
        "    logger.info(\"refined_google_trends_query\", refined_query=refined_query, original_topic=topic)\n",
        "    print(f\"Refined Google Trends Query: '{refined_query}' for Topic: '{topic}'\")\n",
        "\n",
        "    for attempt in range(1, retries + 1):\n",
        "        try:\n",
        "            pytrends.build_payload([refined_query], timeframe=timeframe, geo='US')\n",
        "            interest_over_time = pytrends.interest_over_time()\n",
        "            if not interest_over_time.empty:\n",
        "                latest_value = interest_over_time[refined_query].iloc[-1]\n",
        "                approx_traffic = str(latest_value)\n",
        "                sentiment = analyze_sentiment(approx_traffic)\n",
        "                return {\n",
        "                    'topic': refined_query,\n",
        "                    'interest': approx_traffic,\n",
        "                    'sentiment': sentiment\n",
        "                }\n",
        "            else:\n",
        "                logger.warning(\"no_google_trends_data\", query=refined_query, attempt=attempt)\n",
        "                if attempt < retries:\n",
        "                    refined_query = broaden_query(refined_query)\n",
        "                    logger.info(\"broaden_query_retry\", refined_query=refined_query, attempt=attempt)\n",
        "                    print(f\"Broadening query to '{refined_query}' and retrying.\")\n",
        "        except OpenAIError as e:\n",
        "            logger.error(\"google_trends_openai_error\", query=refined_query, attempt=attempt, error=str(e))\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            logger.error(\"google_trends_error\", query=refined_query, attempt=attempt, error=str(e))\n",
        "            if attempt < retries:\n",
        "                sleep_time = backoff_factor ** attempt\n",
        "                logger.info(\"retrying_google_trends\", sleep_time=sleep_time)\n",
        "                print(f\"Retrying in {sleep_time} seconds...\")\n",
        "                time.sleep(sleep_time)\n",
        "            else:\n",
        "                logger.error(\"google_trends_failed\", query=refined_query, error=str(e))\n",
        "                return None\n",
        "    return None\n",
        "\n",
        "@cached(openai_cache)\n",
        "def generate_summary_cached(content):\n",
        "    \"\"\"\n",
        "    Generates a summary for the given content using cached OpenAI responses.\n",
        "\n",
        "    Parameters:\n",
        "        content (str): The content to summarize.\n",
        "\n",
        "    Returns:\n",
        "        str: A two-sentence summary.\n",
        "    \"\"\"\n",
        "    return generate_summary(content)\n",
        "\n",
        "def generate_summary(content):\n",
        "    \"\"\"\n",
        "    Generates a concise two-sentence summary using OpenAI's GPT-3.5-turbo.\n",
        "\n",
        "    Parameters:\n",
        "        content (str): The content to summarize.\n",
        "\n",
        "    Returns:\n",
        "        str: A two-sentence summary.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a concise summarizer. Provide a clear and brief two-sentence summary of the following content.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": content\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=150,\n",
        "            temperature=0.5,\n",
        "        )\n",
        "        summary = response.choices[0].message.content.strip()\n",
        "        return summary\n",
        "    except OpenAIError as e:\n",
        "        logger.error(\"generate_summary_error\", error=str(e))\n",
        "        return \"No summary available.\"\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    \"\"\"\n",
        "    Analyzes sentiment using VADER and returns 'Positive', 'Negative', or 'Neutral'.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to analyze.\n",
        "\n",
        "    Returns:\n",
        "        str: Sentiment category.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if text == 'N/A':\n",
        "            # Default to Neutral if no applicable text is available\n",
        "            return 'Neutral'\n",
        "        scores = sid.polarity_scores(text)\n",
        "        compound = scores['compound']\n",
        "        if compound >= 0.05:\n",
        "            return 'Positive'\n",
        "        elif compound <= -0.05:\n",
        "            return 'Negative'\n",
        "        else:\n",
        "            return 'Neutral'\n",
        "    except Exception as e:\n",
        "        logger.error(\"sentiment_analysis_error\", text=text, error=str(e))\n",
        "        return 'Neutral'\n",
        "\n",
        "def map_topic_to_trends_query(topic_title):\n",
        "    \"\"\"\n",
        "    Maps topic titles to Google Trends queries using keyword extraction.\n",
        "\n",
        "    Parameters:\n",
        "        topic_title (str): The title of the topic.\n",
        "\n",
        "    Returns:\n",
        "        str: A refined Google Trends query.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        keywords = extract_keywords(topic_title)\n",
        "        return keywords\n",
        "    except Exception as e:\n",
        "        logger.error(\"map_topic_to_trends_query_error\", topic=topic_title, error=str(e))\n",
        "        return topic_title\n",
        "\n",
        "def extract_keywords(topic_title):\n",
        "    \"\"\"\n",
        "    Extracts keywords from the topic title using RAKE.\n",
        "\n",
        "    Parameters:\n",
        "        topic_title (str): The title of the topic.\n",
        "\n",
        "    Returns:\n",
        "        str: A string of top keywords.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        rake.extract_keywords_from_text(topic_title)\n",
        "        keywords = rake.get_ranked_phrases()\n",
        "        return ' '.join(keywords[:3])  # Top 3 keywords as a single string\n",
        "    except Exception as e:\n",
        "        logger.error(\"keyword_extraction_error\", topic=topic_title, error=str(e))\n",
        "        return topic_title\n",
        "\n",
        "def is_post_relevant(post_title, topic):\n",
        "    \"\"\"\n",
        "    Determines if a Reddit post is relevant to the topic using OpenAI's GPT-3.5-turbo.\n",
        "\n",
        "    Parameters:\n",
        "        post_title (str): The title of the Reddit post.\n",
        "        topic (str): The topic to compare against.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if relevant, False otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are a helpful assistant that determines if a Reddit post is relevant to a given topic. \"\n",
        "                        \"Respond with 'Yes' or 'No'.\"\n",
        "                    )\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Is the following Reddit post relevant to the topic '{topic}'?\\n\\nPost Title: {post_title}\\n\\nRespond with 'Yes' or 'No'.\"\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=1,\n",
        "            temperature=0,\n",
        "        )\n",
        "        answer = response.choices[0].message.content.strip().lower()\n",
        "        return answer == 'yes'\n",
        "    except OpenAIError as e:\n",
        "        logger.error(\"is_post_relevant_error\", post_title=post_title, topic=topic, error=str(e))\n",
        "        return False\n",
        "\n",
        "def fetch_reddit_posts(topic, limit=10):\n",
        "    \"\"\"\n",
        "    Fetches Reddit posts related to the topic.\n",
        "\n",
        "    Parameters:\n",
        "        topic (str): The topic to search for.\n",
        "        limit (int): Number of posts to fetch.\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: A list of Reddit post data.\n",
        "    \"\"\"\n",
        "    reddit_posts = []\n",
        "    try:\n",
        "        subreddit = reddit.subreddit('all')\n",
        "        search_results = subreddit.search(topic, limit=limit)\n",
        "        for post in search_results:\n",
        "            if post.over_18:\n",
        "                continue\n",
        "            if is_post_relevant(post.title, topic):\n",
        "                # Analyze sentiment based on the post's title\n",
        "                sentiment = analyze_sentiment(post.title)\n",
        "                reddit_posts.append({\n",
        "                    'type': 'Reddit Post',\n",
        "                    'title': post.title,\n",
        "                    'summary': f\"Score: {post.score}\",\n",
        "                    'source': post.subreddit.display_name,\n",
        "                    'approx_traffic': 'N/A',\n",
        "                    'sentiment': sentiment\n",
        "                })\n",
        "    except Exception as e:\n",
        "        logger.error(\"fetch_reddit_posts_error\", topic=topic, error=str(e))\n",
        "    return reddit_posts\n",
        "\n",
        "print(\"Helper functions defined successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(f\"Current Working Directory: {os.getcwd()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLVzZkCSYp0c",
        "outputId": "0d25aec2-9a73-4589-b0fb-b599c7435ecb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "C5RZdBozr4hJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "cf83a368-e114-4d8c-e788-3e33387ae43e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'openai_cache' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a13fe585eb70>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Block 9: Define Script Generation Function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mcached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopenai_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_script_for_topic_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrend_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mScriptOptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n",
            "\u001b[0;31mNameError\u001b[0m: name 'openai_cache' is not defined"
          ]
        }
      ],
      "source": [
        "# Block 9: Define Script Generation Function\n",
        "\n",
        "@cached(openai_cache)\n",
        "def generate_script_for_topic_cached(topic, trend_data, options: ScriptOptions):\n",
        "    \"\"\"\n",
        "    Generates a script for the given topic using cached OpenAI responses.\n",
        "\n",
        "    Parameters:\n",
        "        topic (str): The topic to generate the script for.\n",
        "        trend_data (Trend): The aggregated trend data for the topic.\n",
        "        options (ScriptOptions): User-defined script customization options.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated script.\n",
        "    \"\"\"\n",
        "    return generate_script_for_topic(topic, trend_data, options)\n",
        "\n",
        "def generate_script_for_topic(topic, trend_data, options: ScriptOptions):\n",
        "    \"\"\"\n",
        "    Generates a script based on the topic, trend data, and customization options.\n",
        "\n",
        "    Parameters:\n",
        "        topic (str): The topic to generate the script for.\n",
        "        trend_data (Trend): The aggregated trend data for the topic.\n",
        "        options (ScriptOptions): User-defined script customization options.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated script.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Construct the prompt based on user options\n",
        "        prompt = (\n",
        "            f\"Create a {options.length} video script about '{topic}'.\\n\"\n",
        "            f\"Style: {options.style}\\n\"\n",
        "            f\"Tone: {options.tone}\\n\"\n",
        "            f\"Use the following data:\\n\"\n",
        "            f\"Description: {trend_data.description}\\n\"\n",
        "            f\"Source: {trend_data.source}\\n\"\n",
        "            f\"Approx Traffic: {trend_data.approx_traffic}\\n\"\n",
        "            f\"Sentiment: {trend_data.sentiment}\\n\"\n",
        "            f\"Related Reddit Posts:\\n\"\n",
        "        )\n",
        "        for post in trend_data.reddit_posts:\n",
        "            prompt += f\"- {post.title} (Sentiment: {post.sentiment})\\n\"\n",
        "\n",
        "        prompt += \"\\nGenerate a concise and engaging script suitable for a video presentation based on the above information.\"\n",
        "\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a creative scriptwriter.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=500,\n",
        "            temperature=0.7,\n",
        "        )\n",
        "        script = response.choices[0].message.content.strip()\n",
        "        return script\n",
        "    except OpenAIError as e:\n",
        "        logger.error(\"generate_script_error\", error=str(e))\n",
        "        return \"No script available.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 10: Fetch and Aggregate Trending Data\n",
        "\n",
        "import asyncio\n",
        "from prettytable import PrettyTable\n",
        "import openai  # Ensure openai is imported\n",
        "from pytrends.request import TrendReq  # Assuming pytrends is used\n",
        "import json\n",
        "\n",
        "# Initialize PyTrends\n",
        "pytrends = TrendReq(hl='en-US', tz=360)\n",
        "\n",
        "async def fetch_and_aggregate_trending_data():\n",
        "    \"\"\"\n",
        "    Fetches and aggregates trending data from RSS feeds, Reddit, and Google Trends.\n",
        "\n",
        "    Returns:\n",
        "        List[Trend]: A list of aggregated trending data.\n",
        "    \"\"\"\n",
        "    rss_trends = await fetch_trending_topics_rss_async(geo='US', limit=10)\n",
        "    aggregated_trends = aggregate_trends_data(rss_trends)\n",
        "    return aggregated_trends\n",
        "\n",
        "async def fetch_trending_topics_rss_async(geo='US', limit=10):\n",
        "    \"\"\"\n",
        "    Asynchronously fetches trending topics from multiple RSS feeds with rate limiting and caching.\n",
        "\n",
        "    Parameters:\n",
        "        geo (str): Geographic location code.\n",
        "        limit (int): Number of entries per RSS feed.\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: A list of trending topics.\n",
        "    \"\"\"\n",
        "    rss_feeds = [\n",
        "        \"https://news.google.com/rss?geo=US\",\n",
        "        \"https://news.google.com/rss/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRGRqTVdZU0FtVnVLQUFQAQ?hl=en-US&gl=US&ceid=US%3Aen\",\n",
        "        \"https://news.google.com/rss/search?q=technology&hl=en-US&gl=US&ceid=US%3Aen\",\n",
        "        \"https://news.google.com/rss/search?q=health&hl=en-US&gl=US&ceid=US%3Aen\",\n",
        "        \"https://news.google.com/rss/search?q=business&hl=en-US&gl=US&ceid=US%3Aen\",\n",
        "        # Add more RSS feed URLs as needed\n",
        "    ]\n",
        "    trending_topics = []\n",
        "\n",
        "    tasks = []\n",
        "    for rss_url in rss_feeds:\n",
        "        tasks.append(fetch_rss_feed_async(rss_url, limit))\n",
        "\n",
        "    results = await asyncio.gather(*tasks, return_exceptions=True)\n",
        "\n",
        "    for result in results:\n",
        "        if isinstance(result, Exception):\n",
        "            logger.error(\"fetch_trending_topics_rss_async_exception\", error=str(result))\n",
        "            continue\n",
        "        for entry in result:\n",
        "            title = entry.get('title', 'No Title')\n",
        "            link = entry.get('link', '')\n",
        "            summary = entry.get('summary', \"No description available.\")\n",
        "            source = extract_source(link)\n",
        "            approx_traffic = entry.get('ht_approx_traffic', 'N/A')\n",
        "            if approx_traffic != 'N/A':\n",
        "                approx_traffic = approx_traffic.strip()\n",
        "                sentiment = analyze_sentiment(approx_traffic)\n",
        "            else:\n",
        "                google_trend_data = fetch_google_trends_cached(title, timeframe='now 7-d')\n",
        "                if google_trend_data:\n",
        "                    approx_traffic = google_trend_data.get('interest', 'N/A')\n",
        "                    sentiment = google_trend_data.get('sentiment', 'Neutral')\n",
        "                else:\n",
        "                    approx_traffic = 'N/A'\n",
        "                    sentiment = 'Neutral'\n",
        "            summary = generate_summary_cached(summary)\n",
        "            trending_topics.append({\n",
        "                'title': title,\n",
        "                'description': summary,\n",
        "                'source': source,\n",
        "                'approx_traffic': approx_traffic,\n",
        "                'sentiment': sentiment\n",
        "            })\n",
        "    return trending_topics\n",
        "\n",
        "async def fetch_rss_feed_async(rss_url, limit):\n",
        "    \"\"\"\n",
        "    Asynchronously fetches and parses an RSS feed.\n",
        "\n",
        "    Parameters:\n",
        "        rss_url (str): The URL of the RSS feed.\n",
        "        limit (int): Number of entries to fetch.\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: A list of RSS feed entries.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        async with aiohttp.ClientSession() as session:\n",
        "            async with session.get(rss_url, timeout=10) as response:\n",
        "                if response.status != 200:\n",
        "                    logger.error(\"rss_fetch_async_error\", rss_url=rss_url, status=response.status)\n",
        "                    return []\n",
        "                content = await response.text()\n",
        "                feed = feedparser.parse(content)\n",
        "                entries = feed.entries[:limit]\n",
        "                return entries\n",
        "    except asyncio.TimeoutError:\n",
        "        logger.error(\"rss_fetch_async_timeout\", rss_url=rss_url)\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logger.error(\"rss_fetch_async_exception\", rss_url=rss_url, error=str(e))\n",
        "        return []\n",
        "\n",
        "def aggregate_trends_data(rss_trends):\n",
        "    \"\"\"\n",
        "    Aggregates data for each trend by fetching Reddit posts and Google Trends data.\n",
        "\n",
        "    Parameters:\n",
        "        rss_trends (List[dict]): List of trends from RSS feeds.\n",
        "\n",
        "    Returns:\n",
        "        List[Trend]: List of aggregated trends.\n",
        "    \"\"\"\n",
        "    aggregated_trends = []\n",
        "    for trend in rss_trends:\n",
        "        topic_title = trend['title']\n",
        "        reddit_posts_data = fetch_reddit_posts(topic_title, limit=5)\n",
        "        reddit_posts = [RedditPost(**post) for post in reddit_posts_data]\n",
        "        # Fetch Google Trends data\n",
        "        google_trend_data = fetch_google_trends_cached(topic_title, timeframe='now 7-d')\n",
        "        if google_trend_data:\n",
        "            google_trend = GoogleTrend(**google_trend_data)\n",
        "            sentiment = google_trend.sentiment\n",
        "        else:\n",
        "            google_trend = None\n",
        "            sentiment = 'Neutral'\n",
        "        aggregated_trends.append(Trend(\n",
        "            title=topic_title,\n",
        "            description=trend['description'],\n",
        "            source=trend['source'],\n",
        "            approx_traffic=trend['approx_traffic'],\n",
        "            sentiment=sentiment,\n",
        "            reddit_posts=reddit_posts,\n",
        "            google_trend=google_trend\n",
        "        ))\n",
        "    return aggregated_trends\n",
        "\n",
        "# Define dataclasses if not already defined\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional\n",
        "\n",
        "@dataclass\n",
        "class RedditPost:\n",
        "    type: str\n",
        "    title: str\n",
        "    summary: str\n",
        "    source: str\n",
        "    approx_traffic: str\n",
        "    sentiment: str\n",
        "\n",
        "@dataclass\n",
        "class GoogleTrend:\n",
        "    topic: str\n",
        "    interest: str\n",
        "    sentiment: str\n",
        "\n",
        "@dataclass\n",
        "class Trend:\n",
        "    title: str\n",
        "    description: str\n",
        "    source: str\n",
        "    approx_traffic: str\n",
        "    sentiment: str\n",
        "    reddit_posts: List[RedditPost]\n",
        "    google_trend: Optional[GoogleTrend]\n"
      ],
      "metadata": {
        "id": "awC_mVXsPsbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gq2Z-jEhsjqO"
      },
      "outputs": [],
      "source": [
        "# Block 10: Define the Main Function (Block 10)\n",
        "\n",
        "import sys\n",
        "import threading\n",
        "import random\n",
        "import textwrap\n",
        "import time\n",
        "from IPython.display import display, Markdown\n",
        "from prettytable import PrettyTable, ALL  # Ensure ALL is imported\n",
        "\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "\n",
        "# Apply the nest_asyncio patch to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to execute the script workflow.\n",
        "    \"\"\"\n",
        "    # Function to display a changing message every 15 seconds with an additional note\n",
        "    def flashing_message(stop_event):\n",
        "        messages = [\n",
        "            \"🔍 Gathering the latest trends... (This could take up to 2 minutes. Please wait.)\",\n",
        "            \"⏳ Processing data, please wait... (This could take up to 2 minutes. Please wait.)\",\n",
        "            \"✨ Almost there, thank you for your patience! (This could take up to 2 minutes. Please wait.)\"\n",
        "        ]\n",
        "        idx = 0\n",
        "        while not stop_event.is_set():\n",
        "            message = messages[idx % len(messages)]\n",
        "            print(f\"\\r{message}   \", end='', flush=True)\n",
        "            for _ in range(15):\n",
        "                if stop_event.is_set():\n",
        "                    break\n",
        "                time.sleep(1)\n",
        "            idx += 1\n",
        "            print('\\r' + ' ' * len(message) + '   ', end='', flush=True)\n",
        "\n",
        "    # ----------------------------\n",
        "    # Step 1: User selects the country for trending topics\n",
        "    # ----------------------------\n",
        "    available_countries = {\n",
        "        'United States': 'US',\n",
        "        'Canada': 'CA',\n",
        "        'United Kingdom': 'GB',\n",
        "        'Australia': 'AU',\n",
        "        'India': 'IN',\n",
        "        'Germany': 'DE',\n",
        "        'France': 'FR',\n",
        "        'Japan': 'JP',\n",
        "        'Brazil': 'BR',\n",
        "        'South Korea': 'KR',\n",
        "        # Add more countries as needed\n",
        "    }\n",
        "\n",
        "    while True:\n",
        "        print(\"Enter the country for trending topics data (e.g., United States or US):\")\n",
        "        country_input = input(\"Country: \").strip()\n",
        "        matching_country = get_matching_country(country_input, available_countries)\n",
        "        if not matching_country:\n",
        "            print(\"No matching countries found. Please try again.\")\n",
        "            # Optionally, display available countries\n",
        "            print(\"Available countries are:\")\n",
        "            for country in available_countries.keys():\n",
        "                print(f\"- {country}\")\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    selected_country = matching_country\n",
        "    selected_country_code = available_countries[selected_country]\n",
        "    print(f\"You selected: {selected_country}\")\n",
        "\n",
        "    # ----------------------------\n",
        "    # Step 2: User selects the time frame for trending topics\n",
        "    # ----------------------------\n",
        "    print(\"\\nSelect the time frame for trending topics:\")\n",
        "    print(\"\\033[1mA.\\033[0m Last 4 hours\")\n",
        "    print(\"\\033[1mB.\\033[0m Last 24 hours\")\n",
        "    time_range_selection = input(\"Enter the letter of the time frame you're interested in: \").strip().upper()\n",
        "    time_range_mapping = {'A': 'Last 4 hours', 'B': 'Last 24 hours'}\n",
        "    time_range = time_range_mapping.get(time_range_selection)\n",
        "    if not time_range:\n",
        "        print(\"Invalid selection. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # ----------------------------\n",
        "    # Step 3: Start flashing message while fetching all data\n",
        "    # ----------------------------\n",
        "    stop_event = threading.Event()\n",
        "    thread = threading.Thread(target=flashing_message, args=(stop_event,))\n",
        "    thread.start()\n",
        "\n",
        "    # ----------------------------\n",
        "    # Step 4: Fetch all data asynchronously with robust error handling\n",
        "    # ----------------------------\n",
        "    async def fetch_data():\n",
        "        try:\n",
        "            aggregated_trends = await fetch_and_aggregate_trending_data()\n",
        "            return aggregated_trends\n",
        "        except Exception as e:\n",
        "            logger.error(\"fetch_data_error\", error=str(e))\n",
        "            return []\n",
        "\n",
        "    loop = asyncio.get_event_loop()\n",
        "    aggregated_trends = loop.create_task(fetch_data())\n",
        "\n",
        "    try:\n",
        "        # Wait for the data to be fetched\n",
        "        loop.run_until_complete(aggregated_trends)\n",
        "        aggregated_trends = aggregated_trends.result()\n",
        "    except Exception as e:\n",
        "        logger.error(\"asyncio_run_until_complete_error\", error=str(e))\n",
        "        aggregated_trends = []\n",
        "\n",
        "    # Stop flashing message\n",
        "    stop_event.set()\n",
        "    thread.join()\n",
        "    print()  # Move to the next line after flashing message\n",
        "\n",
        "    if not aggregated_trends:\n",
        "        logger.error(\"no_trending_topics_found\")\n",
        "        print(\"No trending topics found.\")\n",
        "        return\n",
        "\n",
        "    # ----------------------------\n",
        "    # Step 5: Display trending topics with pagination\n",
        "    # ----------------------------\n",
        "    # Pagination variables\n",
        "    batch_size = 10\n",
        "    total_trends = len(aggregated_trends)\n",
        "    current_index = 0\n",
        "\n",
        "    while current_index < total_trends:\n",
        "        # Determine the end index for the current batch\n",
        "        end_index = min(current_index + batch_size, total_trends)\n",
        "        batch_trends = aggregated_trends[current_index:end_index]\n",
        "\n",
        "        # Display the consolidated list with sentiments\n",
        "        print(f\"\\nCurrently Trending in {selected_country} in the last {time_range} (Showing {current_index + 1} to {end_index} of {total_trends}):\\n\")\n",
        "        table = PrettyTable()\n",
        "        table.field_names = [\"No.\", \"Topic\", \"Description\", \"Source\", \"Approx Traffic\", \"Sentiment\"]\n",
        "        table.hrules = ALL  # Use ALL for horizontal rules\n",
        "        table.max_width = 40  # Suitable for phone screens\n",
        "        for idx, trend in enumerate(batch_trends, start=current_index + 1):\n",
        "            title = textwrap.fill(trend.title, width=40)\n",
        "            description = textwrap.fill(trend.description, width=40)\n",
        "            source = trend.source\n",
        "            approx_traffic = trend.approx_traffic\n",
        "            sentiment = trend.sentiment\n",
        "            table.add_row([idx, title, description, source, approx_traffic, sentiment])\n",
        "        print(table)\n",
        "\n",
        "        current_index = end_index\n",
        "\n",
        "        if current_index >= total_trends:\n",
        "            print(\"====\\nNo more trending topics available.\\n\")\n",
        "            break\n",
        "\n",
        "        # Prompt the user to view more results\n",
        "        while True:\n",
        "            user_input = input(\"Type 'more' or '+' to view more results, or any other key to exit: \").strip().lower()\n",
        "            if user_input in ['more', '+']:\n",
        "                break  # Continue to the next batch\n",
        "            else:\n",
        "                print(\"Exiting the script.\")\n",
        "                return\n",
        "\n",
        "    # ----------------------------\n",
        "    # Step 6: User selects a topic to generate scripts\n",
        "    # ----------------------------\n",
        "    try:\n",
        "        selected_idx = input(\"Enter the number of the topic you're interested in (or type 0 to exit): \").strip()\n",
        "        if selected_idx.lower() == '0':\n",
        "            print(\"Exiting the script.\")\n",
        "            return\n",
        "        selected_idx = int(selected_idx)\n",
        "        if 1 <= selected_idx <= len(aggregated_trends):\n",
        "            selected_topic_data = aggregated_trends[selected_idx - 1]\n",
        "            selected_topic = selected_topic_data.title\n",
        "            # Apply text wrapping to the selected topic message\n",
        "            selected_topic_display = textwrap.fill(f\"You selected: {selected_topic}\", width=40)\n",
        "            print(f\"\\n{selected_topic_display}\")\n",
        "        else:\n",
        "            print(\"Invalid selection. Exiting.\")\n",
        "            return\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Please enter a number. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # ----------------------------\n",
        "    # Step 7: Generate scripts for the selected topic\n",
        "    # ----------------------------\n",
        "    print(\"\\nChoose script customization options:\")\n",
        "    style = input(\"Select script style (Flashy Script / Expressive Script / Normal Script): \").strip().title()\n",
        "    tone = input(\"Select script tone (e.g., Informative, Persuasive, Emotional): \").strip().capitalize()\n",
        "    length = input(\"Enter script length (e.g., 60 seconds, 120 seconds): \").strip()\n",
        "\n",
        "    options = ScriptOptions(\n",
        "        style=style if style in [\"Flashy Script\", \"Expressive Script\", \"Normal Script\"] else \"Normal Script\",\n",
        "        tone=tone if tone else \"Informative\",\n",
        "        length=length if length else \"60 seconds\"\n",
        "    )\n",
        "\n",
        "    print(\"\\nGenerating script for the selected topic...\")\n",
        "    try:\n",
        "        script = generate_script_for_topic_cached(selected_topic, selected_topic_data, options=options)\n",
        "    except Exception as e:\n",
        "        logger.error(\"generate_script_error\", error=str(e))\n",
        "        script = \"Failed to generate script.\"\n",
        "\n",
        "    # Display the script with enhanced formatting as a code block to preserve structure\n",
        "    script_formatted = f\"```plaintext\\n{script}\\n```\"\n",
        "    display(Markdown(f\"### Generated Script for '{selected_topic}':\\n\\n{script_formatted}\\n\\n**Source:** [{selected_topic_data.source}]\"))\n",
        "    print(\"====\\n\")\n",
        "\n",
        "    print(\"Script generation completed.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1s-3u0czUndpxCkgirEnqndEDGpEPjpJ_",
      "authorship_tag": "ABX9TyNryvSftYvBVVmvrp0GRm/M",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}