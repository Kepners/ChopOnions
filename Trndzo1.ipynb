{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcGrQtex0sn3kpQCxBRz1M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kepners/ChopOnions/blob/main/Trndzo1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Install Required Libraries\n",
        "!pip install praw openai pytrends feedparser requests cachetools IPython prettytable textblob\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAUVB0guTX2b",
        "outputId": "e08a70f0-e9d9-497c-e647-56fb8b766169",
        "collapsed": true
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.10/dist-packages (7.8.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: pytrends in /usr/local/lib/python3.10/dist-packages (4.9.2)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.10/dist-packages (6.0.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (5.5.0)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.10/dist-packages (7.34.0)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (3.11.0)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in /usr/local/lib/python3.10/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.10.10)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from pytrends) (2.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pytrends) (5.3.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from IPython) (0.19.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from IPython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from IPython) (4.9.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable) (0.2.13)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->IPython) (0.8.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.9.11)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2024.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->IPython) (0.7.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25->pytrends) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->openai) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 2: Mount Google Drive and Load Environment Variables\n",
        "from google.colab import drive\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to your .env file in Google Drive\n",
        "dotenv_path = '/content/drive/MyDrive/Secrets/.env'\n",
        "\n",
        "# Load the environment variables from the .env file\n",
        "load_dotenv(dotenv_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIud78FOZR3M",
        "outputId": "8f195133-ab46-47f8-f2dd-d0e1db8fbfdd"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 3: Configure Logging\n",
        "import logging\n",
        "\n",
        "# Configure Logging to write to a log file and suppress console output\n",
        "log_file = 'app.log'\n",
        "logging.basicConfig(\n",
        "    level=logging.ERROR,  # Only log ERROR and CRITICAL\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(log_file),\n",
        "        logging.StreamHandler(open(os.devnull, 'w'))  # Suppress console output\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "ujJviMETtuLY"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 4: Download NLTK Data Silently\n",
        "import nltk\n",
        "import sys\n",
        "from contextlib import contextmanager\n",
        "\n",
        "@contextmanager\n",
        "def suppress_stdout():\n",
        "    with open(os.devnull, \"w\") as devnull:\n",
        "        old_stdout = sys.stdout\n",
        "        sys.stdout = devnull\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n",
        "\n",
        "with suppress_stdout():\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    nltk.download('brown')\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('movie_reviews')\n"
      ],
      "metadata": {
        "id": "RPd0MOuGTaTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5254c22-e430-44a7-f466-b0238e7ed016",
        "collapsed": true
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 5: Create Utils Directory\n",
        "import os\n",
        "\n",
        "utils_dir = 'utils'\n",
        "if not os.path.exists(utils_dir):\n",
        "    os.makedirs(utils_dir)\n"
      ],
      "metadata": {
        "id": "bNicEhLDr06j"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 6: Create data_processing.py\n",
        "data_processing_code = \"\"\"\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from textblob import TextBlob\n",
        "\n",
        "def extract_keywords(text, max_keywords=10):\n",
        "    \\\"\"\"\n",
        "    Extracts up to `max_keywords` nouns from the input `text`.\n",
        "    \\\"\"\"\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords and non-alphabetic tokens\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "    # Get part-of-speech tags\n",
        "    tagged_words = pos_tag(filtered_words)\n",
        "    # Keep nouns and proper nouns\n",
        "    keywords = [word for word, pos in tagged_words if pos.startswith('NN')]\n",
        "    # Limit the number of keywords\n",
        "    return keywords[:max_keywords]\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    \\\"\"\"\n",
        "    Analyzes the sentiment of the given text and returns it.\n",
        "    \\\"\"\"\n",
        "    blob = TextBlob(text)\n",
        "    polarity = blob.sentiment.polarity\n",
        "    if polarity > 0.1:\n",
        "        return 'Positive'\n",
        "    elif polarity < -0.1:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\"\"\"\n",
        "with open(os.path.join('utils', 'data_processing.py'), 'w') as file:\n",
        "    file.write(data_processing_code)\n"
      ],
      "metadata": {
        "id": "C5RZdBozr4hJ"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 7: Initialize API Clients and Configure Logging\n",
        "import praw\n",
        "import openai\n",
        "import logging\n",
        "import warnings\n",
        "from utils.data_processing import extract_keywords, analyze_sentiment\n",
        "from cachetools import TTLCache, cached\n",
        "from pytrends.request import TrendReq\n",
        "import feedparser\n",
        "import requests\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Suppress PRAW warnings about asynchronous environments\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='praw')\n",
        "logging.getLogger('praw').setLevel(logging.CRITICAL)\n",
        "\n",
        "# Initialize Reddit API using PRAW\n",
        "try:\n",
        "    reddit = praw.Reddit(\n",
        "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
        "        client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
        "        user_agent=os.getenv('REDDIT_USER_AGENT', 'script:TrendingTopicsScript:1.0 (by u/yourusername)')\n",
        "    )\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error initializing Reddit API: {e}\")\n",
        "\n",
        "# Initialize OpenAI API\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Initialize Pexels API key\n",
        "PEXELS_API_KEY = os.getenv('PEXELS_API_KEY')\n",
        "\n",
        "# Initialize Shutterstock Access Token\n",
        "SHUTTERSTOCK_ACCESS_TOKEN = os.getenv('SHUTTERSTOCK_ACCESS_TOKEN')\n",
        "\n",
        "# Define a list of countries with their codes\n",
        "available_countries = {\n",
        "    'United States': 'US',\n",
        "    'India': 'IN',\n",
        "    'Canada': 'CA',\n",
        "    'United Kingdom': 'GB',\n",
        "    'Australia': 'AU',\n",
        "    'Germany': 'DE',\n",
        "    'France': 'FR',\n",
        "    'Brazil': 'BR',\n",
        "    'Mexico': 'MX',\n",
        "    'Japan': 'JP',\n",
        "    'Russia': 'RU',\n",
        "    'South Korea': 'KR',\n",
        "    'Italy': 'IT',\n",
        "    'Spain': 'ES',\n",
        "    'Netherlands': 'NL',\n",
        "    'Sweden': 'SE',\n",
        "    'Switzerland': 'CH',\n",
        "    'Austria': 'AT',\n",
        "    'Belgium': 'BE',\n",
        "    'New Zealand': 'NZ'\n",
        "}\n",
        "\n",
        "# Mapping from country codes to PyTrends 'pn' parameter for trending searches\n",
        "# For realtime_trending_searches, the acceptable 'pn' values are specific\n",
        "country_code_to_pn_realtime = {\n",
        "    'US': 'US',\n",
        "    'IN': 'IN',\n",
        "    'GB': 'GB',\n",
        "    'CA': 'CA',\n",
        "    'AU': 'AU',\n",
        "    'DE': 'DE',\n",
        "    'FR': 'FR',\n",
        "    'BR': 'BR',\n",
        "    'MX': 'MX',\n",
        "    'JP': 'JP',\n",
        "    'RU': 'RU',\n",
        "    'KR': 'KR'  # South Korea\n",
        "}\n",
        "\n",
        "# For trending_searches, the acceptable 'pn' values are:\n",
        "country_code_to_pn_daily = {\n",
        "    'united_states': 'united_states',\n",
        "    'india': 'india',\n",
        "    'united_kingdom': 'united_kingdom',\n",
        "    'canada': 'canada',\n",
        "    'australia': 'australia',\n",
        "    'germany': 'germany',\n",
        "    'france': 'france',\n",
        "    'brazil': 'brazil'\n",
        "    # Note: Limited countries are available for trending_searches\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "-2yQIa03r7Ml",
        "outputId": "078f2e8b-d467-4038-c392-7cb1ef81c300"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'analyze_sentiment' from 'utils.data_processing' (/content/utils/data_processing.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-ad94df0d20b1>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_processing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mextract_keywords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalyze_sentiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcachetools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTTLCache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpytrends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrendReq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'analyze_sentiment' from 'utils.data_processing' (/content/utils/data_processing.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 8: Define Supporting Functions\n",
        "def fetch_trending_topics_pytrends(selected_country_code, time_range):\n",
        "    try:\n",
        "        pytrends = TrendReq(hl='en-US', tz=360)\n",
        "        if time_range == '4h':\n",
        "            # Realtime trending searches\n",
        "            pn = country_code_to_pn_realtime.get(selected_country_code)\n",
        "            if not pn:\n",
        "                logging.error(f\"Realtime trending searches not available for the selected country.\")\n",
        "                return []\n",
        "            df = pytrends.realtime_trending_searches(pn=pn)\n",
        "        else:\n",
        "            # Daily trending searches\n",
        "            pn = country_code_to_pn_daily.get(selected_country_code.lower())\n",
        "            if not pn:\n",
        "                logging.error(f\"Daily trending searches not available for the selected country.\")\n",
        "                return []\n",
        "            df = pytrends.trending_searches(pn=pn)\n",
        "        if df.empty:\n",
        "            return []\n",
        "        trending_topics = []\n",
        "        for index, row in df.iterrows():\n",
        "            title = row[0]\n",
        "            # Generate a brief reason why the topic is trending\n",
        "            reason = generate_trend_reason(title)\n",
        "            trending_topics.append({\n",
        "                'title': title,\n",
        "                'description': reason,  # Brief reason why it's trending\n",
        "                'sv': 'High',       # Placeholder for Search Volume\n",
        "                'change': 'Up',     # Placeholder for Change\n",
        "                'started': 'Recently'   # Placeholder\n",
        "            })\n",
        "        return trending_topics[:10]  # Limit to top 10\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching Google Trends data via PyTrends: {e}\")\n",
        "        return []\n",
        "\n",
        "def generate_trend_reason(topic):\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are an assistant that provides brief reasons why a topic is trending. Your responses should be concise, in brackets, and two to four words long.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Why is '{topic}' trending? Provide a brief reason in brackets, two to four words long.\"\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=20,\n",
        "            temperature=0.5,\n",
        "        )\n",
        "        reason = response['choices'][0]['message']['content'].strip()\n",
        "        return reason\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating trend reason: {e}\")\n",
        "        return \"[Trending Topic]\"\n",
        "\n",
        "def generate_summary(content):\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a concise summarizer. Provide a clear and brief two-sentence summary of the following content.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": content\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=80,\n",
        "            temperature=0.5,\n",
        "        )\n",
        "        summary = response['choices'][0]['message']['content'].strip()\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating summary: {e}\")\n",
        "        return \"No summary available.\"\n",
        "\n",
        "def search_subreddits_for_topic(topic, limit=5):\n",
        "    \"\"\"\n",
        "    Searches for subreddits related to the topic.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        related_subreddits = []\n",
        "        subreddits_found = reddit.subreddits.search_by_name(query=topic, exact=False, include_nsfw=False)\n",
        "        for subreddit in subreddits_found:\n",
        "            # Fetch subreddit details\n",
        "            try:\n",
        "                subreddit_details = reddit.subreddit(subreddit.display_name)\n",
        "                # Check if subreddit is accessible\n",
        "                if subreddit_details.over18 or subreddit_details.subreddit_type in ['private', 'restricted']:\n",
        "                    continue\n",
        "                related_subreddits.append({\n",
        "                    'name': subreddit.display_name,\n",
        "                    'title': subreddit.title,\n",
        "                    'description': subreddit.public_description or ''\n",
        "                })\n",
        "                if len(related_subreddits) >= limit:\n",
        "                    break\n",
        "            except Exception as inner_e:\n",
        "                logging.error(f\"Error accessing subreddit '{subreddit.display_name}': {inner_e}\")\n",
        "                continue\n",
        "        return related_subreddits\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error searching for subreddits: {e}\")\n",
        "        return []\n",
        "\n",
        "def suggest_subreddits_via_openai(topic):\n",
        "    \"\"\"\n",
        "    Uses OpenAI to suggest relevant subreddit names based on the topic.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        prompt = f\"List 5 subreddit names that are relevant to the topic '{topic}'. Only provide the subreddit names without any additional text.\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ],\n",
        "            max_tokens=50,\n",
        "            temperature=0.5,\n",
        "            n=1,\n",
        "        )\n",
        "        suggestions = response['choices'][0]['message']['content'].strip().split('\\n')\n",
        "        subreddits = [s.strip().lstrip('/r/') for s in suggestions if s.strip()]\n",
        "        return subreddits\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error suggesting subreddits via OpenAI: {e}\")\n",
        "        return []\n",
        "\n",
        "def fetch_related_news(topic, limit=5):\n",
        "    \"\"\"\n",
        "    Fetches related news articles using Google News RSS feed.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        feed_url = f\"https://news.google.com/rss/search?q={requests.utils.quote(topic)}&hl=en-US&gl=US&ceid=US:en\"\n",
        "        feed = feedparser.parse(feed_url)\n",
        "        related_stories = []\n",
        "        for entry in feed.entries[:limit]:\n",
        "            title = entry.title\n",
        "            summary = entry.summary if 'summary' in entry else ''\n",
        "            link = entry.link\n",
        "            related_stories.append({'title': title, 'summary': summary, 'url': link})\n",
        "        return related_stories\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching related news: {e}\")\n",
        "        return []\n",
        "\n",
        "@cached(TTLCache(maxsize=1000, ttl=86400))\n",
        "def check_stock_media_availability(topic):\n",
        "    # Extract keywords from the topic\n",
        "    keywords = extract_keywords(topic)\n",
        "    if not keywords:\n",
        "        logging.error(f\"No valid keywords extracted for topic: {topic}\")\n",
        "        return 0\n",
        "    # Join keywords into a query string\n",
        "    query = ' '.join(keywords)[:100]\n",
        "    total_results = 0\n",
        "    # Check Pexels\n",
        "    total_results += check_pexels_media(query)\n",
        "    # Check Shutterstock\n",
        "    total_results += check_shutterstock_media(query)\n",
        "    return total_results\n",
        "\n",
        "def check_pexels_media(query):\n",
        "    headers = {\n",
        "        'Authorization': PEXELS_API_KEY,\n",
        "        'User-Agent': 'TrendingTopicsScript'\n",
        "    }\n",
        "    params = {\n",
        "        'query': query,\n",
        "        'per_page': 1,\n",
        "        'page': 1\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get('https://api.pexels.com/v1/search', headers=headers, params=params)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            return data.get('total_results', 0)\n",
        "        else:\n",
        "            logging.error(f\"Pexels API error {response.status_code}: {response.text}\")\n",
        "            return 0\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"Error checking Pexels for '{query}': {e}\")\n",
        "        return 0\n",
        "\n",
        "def check_shutterstock_media(query):\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {SHUTTERSTOCK_ACCESS_TOKEN}',\n",
        "        'User-Agent': 'TrendingTopicsScript'\n",
        "    }\n",
        "    params = {\n",
        "        'query': query,\n",
        "        'per_page': 1,\n",
        "        'page': 1,\n",
        "        'view': 'minimal'\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get('https://api.shutterstock.com/v2/images/search', headers=headers, params=params)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            return data.get('total_count', 0)\n",
        "        else:\n",
        "            logging.error(f\"Shutterstock API error {response.status_code}: {response.text}\")\n",
        "            return 0\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"Error checking Shutterstock for '{query}': {e}\")\n",
        "        return 0\n",
        "\n",
        "def generate_script_for_topic(topic, content_summary, style=\"Normal Script\"):\n",
        "    try:\n",
        "        style_prompts = {\n",
        "            \"Flashy Script\": \"Create a flashy, high-energy script with quick cuts, bold visuals, and impactful statements.\",\n",
        "            \"Expressive Script\": \"Compose an expressive script that evokes emotions, using descriptive language and a storytelling approach.\",\n",
        "            \"Normal Script\": \"Write a straightforward script that clearly presents the information in an engaging manner.\"\n",
        "        }\n",
        "        style_prompt = style_prompts.get(style, style_prompts[\"Normal Script\"])\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are a creative scriptwriter for short-form videos. \"\n",
        "                        \"Produce engaging, dynamic scripts that are visually compelling and can be represented \"\n",
        "                        \"using stock images and videos. The script should be suitable for a 30-second video, \"\n",
        "                        \"have a captivating introduction, a clear narrative flow, and a strong conclusion or call-to-action.\"\n",
        "                    )\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": (\n",
        "                        f\"{style_prompt}\\n\"\n",
        "                        f\"Using the following summary, write a script suitable for a 30-second video:\\n\\n\"\n",
        "                        f\"Summary: {content_summary}\\n\\n\"\n",
        "                        \"Ensure the script includes vivid descriptions and is structured with a beginning, middle, and end. \"\n",
        "                        \"Use language that resonates with the target audience, and make sure it's adaptable with widely available stock images and videos.\"\n",
        "                    )\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=800,\n",
        "            temperature=0.7,\n",
        "        )\n",
        "        script_content = response['choices'][0]['message']['content'].strip()\n",
        "        return script_content\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating script: {e}\")\n",
        "        return \"No script available.\"\n"
      ],
      "metadata": {
        "id": "O06ulUQJr9wx"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 9: Define the Main Function\n",
        "def main():\n",
        "    from IPython.display import display, Markdown, HTML\n",
        "    from prettytable import PrettyTable\n",
        "\n",
        "    # Step 1: User selects the country for Google Trends\n",
        "    print(\"Enter the country for Google Trends data (e.g., United States):\")\n",
        "    country_input = input(\"Country: \").strip()\n",
        "    matching_countries = [country for country in available_countries if country_input.lower() in country.lower()]\n",
        "    if not matching_countries:\n",
        "        print(\"No matching countries found. Exiting.\")\n",
        "        return\n",
        "    if len(matching_countries) == 1:\n",
        "        selected_country = matching_countries[0]\n",
        "    else:\n",
        "        print(\"Multiple countries found:\")\n",
        "        for idx, country in enumerate(matching_countries, start=1):\n",
        "            print(f\"{idx}. {country}\")\n",
        "        try:\n",
        "            country_selection = int(input(\"Enter the number of the country you're interested in: \"))\n",
        "            if 1 <= country_selection <= len(matching_countries):\n",
        "                selected_country = matching_countries[country_selection - 1]\n",
        "            else:\n",
        "                logging.error(\"Invalid selection. Exiting.\")\n",
        "                return\n",
        "        except ValueError:\n",
        "            logging.error(\"Invalid input. Please enter a number. Exiting.\")\n",
        "            return\n",
        "\n",
        "    selected_country_code = available_countries[selected_country]\n",
        "    print(f\"You selected: {selected_country}\")\n",
        "\n",
        "    # Step 1b: User selects the time range for Google Trends\n",
        "    print(\"\\nSelect the time range for trending topics:\")\n",
        "    print(\"1. Last 4 hours (Realtime)\")\n",
        "    print(\"2. Last 24 hours (Daily)\")\n",
        "    time_range_selection = input(\"Enter the number of the time range you're interested in: \")\n",
        "    time_range_mapping = {'1': '4h', '2': '24h'}\n",
        "    time_range = time_range_mapping.get(time_range_selection, '24h')\n",
        "\n",
        "    # List locations with daily results available\n",
        "    if time_range == '24h':\n",
        "        print(\"\\nLocations with daily trending data available:\")\n",
        "        for loc in country_code_to_pn_daily.keys():\n",
        "            print(f\"- {loc.capitalize()}\")\n",
        "\n",
        "    # Step 2: Fetch trending topics for the selected country and time range\n",
        "    google_trends_topics = fetch_trending_topics_pytrends(selected_country_code, time_range)\n",
        "\n",
        "    if not google_trends_topics:\n",
        "        logging.error(\"No trending topics found using PyTrends.\")\n",
        "        print(\"No trending topics found for the selected country and time range.\")\n",
        "        return\n",
        "\n",
        "    # Display the trending topics with additional information in a table\n",
        "    print(f\"\\nCurrent Trending Topics in {selected_country}:\\n\")\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [\"No.\", \"Topic\", \"Reason\"]\n",
        "    for idx, topic in enumerate(google_trends_topics, start=1):\n",
        "        title = topic['title']\n",
        "        reason = topic['description'] if topic['description'] else \"[Trending Topic]\"\n",
        "        table.add_row([idx, title, reason])\n",
        "    print(table)\n",
        "\n",
        "    # Allow the user to select a topic\n",
        "    try:\n",
        "        selected_idx = int(input(\"Enter the number of the topic you're interested in: \"))\n",
        "        if 1 <= selected_idx <= len(google_trends_topics):\n",
        "            selected_topic = google_trends_topics[selected_idx - 1]['title']\n",
        "            print(f\"\\nYou selected: {selected_topic}\")\n",
        "        else:\n",
        "            logging.error(\"Invalid selection. Exiting.\")\n",
        "            return\n",
        "    except ValueError:\n",
        "        logging.error(\"Invalid input. Please enter a number. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Step 3: Fetch related news articles\n",
        "    related_stories = fetch_related_news(selected_topic, limit=5)\n",
        "    if related_stories:\n",
        "        print(\"\\nRelated News Articles:\")\n",
        "        for idx, story in enumerate(related_stories, start=1):\n",
        "            display(Markdown(f\"**{idx}. {story['title']}**\"))\n",
        "    else:\n",
        "        print(\"\\nNo related news articles found.\")\n",
        "\n",
        "    # Step 4: Search for subreddits related to the topic\n",
        "    related_subreddits = search_subreddits_for_topic(selected_topic, limit=5)\n",
        "    if related_subreddits:\n",
        "        print(\"\\nRelated Subreddits:\")\n",
        "        for idx, sub in enumerate(related_subreddits, start=1):\n",
        "            display(Markdown(f\"**{idx}. {sub['name']}** - *{sub['title']}*\"))\n",
        "    else:\n",
        "        print(\"\\nNo related subreddits found.\")\n",
        "\n",
        "    # Fetch and display top posts from subreddits\n",
        "    all_posts = []\n",
        "    if related_subreddits:\n",
        "        print(\"\\nTop Reddit Posts from Related Subreddits:\")\n",
        "        for sub in related_subreddits:\n",
        "            subreddit_name = sub['name']\n",
        "            try:\n",
        "                subreddit = reddit.subreddit(subreddit_name)\n",
        "                top_posts = list(subreddit.hot(limit=5))\n",
        "                for post in top_posts:\n",
        "                    if post.over_18:\n",
        "                        continue\n",
        "                    if post.is_self and len(post.selftext) < 20:\n",
        "                        continue  # Skip short self-posts\n",
        "                    all_posts.append({'subreddit': subreddit_name, 'title': post.title, 'score': post.score, 'url': post.url})\n",
        "            except praw.exceptions.APIException as e:\n",
        "                logging.error(f\"APIException when accessing subreddit '{subreddit_name}': {e}\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error fetching posts from subreddit '{subreddit_name}': {e}\")\n",
        "\n",
        "        if all_posts:\n",
        "            # Display posts in a table with formatting\n",
        "            table = PrettyTable()\n",
        "            table.field_names = [\"No.\", \"Subreddit\", \"Title\", \"Upvotes\"]\n",
        "            for idx, post in enumerate(all_posts, start=1):\n",
        "                table.add_row([idx, post['subreddit'], post['title'], post['score']])\n",
        "            print(table)\n",
        "        else:\n",
        "            print(\"\\nNo suitable Reddit posts found.\")\n",
        "    else:\n",
        "        print(\"\\nNo Reddit posts to display.\")\n",
        "\n",
        "    # Combine news articles and Reddit posts\n",
        "    combined_content = []\n",
        "    for story in related_stories:\n",
        "        combined_content.append({'type': 'news', 'title': story['title'], 'url': story['url'], 'summary': story.get('summary', '')})\n",
        "    for post in all_posts:\n",
        "        combined_content.append({'type': 'reddit', 'title': post['title'], 'url': post['url'], 'score': post['score'], 'summary': ''})\n",
        "\n",
        "    if not combined_content:\n",
        "        logging.error(\"No content available for script generation. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Allow user to select content for script generation\n",
        "    print(\"\\nAvailable Content for Script Generation:\")\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [\"No.\", \"Type\", \"Title\", \"Upvotes\"]\n",
        "    for idx, content in enumerate(combined_content, start=1):\n",
        "        content_type = \"News Article\" if content['type'] == 'news' else \"Reddit Post\"\n",
        "        upvotes = content.get('score', '')\n",
        "        table.add_row([idx, content_type, content['title'], upvotes])\n",
        "    print(table)\n",
        "\n",
        "    selected_content_input = input(\"\\nEnter the numbers of the items you want to generate scripts for, separated by commas (e.g., 1,3,5): \")\n",
        "    try:\n",
        "        selected_indices = [int(i.strip()) - 1 for i in selected_content_input.split(',') if i.strip().isdigit()]\n",
        "        selected_items = [combined_content[i] for i in selected_indices if 0 <= i < len(combined_content)]\n",
        "        if not selected_items:\n",
        "            logging.error(\"No valid items selected. Exiting.\")\n",
        "            return\n",
        "    except ValueError:\n",
        "        logging.error(\"Invalid input. Please enter numbers separated by commas. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Fetch content and generate summaries for selected items\n",
        "    print(f\"\\nFetching content and generating summaries for selected items...\")\n",
        "    for item in selected_items:\n",
        "        title = item['title']\n",
        "        if item['type'] == 'reddit':\n",
        "            # Analyze sentiment and include upvotes\n",
        "            sentiment = analyze_sentiment(title)\n",
        "            item['sentiment'] = sentiment\n",
        "            item['summary'] = generate_fan_sentiment_summary(title, sentiment)\n",
        "        else:\n",
        "            # For news articles, use the summary if available\n",
        "            if not item['summary']:\n",
        "                item['summary'] = generate_summary(title)\n",
        "\n",
        "    # Decide Whether to Check Stock Media Availability\n",
        "    check_media = input(\"\\nDo you want to check for stock media availability for the selected items? (yes/no): \").strip().lower()\n",
        "    if check_media not in ['yes', 'y']:\n",
        "        print(\"\\nSkipping stock media availability check.\")\n",
        "        # Proceed to generate scripts without checking\n",
        "        generate_scripts_from_items(selected_items)\n",
        "        print(\"\\nScript execution completed.\")\n",
        "        return\n",
        "\n",
        "    # Continue with stock media availability check\n",
        "    print(\"\\nChecking stock media availability for the selected items...\")\n",
        "    request_count = 0\n",
        "    scripts_generated = 0\n",
        "    for item in selected_items:\n",
        "        title = item['title']\n",
        "        summary = item.get('summary', \"No summary available.\")\n",
        "        if request_count >= 190:\n",
        "            logging.error(\"Approaching API rate limits. Waiting for 60 minutes before continuing...\")\n",
        "            time.sleep(3600)  # Wait for an hour\n",
        "            request_count = 0\n",
        "        total_media = check_stock_media_availability(title)\n",
        "        request_count += 1\n",
        "        if total_media >= 10:\n",
        "            # Display summary before generating script\n",
        "            print(f\"\\nSummary: {summary}\")\n",
        "            # Generate script for this topic\n",
        "            generate_script_with_style(title, summary)\n",
        "            scripts_generated += 1\n",
        "        else:\n",
        "            print(f\"\\nSummary: {summary}\")\n",
        "            print(f\"Not enough stock media for topic: {title}\\n====\\n\")\n",
        "    if scripts_generated == 0:\n",
        "        logging.error(\"No topics with sufficient stock media were found.\")\n",
        "    else:\n",
        "        logging.info(f\"Generated {scripts_generated} script(s) based on available stock media.\")\n",
        "    print(\"\\nScript execution completed.\")\n",
        "\n",
        "def generate_scripts_from_items(selected_items):\n",
        "    for item in selected_items:\n",
        "        title = item['title']\n",
        "        summary = item.get('summary', \"No summary available.\")\n",
        "        print(f\"\\nSummary: {summary}\")\n",
        "        generate_script_with_style(title, summary)\n",
        "        print(\"====\\n\")\n",
        "\n",
        "def generate_script_with_style(topic, summary):\n",
        "    print(\"\\nSelect a script style:\")\n",
        "    print(\"1. Flashy Script\")\n",
        "    print(\"2. Expressive Script\")\n",
        "    print(\"3. Normal Script\")\n",
        "    style_choice = input(\"Enter the number of the script style you prefer: \")\n",
        "    style_mapping = {'1': 'Flashy Script', '2': 'Expressive Script', '3': 'Normal Script'}\n",
        "    style = style_mapping.get(style_choice, 'Normal Script')\n",
        "    script = generate_script_for_topic(topic, summary, style=style)\n",
        "    # Display the script with enhanced formatting\n",
        "    display(Markdown(f\"### Generated {style} for '{topic}':\\n\\n{script}\"))\n",
        "\n",
        "def generate_fan_sentiment_summary(title, sentiment):\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are an assistant that summarizes fan sentiments from online discussions into a brief overview.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"The following Reddit post has a {sentiment} sentiment:\\n\\n'{title}'\\n\\nSummarize this sentiment for inclusion in a video script.\"\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=80,\n",
        "            temperature=0.5,\n",
        "        )\n",
        "        summary = response['choices'][0]['message']['content'].strip()\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating fan sentiment summary: {e}\")\n",
        "        return \"No summary available.\"\n"
      ],
      "metadata": {
        "id": "0YOaDffLsA15"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 9: Run the Main Function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9NQJ5Wr_sEan",
        "outputId": "d052032c-4c90-4391-8edc-53b55d6d5703"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Select a country for Google Trends data:\n",
            "1. United States\n",
            "2. India\n",
            "3. Canada\n",
            "4. United Kingdom\n",
            "5. Australia\n",
            "6. Germany\n",
            "7. France\n",
            "8. Brazil\n",
            "9. Mexico\n",
            "10. Japan\n",
            "11. Russia\n",
            "12. South Korea\n",
            "13. Italy\n",
            "14. Spain\n",
            "15. Netherlands\n",
            "16. Sweden\n",
            "17. Switzerland\n",
            "18. Austria\n",
            "19. Belgium\n",
            "20. New Zealand\n",
            "Enter the number of the country you're interested in: 1\n",
            "You selected: United States\n",
            "\n",
            "Select the time range for trending topics:\n",
            "1. Last 4 hours (Realtime)\n",
            "2. Last 24 hours (Daily)\n",
            "Enter the number of the time range you're interested in: 2\n",
            "\n",
            "Current Trending Topics in United States:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**1. Penn State football** - *[Big game win]* [SV: High | Change: Up | Started: Recently]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**2. Ohio State vs Penn State** - *[College football rivalry]* [SV: High | Change: Up | Started: Recently]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**3. Georgia football** - *[Big win against Clemson]* [SV: High | Change: Up | Started: Recently]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**4. Florida vs Georgia** - *College football rivalry. [Big game upcoming.]* [SV: High | Change: Up | Started: Recently]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**5. Tennessee football** - *[Big win against Florida]* [SV: High | Change: Up | Started: Recently]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**6. Clemson football** - *[Big win against FSU]* [SV: High | Change: Up | Started: Recently]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**7. Miami football** - *[Big win against UNC]* [SV: High | Change: Up | Started: Recently]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**8. Iowa State football** - *[Big upset win]* [SV: High | Change: Up | Started: Recently]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**9. South Carolina football** - *[Big upset win]* [SV: High | Change: Up | Started: Recently]"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**10. SNL** - *(Elon Musk hosting)* [SV: High | Change: Up | Started: Recently]"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of the topic you're interested in: 1\n",
            "\n",
            "You selected: Penn State football\n",
            "\n",
            "Related News Articles:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**1. Ohio State wins 'crossroads' game at Penn State - ESPN**\nURL: https://news.google.com/rss/articles/CBMivAFBVV95cUxOSGNwSDBpTmgwQnRpWHZZd3pXRU5aMkN3QmlSa3lHRFpyakNma2dTdVUwWFd6R3hFTHl5SXVTR3poNnl6aktfZnVkR0dwQkM0N2Y3SGV1d2J0ZmNGZW1QQndSZ3V0RDBEZDktaUVrZnRnTGU5UFpCQVJwSVMtb21WLWo4Tnd0OEMyWXo4Yi1sX1RsN3dvbktVRXhIa1k1TlpWd3pTRERFUFZuSTlnU2F5Q1NWdmhDVFBNQzYyWQ?oc=5"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**2. Pat McAfee Took Shot at Fox's 'Big Noon Kickoff' Before Penn State-Ohio State Game - Sports Illustrated**\nURL: https://news.google.com/rss/articles/CBMioAFBVV95cUxORVJraGVxUkRhbFp5SVNTRHh6dTJNTzZrR1hRWW1vT0djNXNRTjZKU3paQ1dpV2E0TEltS2l2elJLZVp1cnlBRC1oZVMwZl85OWFqVkpKdmFJN1dHQ2V3REZCczlyTkZoMlJ5S0RjdTFVd0JPUUo3QnFHTzBCX0prVkZoZ181QnFSMExKLThLVW51c3ZRZlVKY0JDVHo2RXIt?oc=5"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**3. ESPN’s Pat McAfee takes shot at Big Noon Kickoff before Ohio State vs. Penn State football - cleveland.com**\nURL: https://news.google.com/rss/articles/CBMiywFBVV95cUxNbmxxN2o5Q2dIZ09GWVBhbUNaYldlVGFUc2pVWFJvejBjeDdPMWFoN09YZ2ZoREpHSEpxamVDOWxaZTEtaGFaMG9BYUJZY0dpZVZCeEItWjg4eXlnQ2xKMXhXUTd5cDNjeWRYYUJPekR5UktkVFl2RjRsWTdsWDZ2Qm1BLXUwb3JCbjFUMmdBNFAyVzdQbjM2OG4wTHptY2pjdGhOMkVuYWt0SjhHNjlJdzluS0RkdHd0c3VBcDZQMUtJMjNyTDVmWXdIc9IB3wFBVV95cUxPRGliT2VvMThzbEg5anZrMm9FOG5FM2VWQTlsS002N2g1UnhmVHU2Q3QwUEI4djJrYTc4STVRWS1uT2Z2bklXcHp5VlZWZXlTUmFJTVNOOXBQUDh3Q3dfSlhWaDNJMlBITENOemdaRFA0WUN6VElpRExhUl9yTHoxbUVJN0ZmZEFlY2xrblB3YkJLOUZRV3I2TTFYd3JIeVZwaUVZZ3BVRVZmem82Tnc2VDRUZmZCLUV4UU5sU1U3TzdjTHB3a0Z1c0NCdDVySDNHOVdOWWVxSEhIVVdXZ2ZZ?oc=5"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**4. Photos: Inside look at the atmosphere in and around the Penn State vs. Ohio State football game - Centre Daily Times**\nURL: https://news.google.com/rss/articles/CBMioAFBVV95cUxOMVc3SHV3eVBQTXFxVGREdThnU0pMVXBLUjM1TndFLXI0LUJpa0xhWjFDVVJvOTQxMUYtczNubFRQcWRGVFRjcnRvbjhUNkE4VWIxbGhra1BWa1VaR1c3QkstMnBFX3Y0N2Foc0YzYk1FZXB4WTFHZUxLbGh3enlaMHlyTXdQZ3RsRGpsM2EwRDNhN2taR2Y4bkJnTXc4SDVk0gGgAUFVX3lxTE5ROW5uUGwxdXBSSk00OEFYWXZyLXBBV1p3d1lqcE1MNlMyQ2NLOGlKd2pHemFaOC1naHdmQ2NFLUJlTHZDRURQR1dTNGRuZzFHYkJiRl84YXVxQllHZ05wdkdUeExwQnFlRUtvbmJnajhLUGNEWHdrMGV1TkVHRHR6VTdmVnRpT0IzRlc3dHMydF8zTFdmVGhCSDZPVzk5eFI?oc=5"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**5. Penn State Gets a Primetime White Out, but There’s a Catch - Statecollege.com**\nURL: https://news.google.com/rss/articles/CBMitgFBVV95cUxQZVVfbWR4SVlNYm1BUmc2amwwZjByQktuMW92eUg5Yy1GbGZ0aF9OU1c3cXZmY05zX051cmJoZW5fa3JzRS1maUNvUzEyUWtGTk01NkJ6TTJTd0hGRFkyT25BNkc0QnZFalVVRHpaUldCVGpSUnJ5a2x4QmUydlVsRlk4cG8zQ24yR2hqZmNyNnBaRXlFMWZUdlQ5Y3dhTnhvMDBxd1ZuUGFmNWlMdFJnZHdNQ3I4UQ?oc=5"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Related Subreddits:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**1. PennStateUniversity** - *Penn State University*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**2. WeArePennState** - *Penn State Athletics*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**3. CFB** - *The Internet's Tailgate*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**4. OhioStateFootball** - *THE Place to Discuss Buckeye Football*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**5. PennStateHousing** - *Penn State Housing*"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top Posts from Related Subreddits:\n",
            "\n",
            "All Fetched Posts:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**1. [PennStateUniversity] Frequently Made Posts & Questions v2 - READ THIS BEFORE POSTING!** *(Score: 150)*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**2. [PennStateUniversity] Admissions Megathread** *(Score: 49)*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**3. [PennStateUniversity] When do we ever blame James Franklin for this dismal reality** *(Score: 70)*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**4. [PennStateUniversity] Another View of Jason Smashing That Guys Phone.** *(Score: 195)*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**5. [PennStateUniversity] That loss to Ohio State now pushes James Franklin’s record to 3-19 against top 10 teams at Penn State…** *(Score: 155)*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**6. [WeArePennState] Kelce incident** *(Score: 82)*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**7. [WeArePennState] Death, taxes and losses to OSU and Michigan…** *(Score: 149)*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**8. [WeArePennState] Franklin is never gonna win the big one** *(Score: 113)*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**9. [WeArePennState] That loss to Ohio State now pushes James Franklin’s record to 3-19 against top 10 teams at Penn State…** *(Score: 56)*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**10. [CFB] Picture/Video/GIF Thread** *(Score: 20)*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**11. [CFB] Week 10 Game Thread and Postgame Thread Index** *(Score: 29)*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**12. [CFB] [Postgame Thread] South Carolina Defeats Texas A&M 44-20** *(Score: 3217)*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**13. [CFB] [Postgame Thread] Louisville Defeats Clemson 33-21** *(Score: 1621)*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**14. [CFB] Vanderbilt Linebacker Bryan Longwell shares a text he got from an Auburn Coach at 5:59 am Christmas Morning.  ** *(Score: 2433)*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**15. [OhioStateFootball] [Game Thread] #4 Ohio State at #3 Penn State • Noon, November 2, 2024 • Fox** *(Score: 102)*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**16. [OhioStateFootball] Post Game Thread | Penn State 13 - 20 Ohio State ** *(Score: 195)*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**17. [OhioStateFootball] Facts or nah? ** *(Score: 310)*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**18. [OhioStateFootball] STANDING. ON. BUSINESS. ** *(Score: 732)*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**19. [OhioStateFootball] When your team wins on the road against a Top 3 team and TTUN receives their 4th loss on the same day. ** *(Score: 207)*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**20. [PennStateHousing] Looking for an on-campus sublet in spring 2025** *(Score: 1)*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**21. [PennStateHousing] Looking for Fall 2025 Housing** *(Score: 1)*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**22. [PennStateHousing] Way to know the length of on campus housing waitlists?** *(Score: 1)*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**23. [PennStateHousing] DISCOUNTED rate at the VIEW at State College!!!** *(Score: 1)*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**24. [PennStateHousing] Looking for a sublet for the summer of 2025** *(Score: 1)*"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Available Content for Script Generation:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**1. Ohio State wins 'crossroads' game at Penn State - ESPN** *(Type: News Article)*\nURL: https://news.google.com/rss/articles/CBMivAFBVV95cUxOSGNwSDBpTmgwQnRpWHZZd3pXRU5aMkN3QmlSa3lHRFpyakNma2dTdVUwWFd6R3hFTHl5SXVTR3poNnl6aktfZnVkR0dwQkM0N2Y3SGV1d2J0ZmNGZW1QQndSZ3V0RDBEZDktaUVrZnRnTGU5UFpCQVJwSVMtb21WLWo4Tnd0OEMyWXo4Yi1sX1RsN3dvbktVRXhIa1k1TlpWd3pTRERFUFZuSTlnU2F5Q1NWdmhDVFBNQzYyWQ?oc=5"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**2. Pat McAfee Took Shot at Fox's 'Big Noon Kickoff' Before Penn State-Ohio State Game - Sports Illustrated** *(Type: News Article)*\nURL: https://news.google.com/rss/articles/CBMioAFBVV95cUxORVJraGVxUkRhbFp5SVNTRHh6dTJNTzZrR1hRWW1vT0djNXNRTjZKU3paQ1dpV2E0TEltS2l2elJLZVp1cnlBRC1oZVMwZl85OWFqVkpKdmFJN1dHQ2V3REZCczlyTkZoMlJ5S0RjdTFVd0JPUUo3QnFHTzBCX0prVkZoZ181QnFSMExKLThLVW51c3ZRZlVKY0JDVHo2RXIt?oc=5"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**3. ESPN’s Pat McAfee takes shot at Big Noon Kickoff before Ohio State vs. Penn State football - cleveland.com** *(Type: News Article)*\nURL: https://news.google.com/rss/articles/CBMiywFBVV95cUxNbmxxN2o5Q2dIZ09GWVBhbUNaYldlVGFUc2pVWFJvejBjeDdPMWFoN09YZ2ZoREpHSEpxamVDOWxaZTEtaGFaMG9BYUJZY0dpZVZCeEItWjg4eXlnQ2xKMXhXUTd5cDNjeWRYYUJPekR5UktkVFl2RjRsWTdsWDZ2Qm1BLXUwb3JCbjFUMmdBNFAyVzdQbjM2OG4wTHptY2pjdGhOMkVuYWt0SjhHNjlJdzluS0RkdHd0c3VBcDZQMUtJMjNyTDVmWXdIc9IB3wFBVV95cUxPRGliT2VvMThzbEg5anZrMm9FOG5FM2VWQTlsS002N2g1UnhmVHU2Q3QwUEI4djJrYTc4STVRWS1uT2Z2bklXcHp5VlZWZXlTUmFJTVNOOXBQUDh3Q3dfSlhWaDNJMlBITENOemdaRFA0WUN6VElpRExhUl9yTHoxbUVJN0ZmZEFlY2xrblB3YkJLOUZRV3I2TTFYd3JIeVZwaUVZZ3BVRVZmem82Tnc2VDRUZmZCLUV4UU5sU1U3TzdjTHB3a0Z1c0NCdDVySDNHOVdOWWVxSEhIVVdXZ2ZZ?oc=5"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**4. Photos: Inside look at the atmosphere in and around the Penn State vs. Ohio State football game - Centre Daily Times** *(Type: News Article)*\nURL: https://news.google.com/rss/articles/CBMioAFBVV95cUxOMVc3SHV3eVBQTXFxVGREdThnU0pMVXBLUjM1TndFLXI0LUJpa0xhWjFDVVJvOTQxMUYtczNubFRQcWRGVFRjcnRvbjhUNkE4VWIxbGhra1BWa1VaR1c3QkstMnBFX3Y0N2Foc0YzYk1FZXB4WTFHZUxLbGh3enlaMHlyTXdQZ3RsRGpsM2EwRDNhN2taR2Y4bkJnTXc4SDVk0gGgAUFVX3lxTE5ROW5uUGwxdXBSSk00OEFYWXZyLXBBV1p3d1lqcE1MNlMyQ2NLOGlKd2pHemFaOC1naHdmQ2NFLUJlTHZDRURQR1dTNGRuZzFHYkJiRl84YXVxQllHZ05wdkdUeExwQnFlRUtvbmJnajhLUGNEWHdrMGV1TkVHRHR6VTdmVnRpT0IzRlc3dHMydF8zTFdmVGhCSDZPVzk5eFI?oc=5"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**5. Penn State Gets a Primetime White Out, but There’s a Catch - Statecollege.com** *(Type: News Article)*\nURL: https://news.google.com/rss/articles/CBMitgFBVV95cUxQZVVfbWR4SVlNYm1BUmc2amwwZjByQktuMW92eUg5Yy1GbGZ0aF9OU1c3cXZmY05zX051cmJoZW5fa3JzRS1maUNvUzEyUWtGTk01NkJ6TTJTd0hGRFkyT25BNkc0QnZFalVVRHpaUldCVGpSUnJ5a2x4QmUydlVsRlk4cG8zQ24yR2hqZmNyNnBaRXlFMWZUdlQ5Y3dhTnhvMDBxd1ZuUGFmNWlMdFJnZHdNQ3I4UQ?oc=5"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**6. Frequently Made Posts & Questions v2 - READ THIS BEFORE POSTING!** *(Type: Reddit Post)*\nURL: https://www.reddit.com/r/PennStateUniversity/comments/ltbxgc/frequently_made_posts_questions_v2_read_this/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**7. Admissions Megathread** *(Type: Reddit Post)*\nURL: https://www.reddit.com/r/PennStateUniversity/comments/18n209l/admissions_megathread/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**8. When do we ever blame James Franklin for this dismal reality** *(Type: Reddit Post)*\nURL: https://www.reddit.com/r/PennStateUniversity/comments/1gibifu/when_do_we_ever_blame_james_franklin_for_this/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**9. Another View of Jason Smashing That Guys Phone.** *(Type: Reddit Post)*\nURL: https://x.com/jarrett_daveler/status/1852742194248962053?s=46"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**10. That loss to Ohio State now pushes James Franklin’s record to 3-19 against top 10 teams at Penn State…** *(Type: Reddit Post)*\nURL: https://atozsports.com/college-football/penn-state-football-hc-james-franklin-has-taken-losing-big-games-to-whole-other-level-embarrassing-happy-valley-ohio-state-football/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**11. Kelce incident** *(Type: Reddit Post)*\nURL: https://www.reddit.com/r/WeArePennState/comments/1gi8nzi/kelce_incident/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**12. Death, taxes and losses to OSU and Michigan…** *(Type: Reddit Post)*\nURL: https://www.reddit.com/r/WeArePennState/comments/1gi434h/death_taxes_and_losses_to_osu_and_michigan/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**13. Franklin is never gonna win the big one** *(Type: Reddit Post)*\nURL: https://www.reddit.com/r/WeArePennState/comments/1gi48jl/franklin_is_never_gonna_win_the_big_one/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**14. That loss to Ohio State now pushes James Franklin’s record to 3-19 against top 10 teams at Penn State…** *(Type: Reddit Post)*\nURL: https://atozsports.com/college-football/penn-state-football-hc-james-franklin-has-taken-losing-big-games-to-whole-other-level-embarrassing-happy-valley-ohio-state-football/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**15. Picture/Video/GIF Thread** *(Type: Reddit Post)*\nURL: https://www.reddit.com/r/CFB/comments/1ghx2kt/picturevideogif_thread/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**16. Week 10 Game Thread and Postgame Thread Index** *(Type: Reddit Post)*\nURL: https://www.reddit.com/r/CFB/comments/1ghyc61/week_10_game_thread_and_postgame_thread_index/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**17. [Postgame Thread] South Carolina Defeats Texas A&M 44-20** *(Type: Reddit Post)*\nURL: https://www.reddit.com/r/CFB/comments/1gidz94/postgame_thread_south_carolina_defeats_texas_am/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**18. [Postgame Thread] Louisville Defeats Clemson 33-21** *(Type: Reddit Post)*\nURL: https://www.reddit.com/r/CFB/comments/1gidzuw/postgame_thread_louisville_defeats_clemson_3321/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**19. Vanderbilt Linebacker Bryan Longwell shares a text he got from an Auburn Coach at 5:59 am Christmas Morning.  ** *(Type: Reddit Post)*\nURL: https://www.reddit.com/r/CFB/comments/1gibi95/vanderbilt_linebacker_bryan_longwell_shares_a/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**20. [Game Thread] #4 Ohio State at #3 Penn State • Noon, November 2, 2024 • Fox** *(Type: Reddit Post)*\nURL: https://www.reddit.com/r/OhioStateFootball/comments/1ghx3eu/game_thread_4_ohio_state_at_3_penn_state_noon/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**21. Post Game Thread | Penn State 13 - 20 Ohio State ** *(Type: Reddit Post)*\nURL: https://www.reddit.com/r/OhioStateFootball/comments/1gi462u/post_game_thread_penn_state_13_20_ohio_state/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**22. Facts or nah? ** *(Type: Reddit Post)*\nURL: https://i.redd.it/nqkmhwmxukyd1.jpeg"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**23. STANDING. ON. BUSINESS. ** *(Type: Reddit Post)*\nURL: https://i.redd.it/ivszaw2dhjyd1.jpeg"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**24. When your team wins on the road against a Top 3 team and TTUN receives their 4th loss on the same day. ** *(Type: Reddit Post)*\nURL: https://i.redd.it/hnyavcecclyd1.jpeg"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**25. Looking for an on-campus sublet in spring 2025** *(Type: Reddit Post)*\nURL: https://www.reddit.com/r/PennStateHousing/comments/1gig8ps/looking_for_an_oncampus_sublet_in_spring_2025/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**26. Looking for Fall 2025 Housing** *(Type: Reddit Post)*\nURL: https://www.reddit.com/r/PennStateHousing/comments/1ge91po/looking_for_fall_2025_housing/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**27. Way to know the length of on campus housing waitlists?** *(Type: Reddit Post)*\nURL: https://www.reddit.com/r/PennStateHousing/comments/1g50ei0/way_to_know_the_length_of_on_campus_housing/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**28. DISCOUNTED rate at the VIEW at State College!!!** *(Type: Reddit Post)*\nURL: https://www.reddit.com/r/PennStateHousing/comments/1g4pf0i/discounted_rate_at_the_view_at_state_college/"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**29. Looking for a sublet for the summer of 2025** *(Type: Reddit Post)*\nURL: https://www.reddit.com/r/PennStateHousing/comments/1g4ola6/looking_for_a_sublet_for_the_summer_of_2025/"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Enter the numbers of the items you want to generate scripts for, separated by commas (e.g., 1,3,5): 11\n",
            "\n",
            "Fetching content and generating summaries for selected items...\n",
            "\n",
            "Do you want to check for stock media availability for the selected items? (yes/no): no\n",
            "\n",
            "Skipping stock media availability check.\n",
            "\n",
            "Summary: The speaker expresses a strong preference to see Penn State lose indefinitely rather than witness a negative interaction between Kelce and a fan. They condemn the fan's behavior as utterly disgusting and urge others to distance themselves from such individuals.\n",
            "\n",
            "Select a script style:\n",
            "1. Flashy Script\n",
            "2. Expressive Script\n",
            "3. Normal Script\n",
            "Enter the number of the script style you prefer: 1\n",
            "\n",
            "Generated Flashy Script for 'Kelce incident':\n",
            "[Opening shot of a football field under bright stadium lights]\n",
            "\n",
            "Narrator: \"In the heart of the game, a moment that shook the gridiron.\"\n",
            "\n",
            "[Quick cuts of intense football tackles and catches]\n",
            "\n",
            "Narrator: \"The Kelce incident - where passion met precision.\"\n",
            "\n",
            "[Close-up of a player's determined eyes]\n",
            "\n",
            "Narrator: \"Inches from victory, the play that defined a season.\"\n",
            "\n",
            "[Clip of a crucial game-changing moment]\n",
            "\n",
            "Narrator: \"Witness the grit, the glory, the game-changer.\"\n",
            "\n",
            "[Montage of cheering fans and tense sidelines]\n",
            "\n",
            "Narrator: \"Because in every second, history is made.\"\n",
            "\n",
            "[Final shot of a victorious team celebrating]\n",
            "\n",
            "Narrator: \"This is the Kelce incident. This is football at its finest.\"\n",
            "\n",
            "[Call-to-action text overlay]\n",
            "\n",
            "Narrator: \"Stay tuned for more epic moments. Like, share, and tag your squad! #KelceIncident #GameChanger\"\n",
            "\n",
            "====\n",
            "\n",
            "\n",
            "Script execution completed.\n"
          ]
        }
      ]
    }
  ]
}