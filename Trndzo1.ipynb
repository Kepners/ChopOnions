{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1s-3u0czUndpxCkgirEnqndEDGpEPjpJ_",
      "authorship_tag": "ABX9TyMKFL4incFRqFLjOT2rQsJH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kepners/ChopOnions/blob/main/Trndzo1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Install Required Libraries\n",
        "!pip install openai==0.27.8 praw==7.7.0 pytrends==4.9.0 feedparser==6.0.10 requests cachetools==5.3.1 prettytable==3.7.0 textblob==0.17.1 python-dotenv==0.21.0 fuzzywuzzy==0.18.0 python-Levenshtein==0.20.9\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAUVB0guTX2b",
        "outputId": "a4afcfbd-1595-4416-9835-1e07f4257dca",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai==0.27.8\n",
            "  Downloading openai-0.27.8-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting praw==7.7.0\n",
            "  Downloading praw-7.7.0-py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting pytrends==4.9.0\n",
            "  Downloading pytrends-4.9.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting feedparser==6.0.10\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Collecting cachetools==5.3.1\n",
            "  Downloading cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting prettytable==3.7.0\n",
            "  Downloading prettytable-3.7.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: textblob==0.17.1 in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Collecting python-dotenv==0.21.0\n",
            "  Downloading python_dotenv-0.21.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting fuzzywuzzy==0.18.0\n",
            "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting python-Levenshtein==0.20.9\n",
            "  Downloading python_Levenshtein-0.20.9-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (4.66.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (3.10.10)\n",
            "Collecting prawcore<3,>=2.1 (from praw==7.7.0)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting update-checker>=0.18 (from praw==7.7.0)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw==7.7.0) (1.8.0)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from pytrends==4.9.0) (2.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pytrends==4.9.0) (5.3.0)\n",
            "Collecting sgmllib3k (from feedparser==6.0.10)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable==3.7.0) (0.2.13)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob==0.17.1) (3.8.1)\n",
            "Collecting Levenshtein==0.20.9 (from python-Levenshtein==0.20.9)\n",
            "  Downloading Levenshtein-0.20.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting rapidfuzz<3.0.0,>=2.3.0 (from Levenshtein==0.20.9->python-Levenshtein==0.20.9)\n",
            "  Downloading rapidfuzz-2.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob==0.17.1) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob==0.17.1) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob==0.17.1) (2024.9.11)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends==4.9.0) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends==4.9.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends==4.9.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends==4.9.0) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.27.8) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25->pytrends==4.9.0) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->openai==0.27.8) (0.2.0)\n",
            "Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading praw-7.7.0-py3-none-any.whl (189 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.4/189.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytrends-4.9.0-py3-none-any.whl (15 kB)\n",
            "Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
            "Downloading prettytable-3.7.0-py3-none-any.whl (27 kB)\n",
            "Downloading python_dotenv-0.21.0-py3-none-any.whl (18 kB)\n",
            "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
            "Downloading python_Levenshtein-0.20.9-py3-none-any.whl (9.4 kB)\n",
            "Downloading Levenshtein-0.20.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (174 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/174.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Downloading rapidfuzz-2.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=ef71676a481d89d3c6375caf2b61b1e033ae3128f833fbedb75b6f303304f78a\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, fuzzywuzzy, rapidfuzz, python-dotenv, prettytable, feedparser, cachetools, update-checker, prawcore, Levenshtein, pytrends, python-Levenshtein, praw, openai\n",
            "  Attempting uninstall: prettytable\n",
            "    Found existing installation: prettytable 3.11.0\n",
            "    Uninstalling prettytable-3.11.0:\n",
            "      Successfully uninstalled prettytable-3.11.0\n",
            "  Attempting uninstall: cachetools\n",
            "    Found existing installation: cachetools 5.5.0\n",
            "    Uninstalling cachetools-5.5.0:\n",
            "      Successfully uninstalled cachetools-5.5.0\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 1.52.2\n",
            "    Uninstalling openai-1.52.2:\n",
            "      Successfully uninstalled openai-1.52.2\n",
            "Successfully installed Levenshtein-0.20.9 cachetools-5.3.1 feedparser-6.0.10 fuzzywuzzy-0.18.0 openai-0.27.8 praw-7.7.0 prawcore-2.4.0 prettytable-3.7.0 python-Levenshtein-0.20.9 python-dotenv-0.21.0 pytrends-4.9.0 rapidfuzz-2.15.2 sgmllib3k-1.0.0 update-checker-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 2: Mount Google Drive and Load Environment Variables\n",
        "from google.colab import drive\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to your .env file in Google Drive\n",
        "dotenv_path = '/content/drive/MyDrive/Secrets/.env'\n",
        "\n",
        "# Load the environment variables from the .env file\n",
        "load_dotenv(dotenv_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIud78FOZR3M",
        "outputId": "93f381f4-3c1d-4c16-d920-0c0a328d0ccf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 3: Configure Logging\n",
        "import logging\n",
        "\n",
        "# Configure Logging to write to a log file and suppress console output\n",
        "log_file = 'app.log'\n",
        "logging.basicConfig(\n",
        "    level=logging.ERROR,  # Only log ERROR and CRITICAL\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(log_file),\n",
        "        logging.StreamHandler(open(os.devnull, 'w'))  # Suppress console output\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "ujJviMETtuLY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 4: Download NLTK Data Silently\n",
        "import nltk\n",
        "import sys\n",
        "from contextlib import contextmanager\n",
        "\n",
        "@contextmanager\n",
        "def suppress_stdout():\n",
        "    with open(os.devnull, \"w\") as devnull:\n",
        "        old_stdout = sys.stdout\n",
        "        sys.stdout = devnull\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n",
        "\n",
        "with suppress_stdout():\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    nltk.download('brown')\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('movie_reviews')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGPK1IRBXfBm",
        "outputId": "760673bb-eff1-4934-cea6-02dbb6b34331"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 5: Create Utils Directory\n",
        "import os\n",
        "\n",
        "utils_dir = 'utils'\n",
        "if not os.path.exists(utils_dir):\n",
        "    os.makedirs(utils_dir)\n"
      ],
      "metadata": {
        "id": "RPd0MOuGTaTM",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 6: Create data_processing.py\n",
        "data_processing_code = \"\"\"\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from textblob import TextBlob\n",
        "\n",
        "def extract_keywords(text, max_keywords=10):\n",
        "    \\\"\"\"\n",
        "    Extracts up to `max_keywords` nouns from the input `text`.\n",
        "    \\\"\"\"\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords and non-alphabetic tokens\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "    # Get part-of-speech tags\n",
        "    tagged_words = pos_tag(filtered_words)\n",
        "    # Keep nouns and proper nouns\n",
        "    keywords = [word for word, pos in tagged_words if pos.startswith('NN')]\n",
        "    # Limit the number of keywords\n",
        "    return keywords[:max_keywords]\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    \\\"\"\"\n",
        "    Analyzes the sentiment of the given text and returns it.\n",
        "    \\\"\"\"\n",
        "    blob = TextBlob(text)\n",
        "    polarity = blob.sentiment.polarity\n",
        "    if polarity > 0.1:\n",
        "        return 'Positive'\n",
        "    elif polarity < -0.1:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\"\"\"\n",
        "with open(os.path.join('utils', 'data_processing.py'), 'w') as file:\n",
        "    file.write(data_processing_code)\n"
      ],
      "metadata": {
        "id": "bNicEhLDr06j"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 7: Initialize API Clients and Configure Logging\n",
        "import praw\n",
        "import openai\n",
        "import logging\n",
        "import warnings\n",
        "from utils.data_processing import extract_keywords, analyze_sentiment\n",
        "from cachetools import TTLCache, cached\n",
        "from pytrends.request import TrendReq\n",
        "import feedparser\n",
        "import requests\n",
        "import time\n",
        "import os\n",
        "from fuzzywuzzy import process\n",
        "\n",
        "# Suppress PRAW warnings about asynchronous environments\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='praw')\n",
        "logging.getLogger('praw').setLevel(logging.CRITICAL)\n",
        "\n",
        "# Initialize Reddit API using PRAW\n",
        "try:\n",
        "    reddit = praw.Reddit(\n",
        "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
        "        client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
        "        user_agent=os.getenv('REDDIT_USER_AGENT', 'script:TrendingTopicsScript:1.0 (by u/yourusername)')\n",
        "    )\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error initializing Reddit API: {e}\")\n",
        "\n",
        "# Initialize OpenAI API\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Initialize Pexels API key\n",
        "PEXELS_API_KEY = os.getenv('PEXELS_API_KEY')\n",
        "\n",
        "# Initialize Shutterstock Access Token\n",
        "SHUTTERSTOCK_ACCESS_TOKEN = os.getenv('SHUTTERSTOCK_ACCESS_TOKEN')\n",
        "\n",
        "# Define a list of countries with their codes, including common abbreviations\n",
        "available_countries = {\n",
        "    'United States': 'US',\n",
        "    'US': 'US',\n",
        "    'USA': 'US',\n",
        "    'India': 'IN',\n",
        "    'Canada': 'CA',\n",
        "    'United Kingdom': 'GB',\n",
        "    'UK': 'GB',\n",
        "    'Great Britain': 'GB',\n",
        "    'Australia': 'AU',\n",
        "    'Germany': 'DE',\n",
        "    'France': 'FR',\n",
        "    'Brazil': 'BR',\n",
        "    'Mexico': 'MX',\n",
        "    'Japan': 'JP',\n",
        "    'Russia': 'RU',\n",
        "    'South Korea': 'KR',\n",
        "    'Korea': 'KR',\n",
        "    'Italy': 'IT',\n",
        "    'Spain': 'ES',\n",
        "    'Netherlands': 'NL',\n",
        "    'Sweden': 'SE',\n",
        "    'Switzerland': 'CH',\n",
        "    'Austria': 'AT',\n",
        "    'Belgium': 'BE',\n",
        "    'New Zealand': 'NZ'\n",
        "}\n",
        "\n",
        "# Mapping from country codes to PyTrends 'pn' parameter for trending searches\n",
        "# For realtime_trending_searches, the acceptable 'pn' values are specific\n",
        "country_code_to_pn_realtime = {\n",
        "    'US': 'US',\n",
        "    'IN': 'IN',\n",
        "    'GB': 'GB',\n",
        "    'CA': 'CA',\n",
        "    'AU': 'AU',\n",
        "    'DE': 'DE',\n",
        "    'FR': 'FR',\n",
        "    'BR': 'BR',\n",
        "    'MX': 'MX',\n",
        "    'JP': 'JP',\n",
        "    'RU': 'RU',\n",
        "    'KR': 'KR'  # South Korea\n",
        "}\n",
        "\n",
        "# For trending_searches, the acceptable 'pn' values are:\n",
        "country_code_to_pn_daily = {\n",
        "    'united_states': 'united_states',\n",
        "    'us': 'united_states',\n",
        "    'india': 'india',\n",
        "    'united_kingdom': 'united_kingdom',\n",
        "    'uk': 'united_kingdom',\n",
        "    'gb': 'united_kingdom',\n",
        "    'canada': 'canada',\n",
        "    'australia': 'australia',\n",
        "    'germany': 'germany',\n",
        "    'france': 'france',\n",
        "    'brazil': 'brazil'\n",
        "    # Note: Limited countries are available for trending_searches\n",
        "}\n"
      ],
      "metadata": {
        "id": "C5RZdBozr4hJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d5f0adb-724d-42ef-b260-2d3c11358d71"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Version 7.7.0 of praw is outdated. Version 7.8.1 was released Friday October 25, 2024.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 8: Define Supporting Functions\n",
        "from fuzzywuzzy import process\n",
        "\n",
        "def get_matching_country(input_country, available_countries):\n",
        "    # Combine country names and codes\n",
        "    country_list = list(available_countries.keys())\n",
        "    # Use fuzzy matching to find the best match\n",
        "    match, score = process.extractOne(input_country, country_list)\n",
        "    if score >= 70:  # Threshold can be adjusted\n",
        "        return match\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def fetch_trending_topics_pytrends(selected_country_code, time_range):\n",
        "    try:\n",
        "        pytrends = TrendReq(hl='en-US', tz=360)\n",
        "        if time_range == '4h':\n",
        "            # Realtime trending searches\n",
        "            pn = country_code_to_pn_realtime.get(selected_country_code)\n",
        "            if not pn:\n",
        "                logging.error(f\"Realtime trending searches not available for the selected country.\")\n",
        "                return []\n",
        "            df = pytrends.realtime_trending_searches(pn=pn)\n",
        "        else:\n",
        "            # Daily trending searches\n",
        "            pn = country_code_to_pn_daily.get(selected_country_code.lower())\n",
        "            if not pn:\n",
        "                logging.error(f\"Daily trending searches not available for the selected country.\")\n",
        "                return []\n",
        "            df = pytrends.trending_searches(pn=pn)\n",
        "        if df.empty:\n",
        "            return []\n",
        "        trending_topics = []\n",
        "        for index, row in df.iterrows():\n",
        "            title = row[0]\n",
        "            # Generate a brief reason why the topic is trending\n",
        "            reason = generate_trend_reason(title)\n",
        "            trending_topics.append({\n",
        "                'title': title,\n",
        "                'description': reason,  # Brief reason why it's trending\n",
        "                'sv': 'High',       # Placeholder for Search Volume\n",
        "                'change': 'Up',     # Placeholder for Change\n",
        "                'started': 'Recently'   # Placeholder\n",
        "            })\n",
        "        return trending_topics[:10]  # Limit to top 10\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching Google Trends data via PyTrends: {e}\")\n",
        "        return []\n",
        "\n",
        "def generate_trend_reason(topic):\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are an assistant that provides brief reasons why a topic is trending. Your responses should be concise, in brackets, and two to four words long.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Why is '{topic}' trending? Provide a brief reason in brackets, two to four words long.\"\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=20,\n",
        "            temperature=0.5,\n",
        "        )\n",
        "        reason = response['choices'][0]['message']['content'].strip()\n",
        "        return reason\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating trend reason: {e}\")\n",
        "        return \"[Trending Topic]\"\n",
        "\n",
        "def generate_summary(content):\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a concise summarizer. Provide a clear and brief two-sentence summary of the following content.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": content\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=80,\n",
        "            temperature=0.5,\n",
        "        )\n",
        "        summary = response['choices'][0]['message']['content'].strip()\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating summary: {e}\")\n",
        "        return \"No summary available.\"\n",
        "\n",
        "def search_subreddits_for_topic(topic, limit=5):\n",
        "    \"\"\"\n",
        "    Searches for subreddits related to the topic.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        related_subreddits = []\n",
        "        subreddits_found = reddit.subreddits.search_by_name(query=topic, exact=False, include_nsfw=False)\n",
        "        for subreddit in subreddits_found:\n",
        "            # Fetch subreddit details\n",
        "            try:\n",
        "                subreddit_details = reddit.subreddit(subreddit.display_name)\n",
        "                # Check if subreddit is accessible\n",
        "                if subreddit_details.over18 or subreddit_details.subreddit_type in ['private', 'restricted']:\n",
        "                    continue\n",
        "                related_subreddits.append({\n",
        "                    'name': subreddit.display_name,\n",
        "                    'title': subreddit.title,\n",
        "                    'description': subreddit.public_description or ''\n",
        "                })\n",
        "                if len(related_subreddits) >= limit:\n",
        "                    break\n",
        "            except Exception as inner_e:\n",
        "                logging.error(f\"Error accessing subreddit '{subreddit.display_name}': {inner_e}\")\n",
        "                continue\n",
        "        return related_subreddits\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error searching for subreddits: {e}\")\n",
        "        return []\n",
        "\n",
        "def fetch_related_news(topic, limit=5):\n",
        "    \"\"\"\n",
        "    Fetches related news articles using Google News RSS feed.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        feed_url = f\"https://news.google.com/rss/search?q={requests.utils.quote(topic)}&hl=en-US&gl=US&ceid=US:en\"\n",
        "        feed = feedparser.parse(feed_url)\n",
        "        related_stories = []\n",
        "        for entry in feed.entries[:limit]:\n",
        "            title = entry.title\n",
        "            summary = entry.summary if 'summary' in entry else ''\n",
        "            link = entry.link\n",
        "            related_stories.append({'title': title, 'summary': summary, 'url': link})\n",
        "        return related_stories\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching related news: {e}\")\n",
        "        return []\n",
        "\n",
        "@cached(TTLCache(maxsize=1000, ttl=86400))\n",
        "def check_stock_media_availability(topic):\n",
        "    # Extract keywords from the topic\n",
        "    keywords = extract_keywords(topic)\n",
        "    if not keywords:\n",
        "        logging.error(f\"No valid keywords extracted for topic: {topic}\")\n",
        "        return 0\n",
        "    # Join keywords into a query string\n",
        "    query = ' '.join(keywords)[:100]\n",
        "    total_results = 0\n",
        "    # Check Pexels\n",
        "    total_results += check_pexels_media(query)\n",
        "    # Check Shutterstock\n",
        "    total_results += check_shutterstock_media(query)\n",
        "    return total_results\n",
        "\n",
        "def check_pexels_media(query):\n",
        "    headers = {\n",
        "        'Authorization': PEXELS_API_KEY,\n",
        "        'User-Agent': 'TrendingTopicsScript'\n",
        "    }\n",
        "    params = {\n",
        "        'query': query,\n",
        "        'per_page': 1,\n",
        "        'page': 1\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get('https://api.pexels.com/v1/search', headers=headers, params=params)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            return data.get('total_results', 0)\n",
        "        else:\n",
        "            logging.error(f\"Pexels API error {response.status_code}: {response.text}\")\n",
        "            return 0\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"Error checking Pexels for '{query}': {e}\")\n",
        "        return 0\n",
        "\n",
        "def check_shutterstock_media(query):\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {SHUTTERSTOCK_ACCESS_TOKEN}',\n",
        "        'User-Agent': 'TrendingTopicsScript'\n",
        "    }\n",
        "    params = {\n",
        "        'query': query,\n",
        "        'per_page': 1,\n",
        "        'page': 1,\n",
        "        'view': 'minimal'\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get('https://api.shutterstock.com/v2/images/search', headers=headers, params=params)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            return data.get('total_count', 0)\n",
        "        else:\n",
        "            logging.error(f\"Shutterstock API error {response.status_code}: {response.text}\")\n",
        "            return 0\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"Error checking Shutterstock for '{query}': {e}\")\n",
        "        return 0\n",
        "\n",
        "def generate_script_for_topic(topic, content_summary, style=\"Normal Script\"):\n",
        "    try:\n",
        "        style_prompts = {\n",
        "            \"Flashy Script\": \"Create a flashy, high-energy script with quick cuts, bold visuals, and impactful statements.\",\n",
        "            \"Expressive Script\": \"Compose an expressive script that evokes emotions, using descriptive language and a storytelling approach.\",\n",
        "            \"Normal Script\": \"Write a straightforward script that clearly presents the information in an engaging manner.\"\n",
        "        }\n",
        "        style_prompt = style_prompts.get(style, style_prompts[\"Normal Script\"])\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are a creative scriptwriter for short-form videos. \"\n",
        "                        \"Produce engaging, dynamic scripts that are visually compelling and can be represented \"\n",
        "                        \"using stock images and videos. The script should be suitable for a 30-second video, \"\n",
        "                        \"have a captivating introduction, a clear narrative flow, and a strong conclusion or call-to-action.\"\n",
        "                    )\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": (\n",
        "                        f\"{style_prompt}\\n\"\n",
        "                        f\"Using the following summary, write a script suitable for a 30-second video:\\n\\n\"\n",
        "                        f\"Summary: {content_summary}\\n\\n\"\n",
        "                        \"Ensure the script includes vivid descriptions and is structured with a beginning, middle, and end. \"\n",
        "                        \"Use language that resonates with the target audience, and make sure it's adaptable with widely available stock images and videos.\"\n",
        "                    )\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=800,\n",
        "            temperature=0.7,\n",
        "        )\n",
        "        script_content = response['choices'][0]['message']['content'].strip()\n",
        "        return script_content\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating script: {e}\")\n",
        "        return \"No script available.\"\n",
        "\n",
        "def generate_fan_sentiment_summary(title, sentiment):\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are an assistant that summarizes fan sentiments from online discussions into a brief overview.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"The following Reddit post has a {sentiment} sentiment:\\n\\n'{title}'\\n\\nSummarize this sentiment for inclusion in a video script.\"\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=80,\n",
        "            temperature=0.5,\n",
        "        )\n",
        "        summary = response['choices'][0]['message']['content'].strip()\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating fan sentiment summary: {e}\")\n",
        "        return \"No summary available.\"\n"
      ],
      "metadata": {
        "id": "-2yQIa03r7Ml"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 9: Define the Main Function\n",
        "def main():\n",
        "    from IPython.display import display, Markdown, HTML\n",
        "    from prettytable import PrettyTable\n",
        "\n",
        "    # Step 1: User selects the country for Google Trends\n",
        "    print(\"Enter the country for Google Trends data (e.g., United States):\")\n",
        "    country_input = input(\"Country: \").strip()\n",
        "    matching_country = get_matching_country(country_input, available_countries)\n",
        "    if not matching_country:\n",
        "        print(\"No matching countries found. Exiting.\")\n",
        "        return\n",
        "    selected_country = matching_country\n",
        "    selected_country_code = available_countries[selected_country]\n",
        "    print(f\"You selected: {selected_country}\")\n",
        "\n",
        "    # Step 1b: User selects the time range for Google Trends\n",
        "    print(\"\\nSelect the time range for trending topics:\")\n",
        "    print(\"1. Last 4 hours (Realtime)\")\n",
        "    print(\"2. Last 24 hours (Daily)\")\n",
        "    time_range_selection = input(\"Enter the number of the time range you're interested in: \")\n",
        "    time_range_mapping = {'1': '4h', '2': '24h'}\n",
        "    time_range = time_range_mapping.get(time_range_selection)\n",
        "    if not time_range:\n",
        "        print(\"Invalid selection. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # List locations with daily results available\n",
        "    if time_range == '24h':\n",
        "        print(\"\\nLocations with daily trending data available:\")\n",
        "        for loc in country_code_to_pn_daily.keys():\n",
        "            print(f\"- {loc.capitalize()}\")\n",
        "\n",
        "    # Step 2: Fetch trending topics for the selected country and time range\n",
        "    google_trends_topics = fetch_trending_topics_pytrends(selected_country_code, time_range)\n",
        "\n",
        "    if not google_trends_topics:\n",
        "        logging.error(\"No trending topics found using PyTrends.\")\n",
        "        print(\"No trending topics found for the selected country and time range.\")\n",
        "        return\n",
        "\n",
        "    # Display the trending topics with additional information in a table\n",
        "    print(f\"\\nCurrent Trending Topics in {selected_country}:\\n\")\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [\"No.\", \"Topic\", \"Reason\"]\n",
        "    for idx, topic in enumerate(google_trends_topics, start=1):\n",
        "        title = topic['title']\n",
        "        reason = topic['description'] if topic['description'] else \"[Trending Topic]\"\n",
        "        table.add_row([idx, title, reason])\n",
        "    print(table)\n",
        "\n",
        "    # Allow the user to select a topic\n",
        "    try:\n",
        "        selected_idx = int(input(\"Enter the number of the topic you're interested in: \"))\n",
        "        if 1 <= selected_idx <= len(google_trends_topics):\n",
        "            selected_topic = google_trends_topics[selected_idx - 1]['title']\n",
        "            print(f\"\\nYou selected: {selected_topic}\")\n",
        "        else:\n",
        "            logging.error(\"Invalid selection. Exiting.\")\n",
        "            return\n",
        "    except ValueError:\n",
        "        logging.error(\"Invalid input. Please enter a number. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Step 3: Fetch related news articles\n",
        "    related_stories = fetch_related_news(selected_topic, limit=5)\n",
        "    if related_stories:\n",
        "        print(\"\\nRelated News Articles:\")\n",
        "        for idx, story in enumerate(related_stories, start=1):\n",
        "            display(Markdown(f\"**{idx}. {story['title']}**\"))\n",
        "    else:\n",
        "        print(\"\\nNo related news articles found.\")\n",
        "\n",
        "    # Step 4: Search for subreddits related to the topic\n",
        "    related_subreddits = search_subreddits_for_topic(selected_topic, limit=5)\n",
        "    if related_subreddits:\n",
        "        print(\"\\nRelated Subreddits:\")\n",
        "        for idx, sub in enumerate(related_subreddits, start=1):\n",
        "            display(Markdown(f\"**{idx}. {sub['name']}** - *{sub['title']}*\"))\n",
        "    else:\n",
        "        print(\"\\nNo related subreddits found.\")\n",
        "\n",
        "    # Fetch and display top posts from subreddits\n",
        "    all_posts = []\n",
        "    if related_subreddits:\n",
        "        print(\"\\nTop Reddit Posts from Related Subreddits:\")\n",
        "        for sub in related_subreddits:\n",
        "            subreddit_name = sub['name']\n",
        "            try:\n",
        "                subreddit = reddit.subreddit(subreddit_name)\n",
        "                top_posts = list(subreddit.hot(limit=5))\n",
        "                for post in top_posts:\n",
        "                    if post.over_18:\n",
        "                        continue\n",
        "                    if post.is_self and len(post.selftext) < 20:\n",
        "                        continue  # Skip short self-posts\n",
        "                    all_posts.append({'subreddit': subreddit_name, 'title': post.title, 'score': post.score, 'url': post.url})\n",
        "            except praw.exceptions.APIException as e:\n",
        "                logging.error(f\"APIException when accessing subreddit '{subreddit_name}': {e}\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error fetching posts from subreddit '{subreddit_name}': {e}\")\n",
        "\n",
        "        if all_posts:\n",
        "            # Display posts in a table with formatting\n",
        "            table = PrettyTable()\n",
        "            table.field_names = [\"No.\", \"Subreddit\", \"Title\", \"Upvotes\"]\n",
        "            for idx, post in enumerate(all_posts, start=1):\n",
        "                table.add_row([idx, post['subreddit'], post['title'], post['score']])\n",
        "            print(table)\n",
        "        else:\n",
        "            print(\"\\nNo suitable Reddit posts found.\")\n",
        "    else:\n",
        "        print(\"\\nNo Reddit posts to display.\")\n",
        "\n",
        "    # Combine news articles and Reddit posts\n",
        "    combined_content = []\n",
        "    for story in related_stories:\n",
        "        combined_content.append({'type': 'news', 'title': story['title'], 'url': story['url'], 'summary': story.get('summary', '')})\n",
        "    for post in all_posts:\n",
        "        combined_content.append({'type': 'reddit', 'title': post['title'], 'url': post['url'], 'score': post['score'], 'summary': ''})\n",
        "\n",
        "    if not combined_content:\n",
        "        logging.error(\"No content available for script generation. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Allow user to select content for script generation\n",
        "    print(\"\\nAvailable Content for Script Generation:\")\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [\"No.\", \"Type\", \"Title\", \"Upvotes\"]\n",
        "    for idx, content in enumerate(combined_content, start=1):\n",
        "        content_type = \"News Article\" if content['type'] == 'news' else \"Reddit Post\"\n",
        "        upvotes = content.get('score', '')\n",
        "        table.add_row([idx, content_type, content['title'], upvotes])\n",
        "    print(table)\n",
        "\n",
        "    selected_content_input = input(\"\\nEnter the numbers of the items you want to generate scripts for, separated by commas (e.g., 1,3,5): \")\n",
        "    try:\n",
        "        selected_indices = [int(i.strip()) - 1 for i in selected_content_input.split(',') if i.strip().isdigit()]\n",
        "        selected_items = [combined_content[i] for i in selected_indices if 0 <= i < len(combined_content)]\n",
        "        if not selected_items:\n",
        "            logging.error(\"No valid items selected. Exiting.\")\n",
        "            return\n",
        "    except ValueError:\n",
        "        logging.error(\"Invalid input. Please enter numbers separated by commas. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Fetch content and generate summaries for selected items\n",
        "    print(f\"\\nFetching content and generating summaries for selected items...\")\n",
        "    for item in selected_items:\n",
        "        title = item['title']\n",
        "        if item['type'] == 'reddit':\n",
        "            # Analyze sentiment and include upvotes\n",
        "            sentiment = analyze_sentiment(title)\n",
        "            item['sentiment'] = sentiment\n",
        "            item['summary'] = generate_fan_sentiment_summary(title, sentiment)\n",
        "        else:\n",
        "            # For news articles, use the summary if available\n",
        "            if not item['summary']:\n",
        "                item['summary'] = generate_summary(title)\n",
        "\n",
        "    # Decide Whether to Check Stock Media Availability\n",
        "    check_media = input(\"\\nDo you want to check for stock media availability for the selected items? (yes/no): \").strip().lower()\n",
        "    if check_media not in ['yes', 'y']:\n",
        "        print(\"\\nSkipping stock media availability check.\")\n",
        "        # Proceed to generate scripts without checking\n",
        "        generate_scripts_from_items(selected_items)\n",
        "        print(\"\\nScript execution completed.\")\n",
        "        return\n",
        "\n",
        "    # Continue with stock media availability check\n",
        "    print(\"\\nChecking stock media availability for the selected items...\")\n",
        "    request_count = 0\n",
        "    scripts_generated = 0\n",
        "    for item in selected_items:\n",
        "        title = item['title']\n",
        "        summary = item.get('summary', \"No summary available.\")\n",
        "        if request_count >= 190:\n",
        "            logging.error(\"Approaching API rate limits. Waiting for 60 minutes before continuing...\")\n",
        "            time.sleep(3600)  # Wait for an hour\n",
        "            request_count = 0\n",
        "        total_media = check_stock_media_availability(title)\n",
        "        request_count += 1\n",
        "        if total_media >= 10:\n",
        "            # Display summary before generating script\n",
        "            print(f\"\\nSummary: {summary}\")\n",
        "            # Generate script for this topic\n",
        "            generate_script_with_style(title, summary)\n",
        "            scripts_generated += 1\n",
        "        else:\n",
        "            print(f\"\\nSummary: {summary}\")\n",
        "            print(f\"Not enough stock media for topic: {title}\\n====\\n\")\n",
        "    if scripts_generated == 0:\n",
        "        logging.error(\"No topics with sufficient stock media were found.\")\n",
        "    else:\n",
        "        logging.info(f\"Generated {scripts_generated} script(s) based on available stock media.\")\n",
        "    print(\"\\nScript execution completed.\")\n",
        "\n",
        "def generate_scripts_from_items(selected_items):\n",
        "    for item in selected_items:\n",
        "        title = item['title']\n",
        "        summary = item.get('summary', \"No summary available.\")\n",
        "        print(f\"\\nSummary: {summary}\")\n",
        "        generate_script_with_style(title, summary)\n",
        "        print(\"====\\n\")\n",
        "\n",
        "def generate_script_with_style(topic, summary):\n",
        "    print(\"\\nSelect a script style:\")\n",
        "    print(\"1. Flashy Script\")\n",
        "    print(\"2. Expressive Script\")\n",
        "    print(\"3. Normal Script\")\n",
        "    style_choice = input(\"Enter the number of the script style you prefer: \")\n",
        "    style_mapping = {'1': 'Flashy Script', '2': 'Expressive Script', '3': 'Normal Script'}\n",
        "    style = style_mapping.get(style_choice, 'Normal Script')\n",
        "    script = generate_script_for_topic(topic, summary, style=style)\n",
        "    # Display the script with enhanced formatting\n",
        "    display(Markdown(f\"### Generated {style} for '{topic}':\\n\\n{script}\"))\n"
      ],
      "metadata": {
        "id": "O06ulUQJr9wx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 10: Run the Main Function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "0YOaDffLsA15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ad7bd782-7cb4-4be4-b73f-27cee2abc093"
      },
      "execution_count": 11,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the country for Google Trends data (e.g., United States):\n",
            "Country: uk\n",
            "You selected: UK\n",
            "\n",
            "Select the time range for trending topics:\n",
            "1. Last 4 hours (Realtime)\n",
            "2. Last 24 hours (Daily)\n",
            "Enter the number of the time range you're interested in: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-d07960b35f4a>:35: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  title = row[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Current Trending Topics in UK:\n",
            "\n",
            "+-----+-----------------------------------------------------------------------------------------------+----------------------------------------+\n",
            "| No. |                                             Topic                                             |                 Reason                 |\n",
            "+-----+-----------------------------------------------------------------------------------------------+----------------------------------------+\n",
            "|  1  |                    Tottenham Hotspur F.C., Aston Villa F.C., Premier League                   |         [Premier League match]         |\n",
            "|  2  |                        FC Barcelona, RCD Espanyol de Barcelona, LaLiga                        |          [El Clásico rivalry]          |\n",
            "|  3  |                        Rangers F.C., Philippe Clement, Motherwell F.C.                        |         [Sports news coverage]         |\n",
            "|  4  |      Fantasy football, Manchester United F.C., James Maddison, Chelsea F.C., Bryan Mbeumo     | [Football matches, player performance] |\n",
            "|  5  |                 Wrexham A.F.C., FA Cup, Harrogate Town A.F.C., EFL League Two                 |             [FA Cup upset]             |\n",
            "|  6  |                       Kilmarnock F.C., Dundee F.C., Scottish Premiership                      |      [Scottish Premiership match]      |\n",
            "|  7  |                         Burnley F.C., Millwall F.C., EFL Championship                         |     [Promotion battle intensifies]     |\n",
            "|  8  | Hibernian F.C., Dundee United F.C., Scottish Premiership, David Gray, Hibernian Football Club |      [Scottish Premiership match]      |\n",
            "|  9  |                             Trabzonspor, Fenerbahçe SK, Süper Lig                             |        [Football rivalry match]        |\n",
            "|  10 |                               ACF Fiorentina, Torino FC, Serie A                              |       [Fiorentina-Torino match]        |\n",
            "+-----+-----------------------------------------------------------------------------------------------+----------------------------------------+\n",
            "Enter the number of the topic you're interested in: 4\n",
            "\n",
            "You selected: Fantasy football, Manchester United F.C., James Maddison, Chelsea F.C., Bryan Mbeumo\n",
            "\n",
            "Related News Articles:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**1. Fantasy Premier League: What we learned from Gameweek 6 – Don’t panic after Palmer’s awesome foursome - The Athletic**"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**2. FPL Gameweek 9 Price Changes: Tottenham's Son On The Rise - Sportscasting**"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**3. Fantasy Premier League: Eight takeaways from Gameweek 3 of FPL - The Football Faithful**"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**4. FPL Gameweek 30 early Scout Picks: Chelsea double-up - Fantasy Football Scout**"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**5. FPL review: Cole Palmer creates new record but don’t rip up your squad for one player - Belfast Telegraph**"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "No related subreddits found.\n",
            "\n",
            "No Reddit posts to display.\n",
            "\n",
            "Available Content for Script Generation:\n",
            "+-----+--------------+----------------------------------------------------------------------------------------------------------------------+---------+\n",
            "| No. |     Type     |                                                        Title                                                         | Upvotes |\n",
            "+-----+--------------+----------------------------------------------------------------------------------------------------------------------+---------+\n",
            "|  1  | News Article | Fantasy Premier League: What we learned from Gameweek 6 – Don’t panic after Palmer’s awesome foursome - The Athletic |         |\n",
            "|  2  | News Article |                      FPL Gameweek 9 Price Changes: Tottenham's Son On The Rise - Sportscasting                       |         |\n",
            "|  3  | News Article |                Fantasy Premier League: Eight takeaways from Gameweek 3 of FPL - The Football Faithful                |         |\n",
            "|  4  | News Article |                    FPL Gameweek 30 early Scout Picks: Chelsea double-up - Fantasy Football Scout                     |         |\n",
            "|  5  | News Article |      FPL review: Cole Palmer creates new record but don’t rip up your squad for one player - Belfast Telegraph       |         |\n",
            "+-----+--------------+----------------------------------------------------------------------------------------------------------------------+---------+\n",
            "\n",
            "Enter the numbers of the items you want to generate scripts for, separated by commas (e.g., 1,3,5): 2\n",
            "\n",
            "Fetching content and generating summaries for selected items...\n",
            "\n",
            "Do you want to check for stock media availability for the selected items? (yes/no): n\n",
            "\n",
            "Skipping stock media availability check.\n",
            "\n",
            "Summary: <a href=\"https://news.google.com/rss/articles/CBMiogFBVV95cUxPWGpSRnNsdEtDU1l0NzctR1NWRHUtOFZ4c1hWbzZQYjlVTmpWeGNQNUs3cG5xZEo5YVBnaXFvZm0tcE1fOF9zSktIN0x3YmlVV0lmQnRIdTZEYzZMRXV3Nm5TbUw2VkU2dmRCM0dXMkRacllRbmtMSjZ2TGthSUxxOEhMYjJxUHJJeG1wajNmWkVWN18xOE1BcjVrdnZhSGtBWWc?oc=5\" target=\"_blank\">FPL Gameweek 9 Price Changes: Tottenham's Son On The Rise</a>&nbsp;&nbsp;<font color=\"#6f6f6f\">Sportscasting</font>\n",
            "\n",
            "Select a script style:\n",
            "1. Flashy Script\n",
            "2. Expressive Script\n",
            "3. Normal Script\n",
            "Enter the number of the script style you prefer: 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Markdown' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-50a2bf2da323>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Block 10: Run the Main Function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-d5ed0324215c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nSkipping stock media availability check.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# Proceed to generate scripts without checking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mgenerate_scripts_from_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_items\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nScript execution completed.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-d5ed0324215c>\u001b[0m in \u001b[0;36mgenerate_scripts_from_items\u001b[0;34m(selected_items)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No summary available.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nSummary: {summary}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mgenerate_script_with_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"====\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-d5ed0324215c>\u001b[0m in \u001b[0;36mgenerate_script_with_style\u001b[0;34m(topic, summary)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0mscript\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_script_for_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;31m# Display the script with enhanced formatting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMarkdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"### Generated {style} for '{topic}':\\n\\n{script}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'Markdown' is not defined"
          ]
        }
      ]
    }
  ]
}