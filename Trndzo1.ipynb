{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOix6k4gn3cfM21otF22Vc1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kepners/ChopOnions/blob/main/Trndzo1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Install Required Libraries\n",
        "!pip install openai==0.27.8\n",
        "!pip install python-dotenv\n",
        "!pip install praw\n",
        "!pip install requests\n",
        "!pip install cachetools\n",
        "!pip install nltk\n",
        "!pip install tqdm\n",
        "!pip install numpy\n",
        "!pip install scikit-learn\n",
        "!pip install feedparser\n",
        "!pip install google-search-results  # SerpAPI client\n",
        "!pip install pytrends  # Alternative to SerpAPI for Google Trends\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAUVB0guTX2b",
        "outputId": "b26bcfb7-b577-43dd-a38d-661132aaeb23",
        "collapsed": true
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.27.8 in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (4.66.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (3.10.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.8) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.8) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.8) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.27.8) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.27.8) (4.12.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->openai==0.27.8) (0.2.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: praw in /usr/local/lib/python3.10/dist-packages (7.8.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in /usr/local/lib/python3.10/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.8.30)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (5.5.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.10/dist-packages (6.0.11)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser) (1.0.0)\n",
            "Requirement already satisfied: google-search-results in /usr/local/lib/python3.10/dist-packages (2.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2024.8.30)\n",
            "Requirement already satisfied: pytrends in /usr/local/lib/python3.10/dist-packages (4.9.2)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.10/dist-packages (from pytrends) (2.32.3)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from pytrends) (2.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pytrends) (5.3.0)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.0->pytrends) (2024.8.30)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25->pytrends) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 2: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIud78FOZR3M",
        "outputId": "10208a31-cb16-4262-ed01-2b3c78cc4f94"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 3: Load Environment Variables\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "import logging\n",
        "\n",
        "# Define the path to your .env file in Google Drive\n",
        "dotenv_path = '/content/drive/MyDrive/Secrets/.env'  # Updated with capital 'S'\n",
        "\n",
        "# Load the environment variables from the .env file\n",
        "load_dotenv(dotenv_path)\n",
        "\n",
        "# Retrieve API keys from environment variables\n",
        "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
        "PEXELS_API_KEY = os.getenv('PEXELS_API_KEY')\n",
        "REDDIT_CLIENT_ID = os.getenv('REDDIT_CLIENT_ID')\n",
        "REDDIT_CLIENT_SECRET = os.getenv('REDDIT_CLIENT_SECRET')\n",
        "SHUTTERSTOCK_ACCESS_TOKEN = os.getenv('SHUTTERSTOCK_ACCESS_TOKEN')\n",
        "\n",
        "# Validate that all required environment variables are set\n",
        "required_vars = [\n",
        "    'OPENAI_API_KEY',\n",
        "    'PEXELS_API_KEY',\n",
        "    'REDDIT_CLIENT_ID',\n",
        "    'REDDIT_CLIENT_SECRET',\n",
        "    'SHUTTERSTOCK_ACCESS_TOKEN'\n",
        "]\n",
        "missing_vars = [var for var in required_vars if not os.getenv(var)]\n",
        "\n",
        "if missing_vars:\n",
        "    logging.error(f\"Missing environment variables: {', '.join(missing_vars)}\")\n",
        "    raise SystemExit(\"Please ensure all API keys are set in the .env file.\")\n",
        "else:\n",
        "    logging.info(\"All environment variables loaded successfully.\")\n",
        "\n",
        "# Block 4: Verify Environment Variables\n",
        "for var in required_vars:\n",
        "    if os.getenv(var):\n",
        "        print(f\"{var}: Loaded\")\n",
        "    else:\n",
        "        print(f\"{var}: Not Loaded\")\n",
        "\n",
        "# Block 5: Download NLTK Data\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK data packages\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Block 6A: Create Utils Directory\n",
        "!mkdir -p utils\n",
        "\n",
        "\n",
        "# Block 6A: Create Utils Directory\n",
        "import os\n",
        "\n",
        "utils_dir = 'utils'\n",
        "if not os.path.exists(utils_dir):\n",
        "    os.makedirs(utils_dir)\n",
        "\n",
        "# Block 6B: Create data_processing.py\n",
        "data_processing_code = \"\"\"\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "def extract_keywords(text, max_keywords=10):\n",
        "    \\\"\"\"\n",
        "    Extracts up to `max_keywords` nouns from the input `text`.\n",
        "    \\\"\"\"\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords and non-alphabetic tokens\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "    # Get part-of-speech tags\n",
        "    tagged_words = pos_tag(filtered_words)\n",
        "    # Keep nouns and proper nouns\n",
        "    keywords = [word for word, pos in tagged_words if pos.startswith('NN')]\n",
        "    # Limit the number of keywords\n",
        "    return keywords[:max_keywords]\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join('utils', 'data_processing.py'), 'w') as file:\n",
        "    file.write(data_processing_code)\n",
        "\n",
        "\n",
        "# Block 7: Initialize API Clients and Configure Logging\n",
        "import praw\n",
        "import openai\n",
        "import logging\n",
        "import warnings\n",
        "from utils.data_processing import extract_keywords\n",
        "\n",
        "# Suppress PRAW warnings about asynchronous environments\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='praw')\n",
        "\n",
        "# Retrieve REDDIT_USER_AGENT from environment variables or set a default\n",
        "REDDIT_USER_AGENT = os.getenv('REDDIT_USER_AGENT', 'script:TrendingTopicsScript:1.0 (by u/yourusername)')\n",
        "\n",
        "# Initialize Reddit API using PRAW\n",
        "try:\n",
        "    reddit = praw.Reddit(\n",
        "        client_id=REDDIT_CLIENT_ID,\n",
        "        client_secret=REDDIT_CLIENT_SECRET,\n",
        "        user_agent=REDDIT_USER_AGENT\n",
        "    )\n",
        "    logging.info(\"Reddit API initialized successfully.\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error initializing Reddit API: {e}\")\n",
        "\n",
        "# Initialize OpenAI API\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# Configure Logging\n",
        "logging.basicConfig(\n",
        "    level=logging.ERROR,  # Set to ERROR to display only error messages\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler()  # Log to console\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Function to add separators\n",
        "def add_separator():\n",
        "    print(\"\")\n",
        "\n",
        "# Define English, French, Spanish, Italian Speaking Countries\n",
        "top_countries = {\n",
        "    1: {'name': 'India', 'code': 'IN'},\n",
        "    2: {'name': 'United States', 'code': 'US'},\n",
        "    3: {'name': 'Nigeria', 'code': 'NG'},\n",
        "    4: {'name': 'Mexico', 'code': 'MX'},\n",
        "    5: {'name': 'Philippines', 'code': 'PH'},\n",
        "    6: {'name': 'United Kingdom', 'code': 'GB'},\n",
        "    7: {'name': 'France', 'code': 'FR'},\n",
        "    8: {'name': 'Italy', 'code': 'IT'},\n",
        "    9: {'name': 'South Africa', 'code': 'ZA'},\n",
        "    10: {'name': 'Spain', 'code': 'ES'},  # Added Spain\n",
        "    # Add more countries if needed\n",
        "}\n",
        "\n",
        "# Mapping from country codes to PyTrends 'pn' (payload name)\n",
        "country_code_to_pn = {\n",
        "    'IN': 'india',\n",
        "    'US': 'united_states',\n",
        "    'NG': 'nigeria',\n",
        "    'MX': 'mexico',\n",
        "    'PH': 'philippines',\n",
        "    'GB': 'united_kingdom',\n",
        "    'FR': 'france',\n",
        "    'IT': 'italy',\n",
        "    'ZA': 'south_africa',\n",
        "    'ES': 'spain',  # Added Spain\n",
        "    # Add more mappings as needed\n",
        "}\n",
        "\n",
        "# Function to Allow Users to Choose a Location for Google Trends\n",
        "def select_country():\n",
        "    print(\"Select a country for Google Trends data:\")\n",
        "    for idx, country in top_countries.items():\n",
        "        print(f\"{idx}. {country['name']}\")\n",
        "\n",
        "    # Get user selection\n",
        "    try:\n",
        "        country_selection = int(input(\"Enter the number of the country you're interested in: \"))\n",
        "        if country_selection in top_countries:\n",
        "            selected_country = top_countries[country_selection]\n",
        "            print(f\"You selected: {selected_country['name']}\")\n",
        "        else:\n",
        "            print(\"Invalid selection. Defaulting to United States.\")\n",
        "            selected_country = {'name': 'United States', 'code': 'US'}\n",
        "    except ValueError:\n",
        "        print(\"Invalid input. Defaulting to United States.\")\n",
        "        selected_country = {'name': 'United States', 'code': 'US'}\n",
        "\n",
        "    return selected_country\n",
        "\n",
        "# Function to Fetch Trending Topics from Google Trends using PyTrends\n",
        "def fetch_trending_topics_pytrends(selected_country):\n",
        "    add_separator()\n",
        "    logging.info(f\"Fetching trending topics for {selected_country['name']} from Google Trends via PyTrends...\")\n",
        "    pn = country_code_to_pn.get(selected_country['code'], 'united_states')  # Default to 'united_states' if not found\n",
        "    try:\n",
        "        pytrends = TrendReq(hl='en-US', tz=360)\n",
        "        # Fetch daily trending searches\n",
        "        trending_searches_df = pytrends.trending_searches(pn=pn)\n",
        "        if trending_searches_df.empty:\n",
        "            logging.error(\"No trending topics found using PyTrends.\")\n",
        "            return []\n",
        "        trending_topics = trending_searches_df[0].tolist()\n",
        "        trending_topics = trending_topics[:10]  # Limit to top 10 topics\n",
        "        logging.info(f\"Retrieved {len(trending_topics)} trending topics via PyTrends.\")\n",
        "        # Since PyTrends may not provide descriptions, we'll set them as empty\n",
        "        trending_topics_with_desc = [{'title': topic, 'description': ''} for topic in trending_topics]\n",
        "        return trending_topics_with_desc\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching Google Trends data via PyTrends: {e}\")\n",
        "        return []\n",
        "\n",
        "# Function to Fetch Trending Topics from RSS Feeds (Alternative Approach)\n",
        "def fetch_trending_topics_from_rss(feed_url, limit=10):\n",
        "    add_separator()\n",
        "    logging.info(f\"Fetching trending topics from RSS feed: {feed_url}\")\n",
        "    try:\n",
        "        feed = feedparser.parse(feed_url)\n",
        "        trending_topics = []\n",
        "        for entry in feed.entries[:limit]:\n",
        "            title = entry.title\n",
        "            description = entry.summary if 'summary' in entry else entry.title  # Use summary if available\n",
        "            trending_topics.append({'title': title, 'description': description})\n",
        "        logging.info(f\"Retrieved {len(trending_topics)} trending topics from RSS.\")\n",
        "        return trending_topics\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching trending topics from RSS: {e}\")\n",
        "        return []\n",
        "\n",
        "# Function to Generate a Two-Sentence Summary using OpenAI API\n",
        "def generate_summary(content):\n",
        "    add_separator()\n",
        "    logging.info(\"Generating two-sentence summary using OpenAI API...\")\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are a concise summarizer. Provide a clear and brief two-sentence summary of the following content.\"\n",
        "                    )\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": content\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=60,\n",
        "            temperature=0.5,\n",
        "        )\n",
        "        summary = response['choices'][0]['message']['content'].strip()\n",
        "        logging.info(\"Summary generated successfully.\")\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating summary: {e}\")\n",
        "        return \"No description available.\"\n",
        "\n",
        "# Function to Search Subreddits Related to the Topic\n",
        "def search_subreddits_for_topic(topic, limit=5):\n",
        "    add_separator()\n",
        "    logging.info(f\"Searching for subreddits related to '{topic}'...\")\n",
        "    related_subreddits = []\n",
        "    try:\n",
        "        # Search for subreddits matching the topic\n",
        "        for subreddit in reddit.subreddits.search(query=topic, limit=limit):\n",
        "            related_subreddits.append({\n",
        "                'name': subreddit.display_name,\n",
        "                'title': subreddit.title,\n",
        "                'description': subreddit.public_description or ''\n",
        "            })\n",
        "        logging.info(f\"Found {len(related_subreddits)} subreddits related to '{topic}'.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error searching for subreddits: {e}\")\n",
        "    return related_subreddits\n",
        "\n",
        "# Function to Fetch Posts Related to a Topic from Reddit\n",
        "def fetch_posts_from_reddit(topic, subreddit_name=None, limit=5):\n",
        "    add_separator()\n",
        "    logging.info(f\"Fetching posts related to '{topic}' from Reddit...\")\n",
        "    posts = []\n",
        "    try:\n",
        "        if subreddit_name:\n",
        "            subreddit = reddit.subreddit(subreddit_name)\n",
        "            logging.info(f\"Searching in subreddit: {subreddit_name}\")\n",
        "        else:\n",
        "            subreddit = reddit.subreddit('all')\n",
        "            logging.info(\"Searching in all of Reddit\")\n",
        "        # Search for the topic in Reddit\n",
        "        for post in subreddit.search(topic, limit=limit):\n",
        "            if not post.stickied and not post.over_18:\n",
        "                posts.append({'title': post.title, 'score': post.score, 'url': post.url})\n",
        "        logging.info(f\"Retrieved {len(posts)} posts.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching posts related to '{topic}': {e}\")\n",
        "\n",
        "    return posts\n",
        "\n",
        "# Keyword Extraction Function\n",
        "def extract_keywords(text, max_keywords=5):\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords and non-alphabetic tokens\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "    # Get part-of-speech tags\n",
        "    tagged_words = pos_tag(filtered_words)\n",
        "    # Keep nouns and proper nouns\n",
        "    keywords = [word for word, pos in tagged_words if pos.startswith('NN')]\n",
        "    # Limit the number of keywords\n",
        "    return keywords[:max_keywords]\n",
        "\n",
        "# Cache Setup and Function to Check Stock Media Availability\n",
        "# Cache responses for 24 hours (86400 seconds)\n",
        "cache = TTLCache(maxsize=1000, ttl=86400)\n",
        "\n",
        "@cached(cache)\n",
        "def check_stock_media_availability(topic):\n",
        "    add_separator()\n",
        "    logging.info(f\"Checking stock media for topic: {topic}\")\n",
        "    # Extract keywords from the topic\n",
        "    keywords = extract_keywords(topic)\n",
        "    if not keywords:\n",
        "        logging.error(f\"No valid keywords extracted for topic: {topic}\")\n",
        "        return 0\n",
        "    # Join keywords into a query string\n",
        "    query = ' '.join(keywords)\n",
        "    # Ensure the query does not exceed 100 characters\n",
        "    query = query[:100]\n",
        "    total_results = 0\n",
        "    # Check Pexels\n",
        "    total_results += check_pexels_media(query)\n",
        "    # Check Shutterstock\n",
        "    total_results += check_shutterstock_media(query)\n",
        "    logging.info(f\"Total stock media items available for '{topic}': {total_results}\")\n",
        "    return total_results\n",
        "\n",
        "def check_pexels_media(query):\n",
        "    headers = {\n",
        "        'Authorization': PEXELS_API_KEY,\n",
        "        'User-Agent': 'YourAppName'  # Optionally set User-Agent\n",
        "    }\n",
        "    params = {\n",
        "        'query': query,\n",
        "        'per_page': 1,\n",
        "        'page': 1\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get('https://api.pexels.com/v1/search', headers=headers, params=params)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            total_results = data.get('total_results', 0)\n",
        "            return total_results\n",
        "        else:\n",
        "            logging.error(f\"Pexels API error {response.status_code}: {response.text}\")\n",
        "            return 0\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"Error checking Pexels for '{query}': {e}\")\n",
        "        return 0\n",
        "\n",
        "def check_shutterstock_media(query):\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {SHUTTERSTOCK_ACCESS_TOKEN}',\n",
        "        'User-Agent': 'YourAppName'  # Optionally set User-Agent\n",
        "    }\n",
        "    params = {\n",
        "        'query': query,\n",
        "        'per_page': 1,\n",
        "        'page': 1,\n",
        "        'view': 'minimal'\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get('https://api.shutterstock.com/v2/images/search', headers=headers, params=params)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            total_count = data.get('total_count', 0)\n",
        "            return total_count\n",
        "        else:\n",
        "            logging.error(f\"Shutterstock API error {response.status_code}: {response.text}\")\n",
        "            return 0\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"Error checking Shutterstock for '{query}': {e}\")\n",
        "        return 0\n",
        "\n",
        "# Function to Generate Script for a Topic using OpenAI API\n",
        "def generate_script_for_topic(topic, max_retries=5):\n",
        "    add_separator()\n",
        "    logging.info(\"Generating script using OpenAI API...\")\n",
        "    retry_delay = 1\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": (\n",
        "                            \"You are a creative scriptwriter for short-form videos. \"\n",
        "                            \"Produce engaging, dynamic scripts that are visually compelling and can be represented \"\n",
        "                            \"using stock images and videos. The script should have a captivating introduction, \"\n",
        "                            \"a clear narrative flow, and a strong conclusion or call-to-action.\"\n",
        "                        )\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": (\n",
        "                            f\"Write a short, captivating video script about '{topic}' suitable for platforms like TikTok. \"\n",
        "                            \"Ensure the script includes vivid descriptions and is structured with a beginning, middle, and end. \"\n",
        "                            \"Use language that resonates with the target audience, and make sure it's adaptable with widely available stock images and videos.\"\n",
        "                        )\n",
        "                    }\n",
        "                ],\n",
        "                max_tokens=600,\n",
        "                temperature=0.7,\n",
        "            )\n",
        "            logging.info(\"Script generated successfully.\")\n",
        "            script_content = response['choices'][0]['message']['content'].strip()\n",
        "            print(f\"====\\nGenerated script for '{topic}':\\n{script_content}\\n====\\n\")\n",
        "            return script_content\n",
        "        except openai.error.RateLimitError:\n",
        "            logging.error(f\"Rate limit hit. Waiting {retry_delay} seconds before retrying...\")\n",
        "            time.sleep(retry_delay)\n",
        "            retry_delay *= 2\n",
        "        except openai.error.OpenAIError as e:\n",
        "            logging.error(f\"An OpenAI error occurred: {e}\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            logging.error(f\"An unexpected error occurred: {e}\")\n",
        "            break\n",
        "    return \"Max retries reached. Unable to generate script.\"\n",
        "\n",
        "# Function to Fetch Subreddit Information (Optional)\n",
        "def fetch_subreddit_info(limit=500):\n",
        "    add_separator()\n",
        "    logging.info(\"Fetching subreddit information...\")\n",
        "    subreddit_list = []\n",
        "    try:\n",
        "        # Get the top subreddits\n",
        "        for subreddit in reddit.subreddits.popular(limit=limit):\n",
        "            subreddit_list.append({\n",
        "                'name': subreddit.display_name,\n",
        "                'description': subreddit.public_description or ''\n",
        "            })\n",
        "        logging.info(f\"Retrieved information for {len(subreddit_list)} subreddits.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching subreddits: {e}\")\n",
        "    return subreddit_list\n",
        "\n",
        "# Function to Find Similar Subreddits (Optional)\n",
        "def compute_embeddings(text_list):\n",
        "    embeddings = []\n",
        "    for text in tqdm(text_list, desc=\"Computing embeddings\"):\n",
        "        try:\n",
        "            response = openai.Embedding.create(\n",
        "                input=text,\n",
        "                model=\"text-embedding-ada-002\"\n",
        "            )\n",
        "            embeddings.append(response['data'][0]['embedding'])\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error computing embedding for text: {e}\")\n",
        "            embeddings.append([0]*1536)  # Handle error by adding a zero vector\n",
        "    return embeddings\n",
        "\n",
        "def find_similar_subreddits(input_text, subreddit_list, top_n=10):\n",
        "    add_separator()\n",
        "    logging.info(\"Computing embedding for the input text...\")\n",
        "    try:\n",
        "        input_embedding = openai.Embedding.create(\n",
        "            input=input_text,\n",
        "            model=\"text-embedding-ada-002\"\n",
        "        )['data'][0]['embedding']\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error computing embedding for input text: {e}\")\n",
        "        return []\n",
        "\n",
        "    logging.info(\"Computing embeddings for subreddit descriptions...\")\n",
        "    descriptions = [sub['description'] for sub in subreddit_list]\n",
        "    embeddings = compute_embeddings(descriptions)\n",
        "\n",
        "    logging.info(\"Calculating similarities...\")\n",
        "    similarities = cosine_similarity([input_embedding], embeddings)[0]\n",
        "\n",
        "    # Combine subreddits with similarities\n",
        "    for i, sub in enumerate(subreddit_list):\n",
        "        sub['similarity'] = similarities[i]\n",
        "\n",
        "    # Sort subreddits by similarity\n",
        "    subreddit_list.sort(key=lambda x: x['similarity'], reverse=True)\n",
        "\n",
        "    # Return top N similar subreddits\n",
        "    similar_subreddits = subreddit_list[:top_n]\n",
        "    logging.info(f\"Found {len(similar_subreddits)} similar subreddits.\")\n",
        "    return similar_subreddits\n",
        "\n",
        "# Main Execution Block\n",
        "def main():\n",
        "    add_separator()\n",
        "    print(\"====\")\n",
        "    print(\"Starting script execution...\\n\")\n",
        "\n",
        "    # Step 1: User selects the country for Google Trends\n",
        "    selected_country = select_country()\n",
        "\n",
        "    # Step 2: Fetch trending topics for the selected country\n",
        "    # Attempt PyTrends first\n",
        "    google_trends_topics = fetch_trending_topics_pytrends(selected_country)\n",
        "\n",
        "    # If PyTrends fails or returns insufficient data, try RSS feeds\n",
        "    if not google_trends_topics:\n",
        "        logging.info(\"No trending topics found using PyTrends. Attempting to fetch from RSS feeds.\")\n",
        "        # Define RSS feed URLs based on country or default\n",
        "        # Example RSS feed URL (you may need to replace this with actual RSS feeds related to Google Trends)\n",
        "        rss_feed_url = f\"https://trends.google.com/trends/trendingsearches/daily/rss?geo={selected_country['code']}\"\n",
        "        google_trends_topics = fetch_trending_topics_from_rss(rss_feed_url)\n",
        "\n",
        "    if not google_trends_topics:\n",
        "        logging.error(\"No trending topics found using all available methods.\")\n",
        "        return\n",
        "\n",
        "    # Generate descriptions for trending topics if missing\n",
        "    for topic in google_trends_topics:\n",
        "        if not topic['description'] or topic['description'].lower() == \"no description available.\":\n",
        "            topic['description'] = generate_summary(topic['title'])\n",
        "\n",
        "    # Display the trending topics with descriptions to the user\n",
        "    print(f\"\\n====\\nCurrent Trending Topics in {selected_country['name']}:\")\n",
        "    for idx, topic in enumerate(google_trends_topics, start=1):\n",
        "        title = topic['title']\n",
        "        description = topic['description'] if topic['description'] else \"No description available.\"\n",
        "        print(f\"{idx}. {title} - {description}\")\n",
        "\n",
        "    # Allow the user to select a topic\n",
        "    try:\n",
        "        selected_idx = int(input(\"\\nEnter the number of the topic you're interested in: \"))\n",
        "        if 1 <= selected_idx <= len(google_trends_topics):\n",
        "            selected_topic = google_trends_topics[selected_idx - 1]['title']\n",
        "            print(f\"\\nYou selected: {selected_topic}\")\n",
        "        else:\n",
        "            logging.error(\"Invalid selection. Exiting.\")\n",
        "            return\n",
        "    except ValueError:\n",
        "        logging.error(\"Invalid input. Please enter a number. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Step 3: Search for subreddits related to the topic\n",
        "    related_subreddits = search_subreddits_for_topic(selected_topic, limit=5)\n",
        "    if not related_subreddits:\n",
        "        logging.error(\"No related subreddits found. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Display related subreddits to the user\n",
        "    print(\"\\n====\\nRelated Subreddits:\")\n",
        "    for idx, sub in enumerate(related_subreddits, start=1):\n",
        "        print(f\"{idx}. {sub['name']} - {sub['title']}\")\n",
        "\n",
        "    # Fetch and display top two posts from each subreddit\n",
        "    print(\"\\n====\\nTop 2 Posts from Each Subreddit:\")\n",
        "    all_posts = []\n",
        "    for sub in related_subreddits:\n",
        "        subreddit_name = sub['name']\n",
        "        try:\n",
        "            subreddit = reddit.subreddit(subreddit_name)\n",
        "            top_posts = list(subreddit.hot(limit=2))\n",
        "            print(f\"\\nSubreddit: {subreddit_name}\")\n",
        "            for post in top_posts:\n",
        "                print(f\"- {post.title} (Score: {post.score})\")\n",
        "                all_posts.append({'subreddit': subreddit_name, 'title': post.title, 'score': post.score, 'url': post.url})\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error fetching posts from subreddit '{subreddit_name}': {e}\")\n",
        "            print(f\"- Unable to fetch posts from subreddit '{subreddit_name}'.\")\n",
        "\n",
        "    if not all_posts:\n",
        "        logging.error(\"No posts fetched from the related subreddits. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Step 4: Allow user to select specific posts to generate scripts for\n",
        "    print(\"\\n====\\nAll Fetched Posts:\")\n",
        "    for idx, post in enumerate(all_posts, start=1):\n",
        "        print(f\"{idx}. [{post['subreddit']}] {post['title']} (Score: {post['score']})\")\n",
        "\n",
        "    selected_posts_input = input(\"\\nEnter the numbers of the posts you want to generate scripts for, separated by commas (e.g., 1,3,5): \")\n",
        "    try:\n",
        "        selected_indices = [int(i.strip()) - 1 for i in selected_posts_input.split(',') if i.strip().isdigit()]\n",
        "        selected_posts = [all_posts[i] for i in selected_indices if 0 <= i < len(all_posts)]\n",
        "        if not selected_posts:\n",
        "            logging.error(\"No valid posts selected. Exiting.\")\n",
        "            return\n",
        "    except ValueError:\n",
        "        logging.error(\"Invalid input. Please enter numbers separated by commas. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Fetch content and generate summaries for selected posts\n",
        "    print(f\"\\n====\\nFetching content and generating summaries for selected posts...\")\n",
        "    for post in selected_posts:\n",
        "        title = post['title']\n",
        "        url = post['url']\n",
        "        try:\n",
        "            reddit_post = reddit.submission(url=url)\n",
        "            content = reddit_post.selftext if reddit_post.is_self else reddit_post.title\n",
        "            summary = generate_summary(content)\n",
        "            post['summary'] = summary\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error fetching content for post '{title}': {e}\")\n",
        "            post['summary'] = \"No summary available.\"\n",
        "\n",
        "    # Step 5: Decide Whether to Check Stock Media Availability\n",
        "    check_media = input(\"\\nDo you want to check for stock media availability for the selected posts? (yes/no): \").strip().lower()\n",
        "    if check_media not in ['yes', 'y']:\n",
        "        add_separator()\n",
        "        print(\"====\")\n",
        "        print(\"====\")\n",
        "        print(\"Skipping stock media availability check.\")\n",
        "        # Proceed to generate scripts without checking\n",
        "        for post in selected_posts:\n",
        "            title = post['title']\n",
        "            summary = post.get('summary', \"No summary available.\")\n",
        "            print(f\"\\nSummary: {summary}\")\n",
        "            script = generate_script_for_topic(title)\n",
        "            print(f\"====\\nGenerated script for '{title}':\\n{script}\\n====\\n\")\n",
        "        print(\"\\nScript execution completed.\")\n",
        "        return\n",
        "\n",
        "    # Continue with stock media availability check\n",
        "    add_separator()\n",
        "    print(\"====\")\n",
        "    print(\"Checking stock media availability for the selected posts...\")\n",
        "    request_count = 0\n",
        "    scripts_generated = 0\n",
        "    for post in selected_posts:\n",
        "        title = post['title']\n",
        "        summary = post.get('summary', \"No summary available.\")\n",
        "        if request_count >= 190:\n",
        "            logging.error(\"Approaching API rate limits. Waiting for 60 minutes before continuing...\")\n",
        "            print(\"====\")\n",
        "            print(\"====\")\n",
        "            time.sleep(3600)  # Wait for an hour\n",
        "            request_count = 0\n",
        "        total_media = check_stock_media_availability(title)\n",
        "        request_count += 1\n",
        "        if total_media >= 10:\n",
        "            logging.info(f\"Topic '{title}' has {total_media} stock media items available.\")\n",
        "            # Display summary before generating script\n",
        "            print(f\"\\nSummary: {summary}\")\n",
        "            # Step 6: Generate script for this topic\n",
        "            script = generate_script_for_topic(title)\n",
        "            scripts_generated += 1\n",
        "        else:\n",
        "            logging.info(f\"Not enough stock media for topic: {title}\")\n",
        "            print(f\"\\nSummary: {summary}\")\n",
        "            print(f\"Not enough stock media for topic: {title}\\n====\\n\")\n",
        "    if scripts_generated == 0:\n",
        "        logging.error(\"No topics with sufficient stock media were found.\")\n",
        "    else:\n",
        "        logging.info(f\"Generated {scripts_generated} script(s) based on available stock media.\")\n",
        "    print(\"\\nScript execution completed.\")\n",
        "\n",
        "# Block 8: Run the Main Function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RPd0MOuGTaTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf0bea86-54c8-4956-c433-38ea69ccbaa2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OPENAI_API_KEY: Loaded\n",
            "PEXELS_API_KEY: Loaded\n",
            "REDDIT_CLIENT_ID: Loaded\n",
            "REDDIT_CLIENT_SECRET: Loaded\n",
            "SHUTTERSTOCK_ACCESS_TOKEN: Loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "====\n",
            "Starting script execution...\n",
            "\n",
            "Select a country for Google Trends data:\n",
            "1. India\n",
            "2. United States\n",
            "3. Nigeria\n",
            "4. Mexico\n",
            "5. Philippines\n",
            "6. United Kingdom\n",
            "7. France\n",
            "8. Italy\n",
            "9. South Africa\n",
            "10. Spain\n",
            "Enter the number of the country you're interested in: 6\n",
            "You selected: United Kingdom\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "====\n",
            "Current Trending Topics in United Kingdom:\n",
            "1. Pharrell Williams - Pharrell Williams is a multi-talented musician, producer, and fashion designer known for his work in the music industry and collaborations with various artists. He has achieved success both as a solo artist and as part of the duo The Neptunes and the group N.E.R.D.\n",
            "2. Paddington - \"Paddington\" is a British family film based on the beloved children's book character, a polite and adventurous bear who finds a new home in London. The movie follows Paddington as he navigates city life, makes friends, and gets caught up in a series of comedic mishaps.\n",
            "3. England vs New Zealand - England and New Zealand are two cricket teams that have a historic rivalry, with matches often being closely contested and highly competitive. Both teams have a strong cricketing tradition and have produced some memorable matches and moments in their encounters.\n",
            "4. Tulisa - Tulisa Contostavlos is a British singer and former member of the group N-Dubz who gained fame as a judge on the UK version of \"The X Factor.\" She has also faced legal troubles and controversy throughout her career.\n",
            "5. Maura Higgins - Maura Higgins is an Irish television personality known for her appearance on the reality show Love Island and for her work as a model and influencer.\n",
            "6. Gyokeres - \"Gyokeres\" is a Hungarian word that means \"deep-rooted\" or \"fundamental\" in English.\n",
            "7. TNT Sports - TNT Sports is a sports television network that airs live sports events, analysis, and commentary, catering to a wide range of sports fans with diverse programming. It is known for broadcasting major sporting events such as NBA games and the UEFA Champions League.\n",
            "8. Rohit Bal - Rohit Bal is a renowned Indian fashion designer known for his exquisite designs that blend traditional aesthetics with contemporary styles, making him a prominent figure in the Indian fashion industry. His unique creations have garnered him international acclaim and recognition, solidifying his position as a leading designer in the global fashion scene.\n",
            "9. I'm A Celeb 2024 - \"I'm A Celeb 2024\" is a reality TV show that follows celebrities as they face challenges and compete in the wilderness to win prizes, with the 2024 season promising new twists and surprises for viewers.\n",
            "10. Sporting Lisbon - Sporting Lisbon, also known as Sporting Clube de Portugal, is a professional football club based in Lisbon, Portugal. The club has a rich history and is one of the \"Big Three\" clubs in Portuguese football, alongside Benfica and Porto.\n",
            "\n",
            "Enter the number of the topic you're interested in: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You selected: Paddington\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====\n",
            "Related Subreddits:\n",
            "1. Paddington - Paddington\n",
            "2. moviescirclejerk - /r/moviescirclejerk\n",
            "3. london - London, UK\n",
            "4. movies - Movie News and Discussion\n",
            "5. brisbane - /r/brisbane\n",
            "\n",
            "====\n",
            "Top 2 Posts from Each Subreddit:\n",
            "\n",
            "Subreddit: Paddington\n",
            "- if you love paddington, chances are that you'll love bluey (Score: 18)\n",
            "- How did Paddington get his passport? (Score: 12)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Subreddit: moviescirclejerk\n",
            "- My Own Private Idaho [1991] (Score: 45)\n",
            "- I Saw the Devil (2010) (Score: 176)\n",
            "\n",
            "Subreddit: london\n",
            "- Weekly Q&A Megathread. Please post any questions about visiting, tourism, living, working, budgeting, housing here! (Score: 3)\n",
            "- London Needs This Too (Score: 2544)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Subreddit: movies\n",
            "- Join us Friday 11/8 at 3:00 PM ET for a live AMA/Q&A with Scott Beck & Bryan Woods, co-directors of the upcoming A24 horror film 'Heretic' (starring Hugh Grant, Sophie Thatcher, and Chloe East). They also directed '65' and co-wrote 'A Quiet Place'. (Score: 10)\n",
            "- Hi /r/movies! We are Mary Dauterman and Grace Glowicki, writer/director and lead actor of BOOGER, a gross comedy body horror cat film that will make you puke. You can watch it on VOD now. Ask Us Anything! (Score: 9)\n",
            "\n",
            "Subreddit: brisbane\n",
            "- It's the /r/brisbane random discussion thread. 03/11/2024 (Score: 0)\n",
            "- /r/Brisbane evening discussion thread 02 Nov, 2024 (Score: 6)\n",
            "\n",
            "====\n",
            "All Fetched Posts:\n",
            "1. [Paddington] if you love paddington, chances are that you'll love bluey (Score: 18)\n",
            "2. [Paddington] How did Paddington get his passport? (Score: 12)\n",
            "3. [moviescirclejerk] My Own Private Idaho [1991] (Score: 45)\n",
            "4. [moviescirclejerk] I Saw the Devil (2010) (Score: 176)\n",
            "5. [london] Weekly Q&A Megathread. Please post any questions about visiting, tourism, living, working, budgeting, housing here! (Score: 3)\n",
            "6. [london] London Needs This Too (Score: 2544)\n",
            "7. [movies] Join us Friday 11/8 at 3:00 PM ET for a live AMA/Q&A with Scott Beck & Bryan Woods, co-directors of the upcoming A24 horror film 'Heretic' (starring Hugh Grant, Sophie Thatcher, and Chloe East). They also directed '65' and co-wrote 'A Quiet Place'. (Score: 10)\n",
            "8. [movies] Hi /r/movies! We are Mary Dauterman and Grace Glowicki, writer/director and lead actor of BOOGER, a gross comedy body horror cat film that will make you puke. You can watch it on VOD now. Ask Us Anything! (Score: 9)\n",
            "9. [brisbane] It's the /r/brisbane random discussion thread. 03/11/2024 (Score: 0)\n",
            "10. [brisbane] /r/Brisbane evening discussion thread 02 Nov, 2024 (Score: 6)\n",
            "\n",
            "Enter the numbers of the posts you want to generate scripts for, separated by commas (e.g., 1,3,5): 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Error fetching content for post 'if you love paddington, chances are that you'll love bluey': Invalid URL (subreddit, not submission): http://old.reddit.com/r/bluey\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "====\n",
            "Fetching content and generating summaries for selected posts...\n",
            "\n",
            "Do you want to check for stock media availability for the selected posts? (yes/no): n\n",
            "\n",
            "====\n",
            "====\n",
            "Skipping stock media availability check.\n",
            "\n",
            "Summary: No summary available.\n",
            "\n",
            "====\n",
            "Generated script for 'if you love paddington, chances are that you'll love bluey':\n",
            "[Opening shot of Paddington Bear figurine on a desk]\n",
            "\n",
            "Narrator: \"Hey, Paddington fans! Looking for a new favorite? Meet Bluey!\"\n",
            "\n",
            "[Cut to quick, fun clips of Bluey characters playing]\n",
            "\n",
            "Narrator: \"If you love the charm and heartwarming adventures of Paddington, chances are you'll fall head over heels for Bluey!\"\n",
            "\n",
            "[Transition to images of Bluey merchandise and clips from the show]\n",
            "\n",
            "Narrator: \"Bluey is a lovable Blue Heeler pup who, just like Paddington, brings joy, laughter, and life lessons in every episode.\"\n",
            "\n",
            "[Exciting montage of Bluey's family having fun together]\n",
            "\n",
            "Narrator: \"Join Bluey, Bingo, and their family on imaginative adventures that will warm your heart and tickle your funny bone!\"\n",
            "\n",
            "[Closing shot with Paddington and Bluey side by side]\n",
            "\n",
            "Narrator: \"So, if you adore Paddington's spirit, get ready to embrace the wonderful world of Bluey! It's a paw-some choice for fans of all ages!\"\n",
            "\n",
            "[End with a playful call-to-action overlay: \"Ready to meet your new furry friend? Watch Bluey now! #Bluey #PaddingtonFans\"]\n",
            "====\n",
            "\n",
            "====\n",
            "Generated script for 'if you love paddington, chances are that you'll love bluey':\n",
            "[Opening shot of Paddington Bear figurine on a desk]\n",
            "\n",
            "Narrator: \"Hey, Paddington fans! Looking for a new favorite? Meet Bluey!\"\n",
            "\n",
            "[Cut to quick, fun clips of Bluey characters playing]\n",
            "\n",
            "Narrator: \"If you love the charm and heartwarming adventures of Paddington, chances are you'll fall head over heels for Bluey!\"\n",
            "\n",
            "[Transition to images of Bluey merchandise and clips from the show]\n",
            "\n",
            "Narrator: \"Bluey is a lovable Blue Heeler pup who, just like Paddington, brings joy, laughter, and life lessons in every episode.\"\n",
            "\n",
            "[Exciting montage of Bluey's family having fun together]\n",
            "\n",
            "Narrator: \"Join Bluey, Bingo, and their family on imaginative adventures that will warm your heart and tickle your funny bone!\"\n",
            "\n",
            "[Closing shot with Paddington and Bluey side by side]\n",
            "\n",
            "Narrator: \"So, if you adore Paddington's spirit, get ready to embrace the wonderful world of Bluey! It's a paw-some choice for fans of all ages!\"\n",
            "\n",
            "[End with a playful call-to-action overlay: \"Ready to meet your new furry friend? Watch Bluey now! #Bluey #PaddingtonFans\"]\n",
            "====\n",
            "\n",
            "\n",
            "Script execution completed.\n"
          ]
        }
      ]
    }
  ]
}