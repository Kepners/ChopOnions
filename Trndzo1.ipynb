{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1s-3u0czUndpxCkgirEnqndEDGpEPjpJ_",
      "authorship_tag": "ABX9TyO7nSMjftJnL7/oAdpZMUXz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kepners/ChopOnions/blob/main/Trndzo1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Install Required Libraries\n",
        "!pip install openai==0.27.8 praw==7.7.0 pytrends==4.9.0 feedparser==6.0.10 requests cachetools==5.3.1 prettytable==3.7.0 textblob==0.17.1 python-dotenv==0.21.0 fuzzywuzzy==0.18.0 python-Levenshtein==0.20.9\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAUVB0guTX2b",
        "outputId": "3d71805d-1cc0-4b03-94a2-4f5fe6816b06",
        "collapsed": true
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.27.8 in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: praw==7.7.0 in /usr/local/lib/python3.10/dist-packages (7.7.0)\n",
            "Requirement already satisfied: pytrends==4.9.0 in /usr/local/lib/python3.10/dist-packages (4.9.0)\n",
            "Requirement already satisfied: feedparser==6.0.10 in /usr/local/lib/python3.10/dist-packages (6.0.10)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: cachetools==5.3.1 in /usr/local/lib/python3.10/dist-packages (5.3.1)\n",
            "Requirement already satisfied: prettytable==3.7.0 in /usr/local/lib/python3.10/dist-packages (3.7.0)\n",
            "Requirement already satisfied: textblob==0.17.1 in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: python-dotenv==0.21.0 in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: fuzzywuzzy==0.18.0 in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: python-Levenshtein==0.20.9 in /usr/local/lib/python3.10/dist-packages (0.20.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (4.66.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (3.10.10)\n",
            "Requirement already satisfied: prawcore<3,>=2.1 in /usr/local/lib/python3.10/dist-packages (from praw==7.7.0) (2.4.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw==7.7.0) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw==7.7.0) (1.8.0)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from pytrends==4.9.0) (2.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pytrends==4.9.0) (5.3.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser==6.0.10) (1.0.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable==3.7.0) (0.2.13)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob==0.17.1) (3.8.1)\n",
            "Requirement already satisfied: Levenshtein==0.20.9 in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein==0.20.9) (0.20.9)\n",
            "Requirement already satisfied: rapidfuzz<3.0.0,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein==0.20.9->python-Levenshtein==0.20.9) (2.15.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob==0.17.1) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob==0.17.1) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob==0.17.1) (2024.9.11)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends==4.9.0) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends==4.9.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends==4.9.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends==4.9.0) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.27.8) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25->pytrends==4.9.0) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->openai==0.27.8) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 2: Mount Google Drive and Load Environment Variables\n",
        "from google.colab import drive\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to your .env file in Google Drive\n",
        "dotenv_path = '/content/drive/MyDrive/Secrets/.env'\n",
        "\n",
        "# Load the environment variables from the .env file\n",
        "load_dotenv(dotenv_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIud78FOZR3M",
        "outputId": "374a3668-3f79-4b13-fc05-d0a3a05358ed"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 3: Configure Logging\n",
        "import logging\n",
        "\n",
        "# Configure Logging to write to a log file and suppress console output\n",
        "log_file = 'app.log'\n",
        "logging.basicConfig(\n",
        "    level=logging.ERROR,  # Only log ERROR and CRITICAL\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(log_file),\n",
        "        logging.StreamHandler(open(os.devnull, 'w'))  # Suppress console output\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "ujJviMETtuLY"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 4: Download NLTK Data Silently\n",
        "import nltk\n",
        "import sys\n",
        "from contextlib import contextmanager\n",
        "\n",
        "@contextmanager\n",
        "def suppress_stdout():\n",
        "    with open(os.devnull, \"w\") as devnull:\n",
        "        old_stdout = sys.stdout\n",
        "        sys.stdout = devnull\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n",
        "\n",
        "with suppress_stdout():\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    nltk.download('brown')\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('movie_reviews')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGPK1IRBXfBm",
        "outputId": "5c6fc3a9-54e3-4c42-946e-9a018cffda37"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 5: Create Utils Directory\n",
        "import os\n",
        "\n",
        "utils_dir = 'utils'\n",
        "if not os.path.exists(utils_dir):\n",
        "    os.makedirs(utils_dir)\n"
      ],
      "metadata": {
        "id": "RPd0MOuGTaTM",
        "collapsed": true
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 6: Create data_processing.py\n",
        "data_processing_code = \"\"\"\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from textblob import TextBlob\n",
        "\n",
        "def extract_keywords(text, max_keywords=10):\n",
        "    \\\"\"\"\n",
        "    Extracts up to `max_keywords` nouns from the input `text`.\n",
        "    \\\"\"\"\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords and non-alphabetic tokens\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "    # Get part-of-speech tags\n",
        "    tagged_words = pos_tag(filtered_words)\n",
        "    # Keep nouns and proper nouns\n",
        "    keywords = [word for word, pos in tagged_words if pos.startswith('NN')]\n",
        "    # Limit the number of keywords\n",
        "    return keywords[:max_keywords]\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    \\\"\"\"\n",
        "    Analyzes the sentiment of the given text and returns it.\n",
        "    \\\"\"\"\n",
        "    blob = TextBlob(text)\n",
        "    polarity = blob.sentiment.polarity\n",
        "    if polarity > 0.1:\n",
        "        return 'Positive'\n",
        "    elif polarity < -0.1:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\"\"\"\n",
        "with open(os.path.join('utils', 'data_processing.py'), 'w') as file:\n",
        "    file.write(data_processing_code)\n"
      ],
      "metadata": {
        "id": "bNicEhLDr06j"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 7: Initialize API Clients and Configure Logging\n",
        "import praw\n",
        "import openai\n",
        "import logging\n",
        "import warnings\n",
        "from utils.data_processing import extract_keywords, analyze_sentiment\n",
        "from cachetools import TTLCache, cached\n",
        "from pytrends.request import TrendReq\n",
        "import feedparser\n",
        "import requests\n",
        "import time\n",
        "import os\n",
        "from fuzzywuzzy import process\n",
        "\n",
        "# Suppress PRAW warnings about asynchronous environments\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='praw')\n",
        "logging.getLogger('praw').setLevel(logging.CRITICAL)\n",
        "\n",
        "# Initialize Reddit API using PRAW\n",
        "try:\n",
        "    reddit = praw.Reddit(\n",
        "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
        "        client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
        "        user_agent=os.getenv('REDDIT_USER_AGENT', 'script:TrendingTopicsScript:1.0 (by u/yourusername)')\n",
        "    )\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error initializing Reddit API: {e}\")\n",
        "\n",
        "# Initialize OpenAI API\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Initialize Pexels API key\n",
        "PEXELS_API_KEY = os.getenv('PEXELS_API_KEY')\n",
        "\n",
        "# Initialize Shutterstock Access Token\n",
        "SHUTTERSTOCK_ACCESS_TOKEN = os.getenv('SHUTTERSTOCK_ACCESS_TOKEN')\n",
        "\n",
        "# Define a list of countries with their codes, including common abbreviations\n",
        "available_countries = {\n",
        "    'United States': 'US',\n",
        "    'US': 'US',\n",
        "    'USA': 'US',\n",
        "    'India': 'IN',\n",
        "    'Canada': 'CA',\n",
        "    'United Kingdom': 'GB',\n",
        "    'UK': 'GB',\n",
        "    'Great Britain': 'GB',\n",
        "    'Australia': 'AU',\n",
        "    'Germany': 'DE',\n",
        "    'France': 'FR',\n",
        "    'Brazil': 'BR',\n",
        "    'Mexico': 'MX',\n",
        "    'Japan': 'JP',\n",
        "    'Russia': 'RU',\n",
        "    'South Korea': 'KR',\n",
        "    'Korea': 'KR',\n",
        "    'Italy': 'IT',\n",
        "    'Spain': 'ES',\n",
        "    'Netherlands': 'NL',\n",
        "    'Sweden': 'SE',\n",
        "    'Switzerland': 'CH',\n",
        "    'Austria': 'AT',\n",
        "    'Belgium': 'BE',\n",
        "    'New Zealand': 'NZ'\n",
        "}\n",
        "\n",
        "# Mapping from country codes to PyTrends 'pn' parameter for trending searches\n",
        "# For realtime_trending_searches, the acceptable 'pn' values are specific\n",
        "country_code_to_pn_realtime = {\n",
        "    'US': 'US',\n",
        "    'IN': 'IN',\n",
        "    'GB': 'GB',\n",
        "    'CA': 'CA',\n",
        "    'AU': 'AU',\n",
        "    'DE': 'DE',\n",
        "    'FR': 'FR',\n",
        "    'BR': 'BR',\n",
        "    'MX': 'MX',\n",
        "    'JP': 'JP',\n",
        "    'RU': 'RU',\n",
        "    'KR': 'KR'  # South Korea\n",
        "}\n",
        "\n",
        "# For trending_searches, the acceptable 'pn' values are:\n",
        "country_code_to_pn_daily = {\n",
        "    'united_states': 'united_states',\n",
        "    'us': 'united_states',\n",
        "    'india': 'india',\n",
        "    'united_kingdom': 'united_kingdom',\n",
        "    'uk': 'united_kingdom',\n",
        "    'gb': 'united_kingdom',\n",
        "    'canada': 'canada',\n",
        "    'australia': 'australia',\n",
        "    'germany': 'germany',\n",
        "    'france': 'france',\n",
        "    'brazil': 'brazil'\n",
        "    # Note: Limited countries are available for trending_searches\n",
        "}\n"
      ],
      "metadata": {
        "id": "C5RZdBozr4hJ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 8: Define Supporting Functions\n",
        "from fuzzywuzzy import process\n",
        "import re\n",
        "import textwrap\n",
        "\n",
        "def get_matching_country(input_country, available_countries):\n",
        "    # Combine country names and codes\n",
        "    country_list = list(available_countries.keys())\n",
        "    # Use fuzzy matching to find the best match\n",
        "    match, score = process.extractOne(input_country, country_list)\n",
        "    if score >= 70:  # Threshold can be adjusted\n",
        "        return match\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def fetch_trending_topics_pytrends(selected_country_code, time_range):\n",
        "    try:\n",
        "        pytrends = TrendReq(hl='en-US', tz=360)\n",
        "        if time_range == '4h':\n",
        "            # Realtime trending searches\n",
        "            pn = country_code_to_pn_realtime.get(selected_country_code)\n",
        "            if not pn:\n",
        "                logging.error(f\"Realtime trending searches not available for the selected country.\")\n",
        "                return []\n",
        "            df = pytrends.realtime_trending_searches(pn=pn)\n",
        "        else:\n",
        "            # Daily trending searches\n",
        "            pn = country_code_to_pn_daily.get(selected_country_code.lower())\n",
        "            if not pn:\n",
        "                logging.error(f\"Daily trending searches not available for the selected country.\")\n",
        "                return []\n",
        "            df = pytrends.trending_searches(pn=pn)\n",
        "        if df.empty:\n",
        "            return []\n",
        "        trending_topics = []\n",
        "        for index, row in df.iterrows():\n",
        "            title = row.iloc[0]  # Use .iloc[0] instead of row[0]\n",
        "            # Generate a brief reason why the topic is trending\n",
        "            reason = generate_trend_reason(title)\n",
        "            trending_topics.append({\n",
        "                'title': title,\n",
        "                'description': reason,  # Brief reason why it's trending\n",
        "                'sv': 'High',       # Placeholder for Search Volume\n",
        "                'change': 'Up',     # Placeholder for Change\n",
        "                'started': 'Recently'   # Placeholder\n",
        "            })\n",
        "        return trending_topics[:10]  # Limit to top 10\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching Google Trends data via PyTrends: {e}\")\n",
        "        return []\n",
        "\n",
        "def generate_trend_reason(topic):\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",  # Use 'gpt-4' model\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are an assistant that provides brief reasons why a topic is trending. Your responses should be concise, in brackets, and two to four words long.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Why is '{topic}' trending? Provide a brief reason in brackets, two to four words long.\"\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=20,\n",
        "            temperature=0.5,\n",
        "        )\n",
        "        reason = response.choices[0].message.content.strip()\n",
        "        return reason\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating trend reason: {e}\")\n",
        "        return \"[Trending Topic]\"\n",
        "\n",
        "def generate_summary(content):\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",  # Use 'gpt-4' model\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a concise summarizer. Provide a clear and brief two-sentence summary of the following content.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": content\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=150,  # Increased max_tokens for longer summaries\n",
        "            temperature=0.5,\n",
        "        )\n",
        "        summary = response.choices[0].message.content.strip()\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating summary: {e}\")\n",
        "        return \"No summary available.\"\n",
        "\n",
        "def search_subreddits_for_topic(topic, limit=4):\n",
        "    \"\"\"\n",
        "    Searches for subreddits related to the topic using individual entities.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Split the topic by commas to get individual entities\n",
        "        entities = [entity.strip() for entity in topic.split(',')]\n",
        "        related_subreddits = {}\n",
        "        for entity in entities:\n",
        "            # Use each entity as a query\n",
        "            subreddits_found = reddit.subreddits.search(query=entity, limit=limit)\n",
        "            for subreddit in subreddits_found:\n",
        "                # Fetch subreddit details\n",
        "                try:\n",
        "                    subreddit_details = reddit.subreddit(subreddit.display_name)\n",
        "                    # Check if subreddit is accessible and appropriate\n",
        "                    if subreddit_details.over18 or subreddit_details.subreddit_type in ['private', 'restricted']:\n",
        "                        continue\n",
        "                    # Add subreddit if not already in the list\n",
        "                    if subreddit.display_name.lower() not in related_subreddits:\n",
        "                        related_subreddits[subreddit.display_name.lower()] = {\n",
        "                            'name': subreddit.display_name,\n",
        "                            'title': subreddit.title,\n",
        "                            'description': subreddit.public_description or ''\n",
        "                        }\n",
        "                    # Break if we've reached the limit\n",
        "                    if len(related_subreddits) >= limit:\n",
        "                        break\n",
        "                except Exception as inner_e:\n",
        "                    logging.error(f\"Error accessing subreddit '{subreddit.display_name}': {inner_e}\")\n",
        "                    continue\n",
        "            if len(related_subreddits) >= limit:\n",
        "                break  # Stop searching if we've found enough subreddits\n",
        "        return list(related_subreddits.values())\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error searching for subreddits: {e}\")\n",
        "        return []\n",
        "\n",
        "def fetch_related_news(topic, limit=5):\n",
        "    \"\"\"\n",
        "    Fetches related news articles using Google News RSS feed.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        feed_url = f\"https://news.google.com/rss/search?q={requests.utils.quote(topic)}&hl=en-US&gl=US&ceid=US:en\"\n",
        "        feed = feedparser.parse(feed_url)\n",
        "        related_stories = []\n",
        "        for entry in feed.entries[:limit]:\n",
        "            title = entry.title\n",
        "            summary = entry.summary if 'summary' in entry else ''\n",
        "            # Remove HTML tags from summary\n",
        "            summary = re.sub('<[^<]+?>', '', summary)\n",
        "            if not summary.strip():\n",
        "                # Generate summary using OpenAI if empty\n",
        "                summary = generate_summary(title)\n",
        "            link = entry.link\n",
        "            related_stories.append({'title': title, 'summary': summary, 'url': link})\n",
        "        return related_stories\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching related news: {e}\")\n",
        "        return []\n",
        "\n",
        "@cached(TTLCache(maxsize=1000, ttl=86400))\n",
        "def check_stock_media_availability(topic):\n",
        "    # Extract keywords from the topic\n",
        "    keywords = extract_keywords(topic)\n",
        "    if not keywords:\n",
        "        logging.error(f\"No valid keywords extracted for topic: {topic}\")\n",
        "        return 0\n",
        "    # Join keywords into a query string\n",
        "    query = ' '.join(keywords)[:100]\n",
        "    total_results = 0\n",
        "    # Check Pexels\n",
        "    total_results += check_pexels_media(query)\n",
        "    # Check Shutterstock\n",
        "    total_results += check_shutterstock_media(query)\n",
        "    return total_results\n",
        "\n",
        "def check_pexels_media(query):\n",
        "    headers = {\n",
        "        'Authorization': PEXELS_API_KEY,\n",
        "        'User-Agent': 'TrendingTopicsScript'\n",
        "    }\n",
        "    params = {\n",
        "        'query': query,\n",
        "        'per_page': 1,\n",
        "        'page': 1\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get('https://api.pexels.com/v1/search', headers=headers, params=params)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            return data.get('total_results', 0)\n",
        "        else:\n",
        "            logging.error(f\"Pexels API error {response.status_code}: {response.text}\")\n",
        "            return 0\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"Error checking Pexels for '{query}': {e}\")\n",
        "        return 0\n",
        "\n",
        "def check_shutterstock_media(query):\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {SHUTTERSTOCK_ACCESS_TOKEN}',\n",
        "        'User-Agent': 'TrendingTopicsScript'\n",
        "    }\n",
        "    params = {\n",
        "        'query': query,\n",
        "        'per_page': 1,\n",
        "        'page': 1,\n",
        "        'view': 'minimal'\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get('https://api.shutterstock.com/v2/images/search', headers=headers, params=params)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            return data.get('total_count', 0)\n",
        "        else:\n",
        "            logging.error(f\"Shutterstock API error {response.status_code}: {response.text}\")\n",
        "            return 0\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"Error checking Shutterstock for '{query}': {e}\")\n",
        "        return 0\n",
        "\n",
        "def generate_script_for_topic(topic, content_summary, style=\"Normal Script\"):\n",
        "    try:\n",
        "        style_prompts = {\n",
        "            \"Flashy Script\": \"Create a flashy, high-energy script with quick cuts, bold visuals, and impactful statements.\",\n",
        "            \"Expressive Script\": \"Compose an expressive script that evokes emotions, using descriptive language and a storytelling approach.\",\n",
        "            \"Normal Script\": \"Write a straightforward script that clearly presents the information in an engaging manner.\"\n",
        "        }\n",
        "        style_prompt = style_prompts.get(style, style_prompts[\"Normal Script\"])\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",  # Use 'gpt-4' model\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are a creative scriptwriter for short-form videos. \"\n",
        "                        \"Produce engaging, dynamic scripts that are visually compelling and can be represented \"\n",
        "                        \"using stock images and videos. The script should be suitable for a 60-second video, \"\n",
        "                        \"have a captivating introduction, a clear narrative flow, and a strong conclusion or call-to-action. \"\n",
        "                        \"Include specific details about the trend, such as goal scores, player performances, bookings, and other relevant statistics.\"\n",
        "                    )\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": (\n",
        "                        f\"{style_prompt}\\n\"\n",
        "                        f\"Using the following summary, write a detailed script suitable for a 60-second video:\\n\\n\"\n",
        "                        f\"Summary: {content_summary}\\n\\n\"\n",
        "                        \"Ensure the script includes vivid descriptions and is structured with a beginning, middle, and end. \"\n",
        "                        \"Use language that resonates with the target audience, and make sure it's adaptable with widely available stock images and videos.\"\n",
        "                    )\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=1500,  # Increased max_tokens for longer scripts\n",
        "            temperature=0.7,\n",
        "        )\n",
        "        script_content = response.choices[0].message.content.strip()\n",
        "        return script_content\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating script: {e}\")\n",
        "        return \"No script available.\"\n",
        "\n",
        "def generate_fan_sentiment_summary(title, sentiment):\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-4\",  # Use 'gpt-4' model\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are an assistant that summarizes fan sentiments from online discussions into a brief overview.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"The following Reddit post has a {sentiment} sentiment:\\n\\n'{title}'\\n\\nSummarize this sentiment for inclusion in a video script.\"\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=200,  # Increased max_tokens for longer summaries\n",
        "            temperature=0.5,\n",
        "        )\n",
        "        summary = response.choices[0].message.content.strip()\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating fan sentiment summary: {e}\")\n",
        "        return \"No summary available.\"\n"
      ],
      "metadata": {
        "id": "-2yQIa03r7Ml"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 9: Define the Main Function\n",
        "def main():\n",
        "    from IPython.display import display, Markdown\n",
        "    from prettytable import PrettyTable\n",
        "    import textwrap\n",
        "\n",
        "    # Step 1: User selects the country for Google Trends\n",
        "    print(\"Enter the country for Google Trends data (e.g., United States):\")\n",
        "    country_input = input(\"Country: \").strip()\n",
        "    matching_country = get_matching_country(country_input, available_countries)\n",
        "    if not matching_country:\n",
        "        print(\"No matching countries found. Exiting.\")\n",
        "        return\n",
        "    selected_country = matching_country\n",
        "    selected_country_code = available_countries[selected_country]\n",
        "    print(f\"You selected: {selected_country}\")\n",
        "\n",
        "    # Step 1b: User selects the time range for Google Trends\n",
        "    print(\"\\nSelect the time range for trending topics:\")\n",
        "    print(\"\\033[1m1.\\033[0m Last 4 hours (Realtime)\")\n",
        "    print(\"\\033[1m2.\\033[0m Last 24 hours (Daily)\")\n",
        "    time_range_selection = input(\"Enter the number of the time range you're interested in: \")\n",
        "    time_range_mapping = {'1': '4h', '2': '24h'}\n",
        "    time_range = time_range_mapping.get(time_range_selection)\n",
        "    if not time_range:\n",
        "        print(\"Invalid selection. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # List locations with daily results available\n",
        "    if time_range == '24h':\n",
        "        print(\"\\nLocations with daily trending data available:\")\n",
        "        for loc in country_code_to_pn_daily.keys():\n",
        "            print(f\"- {loc.capitalize()}\")\n",
        "\n",
        "    # Step 2: Fetch trending topics for the selected country and time range\n",
        "    google_trends_topics = fetch_trending_topics_pytrends(selected_country_code, time_range)\n",
        "\n",
        "    if not google_trends_topics:\n",
        "        logging.error(\"No trending topics found using PyTrends.\")\n",
        "        print(\"No trending topics found for the selected country and time range.\")\n",
        "        return\n",
        "\n",
        "    # Display the trending topics with additional information in a table\n",
        "    print(f\"\\nCurrent Trending Topics in {selected_country}:\\n\")\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [\"No.\", \"Topic\", \"Reason\"]\n",
        "    table.max_width = 40  # Set maximum width for columns\n",
        "    for idx, topic in enumerate(google_trends_topics, start=1):\n",
        "        title = textwrap.fill(topic['title'], width=40)\n",
        "        reason = textwrap.fill(topic['description'], width=30)\n",
        "        table.add_row([idx, title, reason])\n",
        "    print(table)\n",
        "\n",
        "    # Allow the user to select a topic\n",
        "    try:\n",
        "        selected_idx = int(input(\"Enter the number of the topic you're interested in: \"))\n",
        "        if 1 <= selected_idx <= len(google_trends_topics):\n",
        "            selected_topic = google_trends_topics[selected_idx - 1]['title']\n",
        "            print(f\"\\nYou selected: {selected_topic}\")\n",
        "        else:\n",
        "            logging.error(\"Invalid selection. Exiting.\")\n",
        "            return\n",
        "    except ValueError:\n",
        "        logging.error(\"Invalid input. Please enter a number. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Step 3: Fetch related news articles\n",
        "    related_stories = fetch_related_news(selected_topic, limit=5)\n",
        "    if related_stories:\n",
        "        print(\"\\nRelated News Articles:\")\n",
        "        for idx, story in enumerate(related_stories, start=1):\n",
        "            story_title = textwrap.fill(story['title'], width=70)\n",
        "            story_summary = textwrap.fill(story['summary'], width=70)\n",
        "            display(Markdown(f\"**{idx}. {story_title}**\\n{story_summary}\\n\"))\n",
        "    else:\n",
        "        print(\"\\nNo related news articles found.\")\n",
        "\n",
        "    # Step 4: Search for subreddits related to the topic\n",
        "    related_subreddits = search_subreddits_for_topic(selected_topic, limit=4)\n",
        "    if related_subreddits:\n",
        "        print(\"\\nRelated Subreddits:\")\n",
        "        for idx, sub in enumerate(related_subreddits, start=1):\n",
        "            sub_name = sub['name']\n",
        "            sub_title = textwrap.fill(sub['title'], width=50)\n",
        "            display(Markdown(f\"**{idx}. {sub_name}** - *{sub_title}*\"))\n",
        "    else:\n",
        "        print(\"\\nNo related subreddits found.\")\n",
        "\n",
        "    # Fetch and display top posts from subreddits\n",
        "    all_posts = []\n",
        "    if related_subreddits:\n",
        "        print(\"\\nTop Reddit Posts from Related Subreddits:\")\n",
        "        for sub in related_subreddits:\n",
        "            subreddit_name = sub['name']\n",
        "            try:\n",
        "                subreddit = reddit.subreddit(subreddit_name)\n",
        "                top_posts = list(subreddit.hot(limit=5))\n",
        "                for post in top_posts:\n",
        "                    if post.over_18:\n",
        "                        continue\n",
        "                    if post.is_self and len(post.selftext) < 20:\n",
        "                        continue  # Skip short self-posts\n",
        "                    all_posts.append({'subreddit': subreddit_name, 'title': post.title, 'score': post.score, 'url': post.url})\n",
        "            except praw.exceptions.APIException as e:\n",
        "                logging.error(f\"APIException when accessing subreddit '{subreddit_name}': {e}\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error fetching posts from subreddit '{subreddit_name}': {e}\")\n",
        "\n",
        "        if all_posts:\n",
        "            # Display posts in a table with formatting\n",
        "            table = PrettyTable()\n",
        "            table.field_names = [\"No.\", \"Subreddit\", \"Title\", \"Upvotes\"]\n",
        "            table.max_width = 40  # Set maximum width for columns\n",
        "            for idx, post in enumerate(all_posts, start=1):\n",
        "                title = textwrap.fill(post['title'], width=40)\n",
        "                table.add_row([idx, post['subreddit'], title, post['score']])\n",
        "            print(table)\n",
        "        else:\n",
        "            print(\"\\nNo suitable Reddit posts found.\")\n",
        "    else:\n",
        "        print(\"\\nNo Reddit posts to display.\")\n",
        "\n",
        "    # Combine news articles and Reddit posts\n",
        "    combined_content = []\n",
        "    for story in related_stories:\n",
        "        combined_content.append({'type': 'news', 'title': story['title'], 'url': story['url'], 'summary': story.get('summary', '')})\n",
        "    for post in all_posts:\n",
        "        combined_content.append({'type': 'reddit', 'title': post['title'], 'url': post['url'], 'score': post['score'], 'summary': ''})\n",
        "\n",
        "    if not combined_content:\n",
        "        logging.error(\"No content available for script generation. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Allow user to select content for script generation\n",
        "    print(\"\\nAvailable Content for Script Generation:\")\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [\"No.\", \"Type\", \"Title\", \"Upvotes\"]\n",
        "    table.max_width = 40  # Set maximum width for columns\n",
        "    for idx, content in enumerate(combined_content, start=1):\n",
        "        content_type = \"News Article\" if content['type'] == 'news' else \"Reddit Post\"\n",
        "        upvotes = content.get('score', '')\n",
        "        title = textwrap.fill(content['title'], width=40)\n",
        "        table.add_row([idx, content_type, title, upvotes])\n",
        "    print(table)\n",
        "\n",
        "    selected_content_input = input(\"\\nEnter the numbers of the items you want to generate scripts for, separated by commas (e.g., 1,3,5): \")\n",
        "    try:\n",
        "        selected_indices = [int(i.strip()) - 1 for i in selected_content_input.split(',') if i.strip().isdigit()]\n",
        "        selected_items = [combined_content[i] for i in selected_indices if 0 <= i < len(combined_content)]\n",
        "        if not selected_items:\n",
        "            logging.error(\"No valid items selected. Exiting.\")\n",
        "            return\n",
        "    except ValueError:\n",
        "        logging.error(\"Invalid input. Please enter numbers separated by commas. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Fetch content and generate summaries for selected items\n",
        "    print(f\"\\nFetching content and generating summaries for selected items...\")\n",
        "    for item in selected_items:\n",
        "        title = item['title']\n",
        "        if item['type'] == 'reddit':\n",
        "            # Analyze sentiment and include upvotes\n",
        "            sentiment = analyze_sentiment(title)\n",
        "            item['sentiment'] = sentiment\n",
        "            item['summary'] = generate_fan_sentiment_summary(title, sentiment)\n",
        "        else:\n",
        "            # For news articles, use the summary if available\n",
        "            if not item['summary']:\n",
        "                item['summary'] = generate_summary(title)\n",
        "\n",
        "    # Decide Whether to Check Stock Media Availability\n",
        "    check_media = input(\"\\nDo you want to check for stock media availability for the selected items? (yes/no): \").strip().lower()\n",
        "    if check_media not in ['yes', 'y']:\n",
        "        print(\"\\nSkipping stock media availability check.\")\n",
        "        # Proceed to generate scripts without checking\n",
        "        generate_scripts_from_items(selected_items)\n",
        "        print(\"\\nScript execution completed.\")\n",
        "        return\n",
        "\n",
        "    # Continue with stock media availability check\n",
        "    print(\"\\nChecking stock media availability for the selected items...\")\n",
        "    request_count = 0\n",
        "    scripts_generated = 0\n",
        "    for item in selected_items:\n",
        "        title = item['title']\n",
        "        summary = item.get('summary', \"No summary available.\")\n",
        "        if request_count >= 190:\n",
        "            logging.error(\"Approaching API rate limits. Waiting for 60 minutes before continuing...\")\n",
        "            time.sleep(3600)  # Wait for an hour\n",
        "            request_count = 0\n",
        "        total_media = check_stock_media_availability(title)\n",
        "        request_count += 1\n",
        "        if total_media >= 10:\n",
        "            # Display summary before generating script\n",
        "            print(f\"\\nSummary: {summary}\")\n",
        "            # Generate script for this topic\n",
        "            generate_script_with_style(title, summary)\n",
        "            scripts_generated += 1\n",
        "        else:\n",
        "            print(f\"\\nSummary: {summary}\")\n",
        "            print(f\"Not enough stock media for topic: {title}\\n====\\n\")\n",
        "    if scripts_generated == 0:\n",
        "        logging.error(\"No topics with sufficient stock media were found.\")\n",
        "    else:\n",
        "        logging.info(f\"Generated {scripts_generated} script(s) based on available stock media.\")\n",
        "    print(\"\\nScript execution completed.\")\n",
        "\n",
        "def generate_scripts_from_items(selected_items):\n",
        "    for item in selected_items:\n",
        "        title = item['title']\n",
        "        summary = item.get('summary', \"No summary available.\")\n",
        "        print(f\"\\nSummary: {summary}\")\n",
        "        generate_script_with_style(title, summary)\n",
        "        print(\"====\\n\")\n",
        "\n",
        "def generate_script_with_style(topic, summary):\n",
        "    from IPython.display import display, Markdown\n",
        "    print(\"\\nSelect a script style:\")\n",
        "    print(\"\\033[1m1.\\033[0m Flashy Script\")\n",
        "    print(\"\\033[1m2.\\033[0m Expressive Script\")\n",
        "    print(\"\\033[1m3.\\033[0m Normal Script\")\n",
        "    style_choice = input(\"Enter the number of the script style you prefer: \")\n",
        "    style_mapping = {'1': 'Flashy Script', '2': 'Expressive Script', '3': 'Normal Script'}\n",
        "    style = style_mapping.get(style_choice, 'Normal Script')\n",
        "    script = generate_script_for_topic(topic, summary, style=style)\n",
        "    # Display the script with enhanced formatting\n",
        "    display(Markdown(f\"### Generated {style} for '{topic}':\\n\\n{script}\"))\n"
      ],
      "metadata": {
        "id": "O06ulUQJr9wx"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 10: Run the Main Function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "0YOaDffLsA15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cef1b787-cec1-4996-ce61-167e1787fbc2"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the country for Google Trends data (e.g., United States):\n",
            "Country: fr\n",
            "You selected: France\n",
            "\n",
            "Select the time range for trending topics:\n",
            "\u001b[1m1.\u001b[0m Last 4 hours (Realtime)\n",
            "\u001b[1m2.\u001b[0m Last 24 hours (Daily)\n",
            "Enter the number of the time range you're interested in: 1\n",
            "\n",
            "Current Trending Topics in France:\n",
            "\n",
            "+-----+------------------------------------------+--------------------------------+\n",
            "| No. |                  Topic                   |             Reason             |\n",
            "+-----+------------------------------------------+--------------------------------+\n",
            "|  1  | FC Nantes, Olympique de Marseille, Ligue |     (Football match event)     |\n",
            "|     |         1, La Beaujoire Stadium          |                                |\n",
            "|  2  | Bob Sinclar, 20H30 Le Dimanche, Laurent  |      (TV show appearance)      |\n",
            "|     | Delahousse, France 2, Graldine Nakache, |                                |\n",
            "|     |  Jean-Paul Rouve, Journal de 20 heures   |                                |\n",
            "|  3  |  Laure Calamy, Bataclan, November 2015   |     (Recent film release)      |\n",
            "|     |          Paris attacks, Attack           |                                |\n",
            "|  4  |   Athletic Club, Real Betis Balompi,    |    (Recent football match)     |\n",
            "|     |       LaLiga, Estadio de San Mams       |                                |\n",
            "|  5  | Aviron Bayonnais, Top 14, Bayonne, Stade |      (Recent rugby match)      |\n",
            "|     |                Toulousain                |                                |\n",
            "|  6  | Florent Pagny, Ael Pagny, The Voice  La | (Family relationship revealed) |\n",
            "|     |     plus belle voix, Azucena Caamao     |                                |\n",
            "|  7  |    Jean-Paul Rouve, Bndicte Martin,    |       (New film release)       |\n",
            "|     |   Clotaire Rouve, Isabelle Nanty, The    |                                |\n",
            "|     |               Tuche Family               |                                |\n",
            "|  8  | Moldova, Presidential election, Russia,  |   (Maia Sandu wins election)   |\n",
            "|     |                Maia Sandu                |                                |\n",
            "|  9  |             Iceland, Volcano             |  (Volcano eruption reported)   |\n",
            "|  10 |          Iran, Student, Tehran           | (University protest reported)  |\n",
            "+-----+------------------------------------------+--------------------------------+\n",
            "Enter the number of the topic you're interested in: 6\n",
            "\n",
            "You selected: Florent Pagny, Ael Pagny, The Voice  La plus belle voix, Azucena Caamao\n",
            "\n",
            "No related news articles found.\n",
            "\n",
            "Related Subreddits:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**1. france** - *France*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**2. canada** - *Canada*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**3. playstation** - *PlayStation - Community  Sharing  Help*"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**4. Wovenhand** - *Wovenhand*"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top Reddit Posts from Related Subreddits:\n",
            "+-----+-------------+------------------------------------------+---------+\n",
            "| No. |  Subreddit  |                  Title                   | Upvotes |\n",
            "+-----+-------------+------------------------------------------+---------+\n",
            "|  1  |    france   |      Week-end Cration - semaine 45      |    2    |\n",
            "|  2  |    france   |         Forum Libre - 2024-11-03         |    9    |\n",
            "|  3  |    france   |        Notepad++ is 21 years old         |   400   |\n",
            "|  4  |    france   | Romainville : accus davoir simul son  |   159   |\n",
            "|     |             |  agression lors dune confrence sur la  |         |\n",
            "|     |             |      Palestine, un lu dmissionne       |         |\n",
            "|  5  |    france   | Accident de chasse : un cerf pitine un  |   707   |\n",
            "|     |             |                 chasseur                 |         |\n",
            "|  6  |    canada   |  Saturdays and Sundays are now Opinion-  |   109   |\n",
            "|     |             |                free days                 |         |\n",
            "|  7  |    canada   |  Welcome / Bienvenue / Witajcie to our   |    35   |\n",
            "|     |             |     Cultural Exchange with r/Polska      |         |\n",
            "|     |             |                (Poland)!                 |         |\n",
            "|  8  |    canada   |  '2032 is not good enough': Kelly Craft  |   579   |\n",
            "|     |             |    says Canada has to spend faster on    |         |\n",
            "|     |             |          defence if Trump wins           |         |\n",
            "|  9  |    canada   |  RCMP plans to go undercover online to   |   692   |\n",
            "|     |             |         trap violent extremists          |         |\n",
            "|  10 |    canada   |   Alberta's ruling party votes to dump   |   430   |\n",
            "|     |             |  emissions reduction plans and embrace   |         |\n",
            "|     |             |              carbon dioxide              |         |\n",
            "|  11 | playstation |     Weekly Tech Support & Purchase     |    1    |\n",
            "|     |             | Recommendations Megathread - November 2, |         |\n",
            "|     |             |                 2024                   |         |\n",
            "|  12 | playstation |     Megathread Weekly PlayStation      |    1    |\n",
            "|     |             |  Discussion - What Are You Playing This  |         |\n",
            "|     |             |       Week?  - November 2, 2024        |         |\n",
            "|  13 | playstation |    Playstation 5 portable home setup     |   846   |\n",
            "|  14 | playstation |             Because why not?             |   500   |\n",
            "|  15 | playstation |      Best game to not get a sequel       |   152   |\n",
            "|  16 |  Wovenhand  |   DEE - Thurs. Dec. 12 @ Ophelias in    |    12   |\n",
            "|     |             |                 Denver!                  |         |\n",
            "|  17 |  Wovenhand  |  Does anyone know what David is playing  |    3    |\n",
            "|     |             |           on the guitar here?            |         |\n",
            "|  18 |  Wovenhand  |   Rock Konzert am 21.9.2024 in Hamburg   |    0    |\n",
            "|     |             |     Bramfelder Chaussee 265  22177      |         |\n",
            "|  19 |  Wovenhand  |         Looking for info on Puur         |    2    |\n",
            "|  20 |  Wovenhand  | David Eugene Edwards Strawfoot version |    11   |\n",
            "|     |             |                (18:45min)                |         |\n",
            "+-----+-------------+------------------------------------------+---------+\n",
            "\n",
            "Available Content for Script Generation:\n",
            "+-----+-------------+------------------------------------------+---------+\n",
            "| No. |     Type    |                  Title                   | Upvotes |\n",
            "+-----+-------------+------------------------------------------+---------+\n",
            "|  1  | Reddit Post |      Week-end Cration - semaine 45      |    2    |\n",
            "|  2  | Reddit Post |         Forum Libre - 2024-11-03         |    9    |\n",
            "|  3  | Reddit Post |        Notepad++ is 21 years old         |   400   |\n",
            "|  4  | Reddit Post | Romainville : accus davoir simul son  |   159   |\n",
            "|     |             |  agression lors dune confrence sur la  |         |\n",
            "|     |             |      Palestine, un lu dmissionne       |         |\n",
            "|  5  | Reddit Post | Accident de chasse : un cerf pitine un  |   707   |\n",
            "|     |             |                 chasseur                 |         |\n",
            "|  6  | Reddit Post |  Saturdays and Sundays are now Opinion-  |   109   |\n",
            "|     |             |                free days                 |         |\n",
            "|  7  | Reddit Post |  Welcome / Bienvenue / Witajcie to our   |    35   |\n",
            "|     |             |     Cultural Exchange with r/Polska      |         |\n",
            "|     |             |                (Poland)!                 |         |\n",
            "|  8  | Reddit Post |  '2032 is not good enough': Kelly Craft  |   579   |\n",
            "|     |             |    says Canada has to spend faster on    |         |\n",
            "|     |             |          defence if Trump wins           |         |\n",
            "|  9  | Reddit Post |  RCMP plans to go undercover online to   |   692   |\n",
            "|     |             |         trap violent extremists          |         |\n",
            "|  10 | Reddit Post |   Alberta's ruling party votes to dump   |   430   |\n",
            "|     |             |  emissions reduction plans and embrace   |         |\n",
            "|     |             |              carbon dioxide              |         |\n",
            "|  11 | Reddit Post |     Weekly Tech Support & Purchase     |    1    |\n",
            "|     |             | Recommendations Megathread - November 2, |         |\n",
            "|     |             |                 2024                   |         |\n",
            "|  12 | Reddit Post |     Megathread Weekly PlayStation      |    1    |\n",
            "|     |             |  Discussion - What Are You Playing This  |         |\n",
            "|     |             |       Week?  - November 2, 2024        |         |\n",
            "|  13 | Reddit Post |    Playstation 5 portable home setup     |   846   |\n",
            "|  14 | Reddit Post |             Because why not?             |   500   |\n",
            "|  15 | Reddit Post |      Best game to not get a sequel       |   152   |\n",
            "|  16 | Reddit Post |   DEE - Thurs. Dec. 12 @ Ophelias in    |    12   |\n",
            "|     |             |                 Denver!                  |         |\n",
            "|  17 | Reddit Post |  Does anyone know what David is playing  |    3    |\n",
            "|     |             |           on the guitar here?            |         |\n",
            "|  18 | Reddit Post |   Rock Konzert am 21.9.2024 in Hamburg   |    0    |\n",
            "|     |             |     Bramfelder Chaussee 265  22177      |         |\n",
            "|  19 | Reddit Post |         Looking for info on Puur         |    2    |\n",
            "|  20 | Reddit Post | David Eugene Edwards Strawfoot version |    11   |\n",
            "|     |             |                (18:45min)                |         |\n",
            "+-----+-------------+------------------------------------------+---------+\n",
            "\n",
            "Enter the numbers of the items you want to generate scripts for, separated by commas (e.g., 1,3,5):  4\n",
            "\n",
            "Fetching content and generating summaries for selected items...\n",
            "\n",
            "Do you want to check for stock media availability for the selected items? (yes/no): n\n",
            "\n",
            "Skipping stock media availability check.\n",
            "\n",
            "Summary: \"In a recent Reddit post with a neutral sentiment, users discussed the incident of an elected official from Romainville who resigned after being accused of staging his own assault during a conference on Palestine.\"\n",
            "\n",
            "Select a script style:\n",
            "\u001b[1m1.\u001b[0m Flashy Script\n",
            "\u001b[1m2.\u001b[0m Expressive Script\n",
            "\u001b[1m3.\u001b[0m Normal Script\n",
            "Enter the number of the script style you prefer: 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Generated Flashy Script for 'Romainville : accus davoir simul son agression lors dune confrence sur la Palestine, un lu dmissionne':\n\nFADE IN:\n\n[00:00-00:02]\n(Stock footage of a bustling cityscape, possibly Parisian, with the text \"ROMAINVILLE\" appearing in bold, edgy font.)\n\nNARRATOR (V.O.)\n\"In the heart of Romainville, a political storm is brewing...\"\n\n[00:03-00:07]\n(Cut to stock footage of a generic politician at a podium, crowds cheering. Text overlays: \"ELECTED OFFICIAL.\")\n\nNARRATOR (V.O.)\n\"An elected official, the city's pride, now the subject of a shocking scandal...\"\n\n[00:08-00:12]\n(Stock footage of a staged fight scene, with the text \"THE ACCUSATION\" flashing on the screen.)\n\nNARRATOR (V.O.)\n\"Accused of staging his own assault during a conference on Palestine.\"\n\n[00:13-00:18]\n(Quick cut to stock image of an anonymous Reddit post, comments flowing rapidly.)\n\nNARRATOR (V.O.)\n\"The incident, now a hot topic on Reddit, stirs a whirlpool of opinions...\"\n\n[00:19-00:25]\n(Stock footage of a gavel striking, the words \"RESIGNATION\" flash on screen.)\n\nNARRATOR (V.O.)\n\"Under the weight of the accusation, the official resigns, leaving the city in shock...\"\n\n[00:26-00:32]\n(Stock footage of citizens, expressive faces of shock, disbelief, and disappointment.)\n\nNARRATOR (V.O.)\n\"Romainville citizens, bewildered...betrayed.\"\n\n[00:33-00:40]\n(Cut to stock footage of a person typing on a computer. The words \"NEUTRAL SENTIMENT\" appear.)\n\nNARRATOR (V.O.)\n\"Yet on Reddit, the sentiment remains neutral...\"\n\n[00:41-00:47]\n(Stock footage of a question mark, social media icons, and people in discussion.)\n\nNARRATOR (V.O.)\n\"Is this a case of political sabotage, or a self-inflicted downfall?\"\n\n[00:48-00:55]\n(Stock footage of cityscape at sunset, lights flickering on.)\n\nNARRATOR (V.O.)\n\"As the sun sets on Romainville, the question lingers in the air...\"\n\n[00:56-00:60]\n(Fade out to black screen with the text: \"JOIN THE DISCUSSION\".)\n\nNARRATOR (V.O.)\n\"Join the discussion... What's your take?\"\n\nFADE OUT.\n\nNARRATOR (V.O.)\n\"Stay tuned for updates.\"\n\nEND OF VIDEO."
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====\n",
            "\n",
            "\n",
            "Script execution completed.\n"
          ]
        }
      ]
    }
  ]
}