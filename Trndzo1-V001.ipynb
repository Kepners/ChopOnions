{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1s-3u0czUndpxCkgirEnqndEDGpEPjpJ_",
      "authorship_tag": "ABX9TyMZ98xhDwdEiokOI6gjw5/S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kepners/ChopOnions/blob/main/Trndzo1-V001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Install Required Libraries\n",
        "!pip install openai==0.27.8 praw==7.7.0 pytrends==4.9.0 feedparser==6.0.10 textblob requests cachetools==5.3.1 prettytable==3.7.0 textblob==0.17.1 python-dotenv==0.21.0 fuzzywuzzy==0.18.0 python-Levenshtein==0.20.9\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAUVB0guTX2b",
        "outputId": "252ce2fc-126b-45a3-f72b-16f6f5828f47",
        "collapsed": true
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.27.8 in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: praw==7.7.0 in /usr/local/lib/python3.10/dist-packages (7.7.0)\n",
            "Requirement already satisfied: pytrends==4.9.0 in /usr/local/lib/python3.10/dist-packages (4.9.0)\n",
            "Requirement already satisfied: feedparser==6.0.10 in /usr/local/lib/python3.10/dist-packages (6.0.10)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: cachetools==5.3.1 in /usr/local/lib/python3.10/dist-packages (5.3.1)\n",
            "Requirement already satisfied: prettytable==3.7.0 in /usr/local/lib/python3.10/dist-packages (3.7.0)\n",
            "Requirement already satisfied: python-dotenv==0.21.0 in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: fuzzywuzzy==0.18.0 in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: python-Levenshtein==0.20.9 in /usr/local/lib/python3.10/dist-packages (0.20.9)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (4.66.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.27.8) (3.10.10)\n",
            "Requirement already satisfied: prawcore<3,>=2.1 in /usr/local/lib/python3.10/dist-packages (from praw==7.7.0) (2.4.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw==7.7.0) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw==7.7.0) (1.8.0)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from pytrends==4.9.0) (2.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pytrends==4.9.0) (5.3.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser==6.0.10) (1.0.0)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable==3.7.0) (0.2.13)\n",
            "Requirement already satisfied: Levenshtein==0.20.9 in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein==0.20.9) (0.20.9)\n",
            "Requirement already satisfied: rapidfuzz<3.0.0,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein==0.20.9->python-Levenshtein==0.20.9) (2.15.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.9.11)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends==4.9.0) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends==4.9.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends==4.9.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends==4.9.0) (2024.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.27.8) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->openai==0.27.8) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25->pytrends==4.9.0) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->openai==0.27.8) (0.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 2: Mount Google Drive and Load Environment Variables\n",
        "from google.colab import drive\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to your .env file in Google Drive\n",
        "dotenv_path = '/content/drive/MyDrive/Secrets/.env'\n",
        "\n",
        "# Load the environment variables from the .env file\n",
        "load_dotenv(dotenv_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIud78FOZR3M",
        "outputId": "e400fcef-fb76-4e99-9926-49813e686d2d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 3: Configure Logging\n",
        "import logging\n",
        "\n",
        "# Configure Logging to write to a log file and suppress console output\n",
        "log_file = 'app.log'\n",
        "logging.basicConfig(\n",
        "    level=logging.ERROR,  # Only log ERROR and CRITICAL\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(log_file),\n",
        "        logging.StreamHandler(open(os.devnull, 'w'))  # Suppress console output\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "ujJviMETtuLY"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 4: Download NLTK Data Silently\n",
        "import nltk\n",
        "import sys\n",
        "from contextlib import contextmanager\n",
        "\n",
        "@contextmanager\n",
        "def suppress_stdout():\n",
        "    with open(os.devnull, \"w\") as devnull:\n",
        "        old_stdout = sys.stdout\n",
        "        sys.stdout = devnull\n",
        "        try:\n",
        "            yield\n",
        "        finally:\n",
        "            sys.stdout = old_stdout\n",
        "\n",
        "with suppress_stdout():\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('averaged_perceptron_tagger')\n",
        "    nltk.download('brown')\n",
        "    nltk.download('wordnet')\n",
        "    nltk.download('movie_reviews')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGPK1IRBXfBm",
        "outputId": "20acf18c-006c-4555-bde5-9c2180d2afd3"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 5: Create Utils Directory\n",
        "import os\n",
        "\n",
        "utils_dir = 'utils'\n",
        "if not os.path.exists(utils_dir):\n",
        "    os.makedirs(utils_dir)\n"
      ],
      "metadata": {
        "id": "RPd0MOuGTaTM",
        "collapsed": true
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 6: Create data_processing.py\n",
        "data_processing_code = \"\"\"\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from textblob import TextBlob\n",
        "\n",
        "def extract_keywords(text, max_keywords=10):\n",
        "    \\\"\"\"\n",
        "    Extracts up to `max_keywords` nouns from the input `text`.\n",
        "    \\\"\"\"\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords and non-alphabetic tokens\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "    # Get part-of-speech tags\n",
        "    tagged_words = pos_tag(filtered_words)\n",
        "    # Keep nouns and proper nouns\n",
        "    keywords = [word for word, pos in tagged_words if pos.startswith('NN')]\n",
        "    # Limit the number of keywords\n",
        "    return keywords[:max_keywords]\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    \\\"\"\"\n",
        "    Analyzes the sentiment of the given text and returns it.\n",
        "    \\\"\"\"\n",
        "    blob = TextBlob(text)\n",
        "    polarity = blob.sentiment.polarity\n",
        "    if polarity > 0.1:\n",
        "        return 'Positive'\n",
        "    elif polarity < -0.1:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\"\"\"\n",
        "with open(os.path.join('utils', 'data_processing.py'), 'w') as file:\n",
        "    file.write(data_processing_code)\n"
      ],
      "metadata": {
        "id": "bNicEhLDr06j"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 7: Initialize API Clients and Configure Logging\n",
        "import praw\n",
        "import openai\n",
        "import logging\n",
        "import warnings\n",
        "from utils.data_processing import extract_keywords, analyze_sentiment\n",
        "from cachetools import TTLCache, cached\n",
        "from pytrends.request import TrendReq\n",
        "import feedparser\n",
        "import requests\n",
        "import time\n",
        "import os\n",
        "from fuzzywuzzy import process\n",
        "\n",
        "# Suppress PRAW warnings about asynchronous environments\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='praw')\n",
        "logging.getLogger('praw').setLevel(logging.CRITICAL)\n",
        "\n",
        "# Initialize Reddit API using PRAW\n",
        "try:\n",
        "    reddit = praw.Reddit(\n",
        "        client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
        "        client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
        "        user_agent=os.getenv('REDDIT_USER_AGENT', 'script:TrendingTopicsScript:1.0 (by u/yourusername)')\n",
        "    )\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error initializing Reddit API: {e}\")\n",
        "\n",
        "# Initialize OpenAI API\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Initialize Pexels API key\n",
        "PEXELS_API_KEY = os.getenv('PEXELS_API_KEY')\n",
        "\n",
        "# Initialize Shutterstock Access Token\n",
        "SHUTTERSTOCK_ACCESS_TOKEN = os.getenv('SHUTTERSTOCK_ACCESS_TOKEN')\n",
        "\n",
        "# Define a list of countries with their codes, including common abbreviations\n",
        "available_countries = {\n",
        "    'United States': 'US',\n",
        "    'US': 'US',\n",
        "    'USA': 'US',\n",
        "    'India': 'IN',\n",
        "    'Canada': 'CA',\n",
        "    'United Kingdom': 'GB',\n",
        "    'UK': 'GB',\n",
        "    'Great Britain': 'GB',\n",
        "    'Australia': 'AU',\n",
        "    'Germany': 'DE',\n",
        "    'France': 'FR',\n",
        "    'Brazil': 'BR',\n",
        "    'Mexico': 'MX',\n",
        "    'Japan': 'JP',\n",
        "    'Russia': 'RU',\n",
        "    'South Korea': 'KR',\n",
        "    'Korea': 'KR',\n",
        "    'Italy': 'IT',\n",
        "    'Spain': 'ES',\n",
        "    'Netherlands': 'NL',\n",
        "    'Sweden': 'SE',\n",
        "    'Switzerland': 'CH',\n",
        "    'Austria': 'AT',\n",
        "    'Belgium': 'BE',\n",
        "    'New Zealand': 'NZ'\n",
        "}\n",
        "\n",
        "# Mapping from country codes to PyTrends 'pn' parameter for trending searches\n",
        "# For realtime_trending_searches, the acceptable 'pn' values are specific\n",
        "country_code_to_pn_realtime = {\n",
        "    'US': 'US',\n",
        "    'IN': 'IN',\n",
        "    'GB': 'GB',\n",
        "    'CA': 'CA',\n",
        "    'AU': 'AU',\n",
        "    'DE': 'DE',\n",
        "    'FR': 'FR',\n",
        "    'BR': 'BR',\n",
        "    'MX': 'MX',\n",
        "    'JP': 'JP',\n",
        "    'RU': 'RU',\n",
        "    'KR': 'KR'  # South Korea\n",
        "}\n",
        "\n",
        "# For trending_searches, the acceptable 'pn' values are:\n",
        "country_code_to_pn_daily = {\n",
        "    'united_states': 'united_states',\n",
        "    'us': 'united_states',\n",
        "    'india': 'india',\n",
        "    'united_kingdom': 'united_kingdom',\n",
        "    'uk': 'united_kingdom',\n",
        "    'gb': 'united_kingdom',\n",
        "    'canada': 'canada',\n",
        "    'australia': 'australia',\n",
        "    'germany': 'germany',\n",
        "    'france': 'france',\n",
        "    'brazil': 'brazil'\n",
        "    # Note: Limited countries are available for trending_searches\n",
        "}\n"
      ],
      "metadata": {
        "id": "C5RZdBozr4hJ"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 8: Define Supporting Functions\n",
        "import logging\n",
        "import re\n",
        "import textwrap\n",
        "import requests\n",
        "from cachetools import TTLCache, cached\n",
        "import feedparser\n",
        "import openai\n",
        "from fuzzywuzzy import process\n",
        "from pytrends.request import TrendReq\n",
        "import praw\n",
        "import time\n",
        "import os\n",
        "from textblob import TextBlob  # Ensure TextBlob is installed\n",
        "\n",
        "# Initialize OpenAI API\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "# Initialize Reddit API using PRAW\n",
        "reddit = praw.Reddit(\n",
        "    client_id=os.getenv('REDDIT_CLIENT_ID'),\n",
        "    client_secret=os.getenv('REDDIT_CLIENT_SECRET'),\n",
        "    user_agent=os.getenv('REDDIT_USER_AGENT', 'script:TrendingTopicsScript:1.0 (by u/yourusername)')\n",
        ")\n",
        "\n",
        "# Define a list of countries with their codes, including common abbreviations\n",
        "available_countries = {\n",
        "    'United States': 'US',\n",
        "    'US': 'US',\n",
        "    'USA': 'US',\n",
        "    'America': 'US',\n",
        "    'India': 'IN',\n",
        "    'Canada': 'CA',\n",
        "    'United Kingdom': 'GB',\n",
        "    'UK': 'GB',\n",
        "    'Great Britain': 'GB',\n",
        "    'Australia': 'AU',\n",
        "    'Germany': 'DE',\n",
        "    'France': 'FR',\n",
        "    'Brazil': 'BR',\n",
        "    'Mexico': 'MX',\n",
        "    'Japan': 'JP',\n",
        "    'Russia': 'RU',\n",
        "    'South Korea': 'KR',\n",
        "    'Korea': 'KR',\n",
        "    'Italy': 'IT',\n",
        "    'Spain': 'ES',\n",
        "    'Netherlands': 'NL',\n",
        "    'Sweden': 'SE',\n",
        "    'Switzerland': 'CH',\n",
        "    'Austria': 'AT',\n",
        "    'Belgium': 'BE',\n",
        "    'New Zealand': 'NZ'\n",
        "}\n",
        "\n",
        "# Mapping from country codes to PyTrends 'pn' parameter for trending searches\n",
        "country_code_to_pn_realtime = {\n",
        "    'US': 'united_states',\n",
        "    'IN': 'india',\n",
        "    'GB': 'united_kingdom',\n",
        "    'CA': 'canada',\n",
        "    'AU': 'australia',\n",
        "    'DE': 'germany',\n",
        "    'FR': 'france',\n",
        "    'BR': 'brazil',\n",
        "    'MX': 'mexico',\n",
        "    'JP': 'japan',\n",
        "    'RU': 'russia',\n",
        "    'KR': 'south_korea'  # South Korea\n",
        "}\n",
        "\n",
        "# For trending_searches, the acceptable 'pn' values are:\n",
        "country_code_to_pn_daily = {\n",
        "    'us': 'united_states',\n",
        "    'united_states': 'united_states',\n",
        "    'usa': 'united_states',\n",
        "    'america': 'united_states',\n",
        "    'india': 'india',\n",
        "    'united_kingdom': 'united_kingdom',\n",
        "    'uk': 'united_kingdom',\n",
        "    'gb': 'united_kingdom',\n",
        "    'canada': 'canada',\n",
        "    'australia': 'australia',\n",
        "    'germany': 'germany',\n",
        "    'france': 'france',\n",
        "    'brazil': 'brazil',\n",
        "    'mexico': 'mexico',\n",
        "    'japan': 'japan',\n",
        "    'russia': 'russia',\n",
        "    'south_korea': 'south_korea',\n",
        "    'korea': 'south_korea',\n",
        "    'italy': 'italy',\n",
        "    'spain': 'spain',\n",
        "    'netherlands': 'netherlands',\n",
        "    'sweden': 'sweden',\n",
        "    'switzerland': 'switzerland',\n",
        "    'austria': 'austria',\n",
        "    'belgium': 'belgium',\n",
        "    'new_zealand': 'new_zealand'\n",
        "}\n",
        "\n",
        "# List of supported 'pn' for Realtime trends\n",
        "supported_pn_realtime = [\n",
        "    'united_states',\n",
        "    'india',\n",
        "    'canada',\n",
        "    'australia',\n",
        "    'germany',\n",
        "    'france',\n",
        "    'brazil',\n",
        "    'mexico',\n",
        "    'japan',\n",
        "    'russia',\n",
        "    'south_korea'\n",
        "]\n",
        "\n",
        "# Initialize caches\n",
        "post_relevance_cache = TTLCache(maxsize=1000, ttl=3600)  # Cache for 1 hour\n",
        "trend_reason_cache = TTLCache(maxsize=1000, ttl=86400)  # Cache for 24 hours\n",
        "\n",
        "@cached(post_relevance_cache)\n",
        "def is_post_relevant(post_title, topic):\n",
        "    \"\"\"\n",
        "    Determines if a Reddit post is relevant to the selected topic using OpenAI's GPT-3.5-turbo.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are a helpful assistant that determines if a Reddit post is relevant to a given topic. \"\n",
        "                        \"You should respond with 'Yes' or 'No'.\"\n",
        "                    )\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Is the following Reddit post relevant to the topic '{topic}'?\\n\\nPost Title: {post_title}\\n\\nRespond with 'Yes' or 'No'.\"\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=1,\n",
        "            temperature=0,\n",
        "        )\n",
        "        answer = response.choices[0].message.content.strip().lower()\n",
        "        return answer == 'yes'\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error checking post relevance: {e}\")\n",
        "        return False\n",
        "\n",
        "def get_matching_country(input_country, available_countries):\n",
        "    \"\"\"\n",
        "    Uses fuzzy matching to find the best matching country from the available_countries.\n",
        "    \"\"\"\n",
        "    # Combine country names and codes\n",
        "    country_list = list(available_countries.keys())\n",
        "    # Use fuzzy matching to find the best match\n",
        "    match, score = process.extractOne(input_country, country_list)\n",
        "    if score >= 70:  # Threshold can be adjusted\n",
        "        return match\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def fetch_trending_topics_pytrends(selected_country_code, time_range, retries=3, backoff_factor=2):\n",
        "    \"\"\"\n",
        "    Fetches trending topics from Google Trends using PyTrends based on the selected country and time range.\n",
        "    Implements a retry mechanism for robustness.\n",
        "    \"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            pytrends = TrendReq(hl='en-US', tz=360)\n",
        "            if time_range == '4h':\n",
        "                # Realtime trending searches\n",
        "                pn = country_code_to_pn_realtime.get(selected_country_code)\n",
        "                if not pn:\n",
        "                    logging.error(f\"Realtime trending searches not available for the selected country.\")\n",
        "                    return []\n",
        "                if pn not in supported_pn_realtime:\n",
        "                    logging.error(f\"Realtime trending searches not supported for '{selected_country_code}'.\")\n",
        "                    return []\n",
        "                print(f\"Fetching Realtime trends for pn: {pn}\")  # Debugging\n",
        "                df = pytrends.realtime_trending_searches(pn=pn)\n",
        "            else:\n",
        "                # Daily trending searches\n",
        "                pn = country_code_to_pn_daily.get(selected_country_code.lower())\n",
        "                if not pn:\n",
        "                    logging.error(f\"Daily trending searches not available for the selected country.\")\n",
        "                    return []\n",
        "                print(f\"Fetching Daily trends for pn: {pn}\")  # Debugging\n",
        "                df = pytrends.trending_searches(pn=pn)\n",
        "            if df.empty:\n",
        "                logging.warning(\"No trending topics returned from PyTrends.\")\n",
        "                return []\n",
        "            trending_topics = []\n",
        "            for index, row in df.iterrows():\n",
        "                title = row.iloc[0]  # Use .iloc[0] instead of row[0]\n",
        "                # Generate a brief reason why the topic is trending\n",
        "                reason = generate_trend_reason(title)\n",
        "                trending_topics.append({\n",
        "                    'title': title,\n",
        "                    'description': reason,  # Brief reason why it's trending\n",
        "                    'sv': 'High',       # Placeholder for Search Volume\n",
        "                    'change': 'Up',     # Placeholder for Change\n",
        "                    'started': 'Recently'   # Placeholder\n",
        "                })\n",
        "            return trending_topics[:10]  # Limit to top 10\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Attempt {attempt + 1} - Error fetching Google Trends data via PyTrends: {e}\")\n",
        "            if attempt < retries - 1:\n",
        "                wait_time = backoff_factor ** attempt\n",
        "                logging.info(f\"Retrying in {wait_time} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                logging.error(\"Max retries reached. Returning empty list.\")\n",
        "                return []\n",
        "\n",
        "@cached(trend_reason_cache)\n",
        "def generate_trend_reason(topic, retries=3, backoff_factor=2):\n",
        "    \"\"\"\n",
        "    Generates a brief reason why a topic is trending using OpenAI's GPT-3.5-turbo.\n",
        "    Implements a retry mechanism to handle potential API errors.\n",
        "    \"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": \"You are an assistant that provides brief reasons why a topic is trending. Your responses should be concise, in brackets, and two to four words long.\"\n",
        "                    },\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": f\"Why is '{topic}' trending? Provide a brief reason in brackets, two to four words long.\"\n",
        "                    }\n",
        "                ],\n",
        "                max_tokens=20,\n",
        "                temperature=0.5,\n",
        "            )\n",
        "            reason = response.choices[0].message.content.strip()\n",
        "            return reason\n",
        "        except openai.error.OpenAIError as e:\n",
        "            logging.error(f\"Attempt {attempt + 1} - Error generating trend reason: {e}\")\n",
        "            if attempt < retries - 1:\n",
        "                wait_time = backoff_factor ** attempt\n",
        "                logging.info(f\"Retrying in {wait_time} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                logging.error(\"Max retries reached. Using default reason.\")\n",
        "                return \"[Trending Topic]\"\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Unexpected error generating trend reason: {e}\")\n",
        "            return \"[Trending Topic]\"\n",
        "\n",
        "def generate_summary(content):\n",
        "    \"\"\"\n",
        "    Generates a concise two-sentence summary of the provided content using OpenAI's GPT-3.5-turbo.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a concise summarizer. Provide a clear and brief two-sentence summary of the following content.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": content\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=150,  # Increased max_tokens for longer summaries\n",
        "            temperature=0.5,\n",
        "        )\n",
        "        summary = response.choices[0].message.content.strip()\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating summary: {e}\")\n",
        "        return \"No summary available.\"\n",
        "\n",
        "def search_subreddits_for_topic(topic, limit=4):\n",
        "    \"\"\"\n",
        "    Searches for subreddits related to the topic using individual entities.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Split the topic by commas to get individual entities\n",
        "        entities = [entity.strip() for entity in topic.split(',')]\n",
        "        related_subreddits = {}\n",
        "        for entity in entities:\n",
        "            # Use each entity as a query\n",
        "            subreddits_found = reddit.subreddits.search(query=entity, limit=limit)\n",
        "            for subreddit in subreddits_found:\n",
        "                # Fetch subreddit details\n",
        "                try:\n",
        "                    subreddit_details = reddit.subreddit(subreddit.display_name)\n",
        "                    # Check if subreddit is accessible and appropriate\n",
        "                    if subreddit_details.over_18 or subreddit_details.subreddit_type in ['private', 'restricted']:\n",
        "                        continue\n",
        "                    # Add subreddit if not already in the list\n",
        "                    if subreddit.display_name.lower() not in related_subreddits:\n",
        "                        related_subreddits[subreddit.display_name.lower()] = {\n",
        "                            'name': subreddit.display_name,\n",
        "                            'title': subreddit.title,\n",
        "                            'description': subreddit.public_description or ''\n",
        "                        }\n",
        "                    # Break if we've reached the limit\n",
        "                    if len(related_subreddits) >= limit:\n",
        "                        break\n",
        "                except Exception as inner_e:\n",
        "                    logging.error(f\"Error accessing subreddit '{subreddit.display_name}': {inner_e}\")\n",
        "                    continue\n",
        "            if len(related_subreddits) >= limit:\n",
        "                break  # Stop searching if we've found enough subreddits\n",
        "        return list(related_subreddits.values())\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error searching for subreddits: {e}\")\n",
        "        return []\n",
        "\n",
        "def fetch_related_news(topic, limit=5):\n",
        "    \"\"\"\n",
        "    Fetches related news articles using Google News RSS feed.\n",
        "    Correctly extracts the actual article URLs from Google News redirect links.\n",
        "    Includes sentiment analysis for each article.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        feed_url = f\"https://news.google.com/rss/search?q={requests.utils.quote(topic)}&hl=en-US&gl=US&ceid=US:en\"\n",
        "        feed = feedparser.parse(feed_url)\n",
        "        related_stories = []\n",
        "        for entry in feed.entries[:limit]:\n",
        "            title = entry.title\n",
        "            summary = entry.summary if 'summary' in entry else ''\n",
        "            # Remove HTML tags from summary\n",
        "            summary = re.sub('<[^<]+?>', '', summary)\n",
        "            link = entry.link\n",
        "            # Fix the link to get the actual news article URL\n",
        "            if 'link' in entry:\n",
        "                link = entry.link\n",
        "                # Extract actual article URL from Google News redirect link\n",
        "                parsed_link = requests.utils.urlparse(link)\n",
        "                actual_link = requests.utils.parse_qs(parsed_link.query).get('url')\n",
        "                if actual_link:\n",
        "                    link = actual_link[0]\n",
        "            else:\n",
        "                link = ''\n",
        "            # Generate summary using OpenAI if summary is empty\n",
        "            if not summary.strip():\n",
        "                summary = generate_summary(title)\n",
        "            # Perform sentiment analysis\n",
        "            sentiment = analyze_sentiment(title)\n",
        "            related_stories.append({\n",
        "                'title': title,\n",
        "                'summary': summary,\n",
        "                'url': link,\n",
        "                'sentiment': sentiment\n",
        "            })\n",
        "        return related_stories\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error fetching related news: {e}\")\n",
        "        return []\n",
        "\n",
        "@cached(TTLCache(maxsize=1000, ttl=86400))  # 24 hours cache\n",
        "def check_stock_media_availability(topic):\n",
        "    \"\"\"\n",
        "    Checks the availability of stock media related to the topic across Pexels and Shutterstock.\n",
        "    \"\"\"\n",
        "    # Extract keywords from the topic\n",
        "    keywords = extract_keywords(topic)\n",
        "    if not keywords:\n",
        "        logging.error(f\"No valid keywords extracted for topic: {topic}\")\n",
        "        return 0\n",
        "    # Join keywords into a query string\n",
        "    query = ' '.join(keywords)[:100]\n",
        "    total_results = 0\n",
        "    # Check Pexels\n",
        "    total_results += check_pexels_media(query)\n",
        "    # Check Shutterstock\n",
        "    total_results += check_shutterstock_media(query)\n",
        "    return total_results\n",
        "\n",
        "def check_pexels_media(query):\n",
        "    \"\"\"\n",
        "    Checks Pexels API for media related to the query.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        'Authorization': os.getenv('PEXELS_API_KEY'),\n",
        "        'User-Agent': 'TrendingTopicsScript'\n",
        "    }\n",
        "    params = {\n",
        "        'query': query,\n",
        "        'per_page': 1,\n",
        "        'page': 1\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get('https://api.pexels.com/v1/search', headers=headers, params=params)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            return data.get('total_results', 0)\n",
        "        else:\n",
        "            logging.error(f\"Pexels API error {response.status_code}: {response.text}\")\n",
        "            return 0\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"Error checking Pexels for '{query}': {e}\")\n",
        "        return 0\n",
        "\n",
        "def check_shutterstock_media(query):\n",
        "    \"\"\"\n",
        "    Checks Shutterstock API for media related to the query.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        'Authorization': f'Bearer {os.getenv(\"SHUTTERSTOCK_ACCESS_TOKEN\")}',\n",
        "        'User-Agent': 'TrendingTopicsScript'\n",
        "    }\n",
        "    params = {\n",
        "        'query': query,\n",
        "        'per_page': 1,\n",
        "        'page': 1,\n",
        "        'view': 'minimal'\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get('https://api.shutterstock.com/v2/images/search', headers=headers, params=params)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            return data.get('total_count', 0)\n",
        "        else:\n",
        "            logging.error(f\"Shutterstock API error {response.status_code}: {response.text}\")\n",
        "            return 0\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        logging.error(f\"Error checking Shutterstock for '{query}': {e}\")\n",
        "        return 0\n",
        "\n",
        "def generate_script_for_topic(topic, content_summary, style=\"Normal Script\"):\n",
        "    \"\"\"\n",
        "    Generates a video script for the given topic and summary using OpenAI's GPT-3.5-turbo.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        style_prompts = {\n",
        "            \"Flashy Script\": \"Create a flashy, high-energy script with quick cuts, bold visuals, and impactful statements.\",\n",
        "            \"Expressive Script\": \"Compose an expressive script that evokes emotions, using descriptive language and a storytelling approach.\",\n",
        "            \"Normal Script\": \"Write a straightforward script that clearly presents the information in an engaging manner.\"\n",
        "        }\n",
        "        style_prompt = style_prompts.get(style, style_prompts[\"Normal Script\"])\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",  # Switched to 'gpt-3.5-turbo' for faster responses\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": (\n",
        "                        \"You are a creative scriptwriter for short-form videos. \"\n",
        "                        \"Produce engaging, dynamic scripts that are visually compelling and can be represented \"\n",
        "                        \"using stock images and videos. The script should be suitable for a 60-second video, \"\n",
        "                        \"have a captivating introduction, a clear narrative flow, and a strong conclusion or call-to-action. \"\n",
        "                        \"Include specific details about the trend, such as goal scores, player performances, bookings, and other relevant statistics.\"\n",
        "                    )\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": (\n",
        "                        f\"{style_prompt}\\n\"\n",
        "                        f\"Using the following summary, write a detailed script suitable for a 60-second video:\\n\\n\"\n",
        "                        f\"Summary: {content_summary}\\n\\n\"\n",
        "                        \"Ensure the script includes vivid descriptions and is structured with a beginning, middle, and end. \"\n",
        "                        \"Use language that resonates with the target audience, and make sure it's adaptable with widely available stock images and videos.\"\n",
        "                    )\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=1500,  # Increased max_tokens for longer scripts\n",
        "            temperature=0.7,\n",
        "        )\n",
        "        script_content = response.choices[0].message.content.strip()\n",
        "        return script_content\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating script: {e}\")\n",
        "        return \"No script available.\"\n",
        "\n",
        "def generate_fan_sentiment_summary(title, sentiment):\n",
        "    \"\"\"\n",
        "    Generates a summary of fan sentiment from a Reddit post title using OpenAI's GPT-3.5-turbo.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",  # Switched to 'gpt-3.5-turbo' for faster responses\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are an assistant that summarizes fan sentiments from online discussions into a brief overview.\"\n",
        "                },\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"The following Reddit post has a {sentiment} sentiment:\\n\\n'{title}'\\n\\nSummarize this sentiment for inclusion in a video script.\"\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=200,  # Increased max_tokens for longer summaries\n",
        "            temperature=0.5,\n",
        "        )\n",
        "        summary = response.choices[0].message.content.strip()\n",
        "        return summary\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error generating fan sentiment summary: {e}\")\n",
        "        return \"No summary available.\"\n",
        "\n",
        "def extract_keywords(topic):\n",
        "    \"\"\"\n",
        "    Extracts keywords from the topic for media search.\n",
        "    This is a placeholder function. Implement actual keyword extraction as needed.\n",
        "    \"\"\"\n",
        "    # Simple implementation: split by commas and return stripped words\n",
        "    return [word.strip() for word in topic.split(',') if word.strip()]\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    \"\"\"\n",
        "    Analyzes the sentiment of the given text using TextBlob.\n",
        "    Returns 'Positive', 'Negative', or 'Neutral'.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        blob = TextBlob(text)\n",
        "        polarity = blob.sentiment.polarity\n",
        "        if polarity > 0.1:\n",
        "            return \"Positive\"\n",
        "        elif polarity < -0.1:\n",
        "            return \"Negative\"\n",
        "        else:\n",
        "            return \"Neutral\"\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error analyzing sentiment: {e}\")\n",
        "        return \"Neutral\"\n"
      ],
      "metadata": {
        "id": "-2yQIa03r7Ml"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 9: Define the Main Function\n",
        "import sys\n",
        "import threading\n",
        "import random\n",
        "import textwrap\n",
        "import time\n",
        "from IPython.display import display, Markdown\n",
        "from prettytable import PrettyTable\n",
        "import logging\n",
        "\n",
        "def main():\n",
        "    # Function to display a changing message every 15 seconds\n",
        "    def flashing_message(stop_event):\n",
        "        messages = [\n",
        "            \"🔍 Gathering the latest trends...\",\n",
        "            \"⏳ Processing data, please wait...\",\n",
        "            \"✨ Almost there, thank you for your patience!\"\n",
        "        ]\n",
        "        idx = 0\n",
        "        while not stop_event.is_set():\n",
        "            message = messages[idx % len(messages)]\n",
        "            print(f\"\\r{message}   \", end='', flush=True)\n",
        "            for _ in range(15):\n",
        "                if stop_event.is_set():\n",
        "                    break\n",
        "                time.sleep(1)\n",
        "            idx += 1\n",
        "            print('\\r' + ' ' * len(message) + '   ', end='', flush=True)\n",
        "\n",
        "    # Step 1: User selects the country for Google Trends\n",
        "    while True:\n",
        "        print(\"Enter the country for Google Trends data (e.g., United States):\")\n",
        "        country_input = input(\"Country: \").strip()\n",
        "        matching_country = get_matching_country(country_input, available_countries)\n",
        "        if not matching_country:\n",
        "            print(\"No matching countries found. Please try again.\")\n",
        "            # Optionally, display available countries\n",
        "            print(\"Available countries are:\")\n",
        "            for country in available_countries.keys():\n",
        "                print(f\"- {country}\")\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    selected_country = matching_country\n",
        "    selected_country_code = available_countries[selected_country]\n",
        "    print(f\"You selected: {selected_country}\")\n",
        "\n",
        "    # Step 1b: User selects the time range for Google Trends\n",
        "    print(\"\\nSelect the time range for trending topics:\")\n",
        "    print(\"\\033[1mA.\\033[0m Last 4 hours (Realtime)\")\n",
        "    print(\"\\033[1mB.\\033[0m Last 24 hours (Daily)\")\n",
        "    time_range_selection = input(\"Enter the letter of the time range you're interested in: \").strip().upper()\n",
        "    time_range_mapping = {'A': '4h', 'B': '24h'}\n",
        "    time_range = time_range_mapping.get(time_range_selection)\n",
        "    if not time_range:\n",
        "        print(\"Invalid selection. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Validate if the selected country supports the chosen time range\n",
        "    if time_range == '4h':\n",
        "        pn = country_code_to_pn_realtime.get(selected_country_code)\n",
        "        if not pn or pn not in supported_pn_realtime:\n",
        "            print(f\"Realtime trending searches are not supported for '{selected_country}'. Switching to Daily trends.\")\n",
        "            logging.info(f\"Switching to Daily trends for '{selected_country}'.\")\n",
        "            time_range = '24h'\n",
        "\n",
        "    # Start flashing message while fetching trending topics\n",
        "    stop_event = threading.Event()\n",
        "    thread = threading.Thread(target=flashing_message, args=(stop_event,))\n",
        "    thread.start()\n",
        "\n",
        "    # Step 2: Fetch trending topics for the selected country and time range\n",
        "    google_trends_topics = fetch_trending_topics_pytrends(selected_country_code, time_range)\n",
        "\n",
        "    # Stop flashing message\n",
        "    stop_event.set()\n",
        "    thread.join()\n",
        "    print()  # Move to the next line after flashing message\n",
        "\n",
        "    if not google_trends_topics:\n",
        "        logging.error(\"No trending topics found using PyTrends.\")\n",
        "        print(\"No trending topics found for the selected country and time range.\")\n",
        "        return\n",
        "\n",
        "    # Display the trending topics with additional information in a table\n",
        "    print(f\"\\nCurrent Trending Topics in {selected_country}:\\n\")\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [\"No.\", \"Topic\", \"Reason\"]\n",
        "    table.hrules = prettytable.ALL  # Add horizontal lines between rows\n",
        "    table.max_width = 40  # Set maximum width for columns suitable for phone screens\n",
        "    for idx, topic in enumerate(google_trends_topics, start=1):\n",
        "        title = textwrap.fill(topic['title'], width=40)\n",
        "        reason = textwrap.fill(topic['description'], width=40)\n",
        "        table.add_row([idx, title, reason])\n",
        "    print(table)\n",
        "\n",
        "    # Allow the user to select a topic\n",
        "    try:\n",
        "        selected_idx = int(input(\"Enter the number of the topic you're interested in: \"))\n",
        "        if 1 <= selected_idx <= len(google_trends_topics):\n",
        "            selected_topic = google_trends_topics[selected_idx - 1]['title']\n",
        "            # Apply text wrapping to the selected topic message\n",
        "            selected_topic_display = textwrap.fill(f\"You selected: {selected_topic}\", width=40)\n",
        "            print(f\"\\n{selected_topic_display}\")\n",
        "        else:\n",
        "            logging.error(\"Invalid selection. Exiting.\")\n",
        "            print(\"Invalid selection. Exiting.\")\n",
        "            return\n",
        "    except ValueError:\n",
        "        logging.error(\"Invalid input. Please enter a number. Exiting.\")\n",
        "        print(\"Invalid input. Please enter a number. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Step 3: Fetch related news articles\n",
        "    related_stories = fetch_related_news(selected_topic, limit=5)\n",
        "    if related_stories:\n",
        "        print(\"\\nRelated News Articles:\")\n",
        "        for idx, story in enumerate(related_stories, start=1):\n",
        "            story_title = textwrap.fill(story['title'], width=40)\n",
        "            story_summary = textwrap.fill(story['summary'], width=40)\n",
        "            display(Markdown(f\"**{idx}. {story_title}**\\n{story_summary}\\n\"))\n",
        "    else:\n",
        "        print(\"\\nNo related news articles found.\")\n",
        "\n",
        "    # Step 4: Search Reddit directly using the topic as a query\n",
        "    print(\"\\nSearching Reddit for posts related to the topic...\")\n",
        "    all_posts = []\n",
        "    try:\n",
        "        subreddit = reddit.subreddit('all')\n",
        "        search_results = subreddit.search(selected_topic, limit=20)\n",
        "        for post in search_results:\n",
        "            if post.over_18:\n",
        "                continue\n",
        "            if is_post_relevant(post.title, selected_topic):\n",
        "                all_posts.append({\n",
        "                    'subreddit': post.subreddit.display_name,\n",
        "                    'title': post.title,\n",
        "                    'score': post.score,\n",
        "                    'url': post.url\n",
        "                })\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error searching Reddit: {e}\")\n",
        "\n",
        "    # Display posts in a table with formatting\n",
        "    if all_posts:\n",
        "        table = PrettyTable()\n",
        "        table.field_names = [\"No.\", \"Subreddit\", \"Title\", \"Upvotes\"]\n",
        "        table.hrules = prettytable.ALL  # Add horizontal lines\n",
        "        table.max_width = 40  # Adjust width as needed\n",
        "        for idx, post in enumerate(all_posts, start=1):\n",
        "            title = textwrap.fill(post['title'], width=40)\n",
        "            table.add_row([idx, post['subreddit'], title, post['score']])\n",
        "        print(table)\n",
        "    else:\n",
        "        print(\"\\nNo relevant Reddit posts found.\")\n",
        "\n",
        "    # Combine news articles and Reddit posts\n",
        "    combined_content = []\n",
        "    for story in related_stories:\n",
        "        combined_content.append({\n",
        "            'type': 'News Article',\n",
        "            'title': story['title'],\n",
        "            'url': story['url'],\n",
        "            'summary': story.get('summary', ''),\n",
        "            'sentiment': story.get('sentiment', 'Neutral')\n",
        "        })\n",
        "    for post in all_posts:\n",
        "        combined_content.append({\n",
        "            'type': 'Reddit Post',\n",
        "            'title': post['title'],\n",
        "            'url': post['url'],\n",
        "            'score': post['score'],\n",
        "            'summary': '',\n",
        "            'sentiment': post['sentiment']\n",
        "        })\n",
        "\n",
        "    if not combined_content:\n",
        "        logging.error(\"No content available for script generation. Exiting.\")\n",
        "        print(\"\\nNo content available for script generation.\")\n",
        "        return\n",
        "\n",
        "    # Allow user to select content for script generation\n",
        "    print(\"\\nAvailable Content for Script Generation:\")\n",
        "    table = PrettyTable()\n",
        "    table.field_names = [\"No.\", \"Type\", \"Title\", \"Sentiment\"]\n",
        "    table.hrules = prettytable.ALL  # Add horizontal lines between rows\n",
        "    table.max_width = 40  # Adjust width as needed\n",
        "    for idx, content in enumerate(combined_content, start=1):\n",
        "        content_type = content['type']\n",
        "        title = textwrap.fill(content['title'], width=40)\n",
        "        sentiment = content.get('sentiment', 'N/A')\n",
        "        table.add_row([idx, content_type, title, sentiment])\n",
        "    print(table)\n",
        "\n",
        "    # Shorten the input prompt and ensure it wraps properly\n",
        "    input_prompt = \"Enter the numbers of the items you want to generate scripts for (e.g., 1,3,5):\\n\"\n",
        "    try:\n",
        "        selected_content_input = input(input_prompt)\n",
        "        selected_indices = [int(i.strip()) - 1 for i in selected_content_input.split(',') if i.strip().isdigit()]\n",
        "        selected_items = [combined_content[i] for i in selected_indices if 0 <= i < len(combined_content)]\n",
        "        if not selected_items:\n",
        "            logging.error(\"No valid items selected. Exiting.\")\n",
        "            print(\"No valid items selected. Exiting.\")\n",
        "            return\n",
        "    except ValueError:\n",
        "        logging.error(\"Invalid input. Please enter numbers separated by commas. Exiting.\")\n",
        "        print(\"Invalid input. Please enter numbers separated by commas. Exiting.\")\n",
        "        return\n",
        "\n",
        "    # Fetch content and generate summaries for selected items\n",
        "    print(\"\\nFetching content and generating summaries for selected items...\")\n",
        "    for item in selected_items:\n",
        "        title = item['title']\n",
        "        sentiment = item['sentiment']\n",
        "        if item['type'] == 'Reddit Post':\n",
        "            item['summary'] = generate_fan_sentiment_summary(title, sentiment)\n",
        "        else:\n",
        "            if not item['summary']:\n",
        "                item['summary'] = generate_summary(title)\n",
        "\n",
        "    # Decide Whether to Check Stock Media Availability\n",
        "    check_media = input(\"\\nDo you want to check for stock media availability for the selected items? (yes/no): \").strip().lower()\n",
        "    if check_media not in ['yes', 'y']:\n",
        "        print(\"\\nSkipping stock media availability check.\")\n",
        "        # Proceed to generate scripts without checking\n",
        "        generate_scripts_from_items(selected_items)\n",
        "        print(\"\\nScript execution completed.\")\n",
        "        return\n",
        "\n",
        "    # Continue with stock media availability check\n",
        "    print(\"\\nChecking stock media availability for the selected items...\")\n",
        "    request_count = 0\n",
        "    scripts_generated = 0\n",
        "    for item in selected_items:\n",
        "        title = item['title']\n",
        "        summary = item.get('summary', \"No summary available.\")\n",
        "        if request_count >= 190:\n",
        "            logging.error(\"Approaching API rate limits. Waiting for 60 minutes before continuing...\")\n",
        "            print(\"Approaching API rate limits. Waiting for 60 minutes before continuing...\")\n",
        "            time.sleep(3600)  # Wait for an hour\n",
        "            request_count = 0\n",
        "        total_media = check_stock_media_availability(title)\n",
        "        request_count += 1\n",
        "        if total_media >= 10:\n",
        "            # Display summary before generating script\n",
        "            summary_wrapped = textwrap.fill(summary, width=40)\n",
        "            print(f\"\\nSummary: {summary_wrapped}\")\n",
        "            # Generate script for this topic\n",
        "            generate_script_with_style(title, summary)\n",
        "            scripts_generated += 1\n",
        "        else:\n",
        "            summary_wrapped = textwrap.fill(summary, width=40)\n",
        "            print(f\"\\nSummary: {summary_wrapped}\")\n",
        "            print(f\"Not enough stock media for topic: {title}\\n====\\n\")\n",
        "    if scripts_generated == 0:\n",
        "        logging.error(\"No topics with sufficient stock media were found.\")\n",
        "        print(\"\\nNo topics with sufficient stock media were found.\")\n",
        "    else:\n",
        "        logging.info(f\"Generated {scripts_generated} script(s) based on available stock media.\")\n",
        "    print(\"\\nScript execution completed.\")\n",
        "\n",
        "def generate_scripts_from_items(selected_items):\n",
        "    for item in selected_items:\n",
        "        title = item['title']\n",
        "        summary = item.get('summary', \"No summary available.\")\n",
        "        summary_wrapped = textwrap.fill(summary, width=40)\n",
        "        print(f\"\\nSummary: {summary_wrapped}\")\n",
        "        generate_script_with_style(title, summary)\n",
        "        print(\"====\\n\")\n",
        "\n",
        "def generate_script_with_style(topic, summary):\n",
        "    from IPython.display import display, Markdown\n",
        "    print(\"\\nSelect a script style:\")\n",
        "    print(\"\\033[1m1.\\033[0m Flashy Script\")\n",
        "    print(\"\\033[1m2.\\033[0m Expressive Script\")\n",
        "    print(\"\\033[1m3.\\033[0m Normal Script\")\n",
        "    style_choice = input(\"Enter the number of the script style you prefer: \")\n",
        "    style_mapping = {'1': 'Flashy Script', '2': 'Expressive Script', '3': 'Normal Script'}\n",
        "    style = style_mapping.get(style_choice, 'Normal Script')\n",
        "    script = generate_script_for_topic(topic, summary, style=style)\n",
        "    # Display the script with enhanced formatting\n",
        "    script_wrapped = textwrap.fill(script, width=40)\n",
        "    display(Markdown(f\"### Generated {style} for '{topic}':\\n\\n{script_wrapped}\"))\n"
      ],
      "metadata": {
        "id": "O06ulUQJr9wx"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 10: Run the Main Function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "0YOaDffLsA15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd0f6b47-22a3-4900-9bfd-323449c43ee9"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the country for Google Trends data (e.g., United States):\n",
            "Country: us\n",
            "You selected: US\n",
            "\n",
            "Select the time range for trending topics:\n",
            "\u001b[1mA.\u001b[0m Last 4 hours (Realtime)\n",
            "\u001b[1mB.\u001b[0m Last 24 hours (Daily)\n",
            "Enter the letter of the time range you're interested in: a\n",
            "🔍 Gathering the latest trends...   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Attempt 1 - Error fetching Google Trends data via PyTrends: The request failed: Google returned a response with code 400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching Realtime trends for pn: united_states\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Attempt 2 - Error fetching Google Trends data via PyTrends: The request failed: Google returned a response with code 400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching Realtime trends for pn: united_states\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Attempt 3 - Error fetching Google Trends data via PyTrends: The request failed: Google returned a response with code 400\n",
            "ERROR:root:Max retries reached. Returning empty list.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching Realtime trends for pn: united_states\n",
            "                                   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:No trending topics found using PyTrends.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "No trending topics found for the selected country and time range.\n"
          ]
        }
      ]
    }
  ]
}