{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kepners/ChopOnions/blob/main/Trndzo2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Install Required Packages\n",
        "\n",
        "!pip install --upgrade openai==0.28.1 python-dotenv praw rake_nltk newsapi-python --upgrade lxml_html_clean bleach --upgrade newspaper3k feedparser aiohttp nest_asyncio structlog beautifulsoup4 cachetools fuzzywuzzy python-Levenshtein nltk pytrends ratelimit prettytable bs4 configparser structlog\n",
        "print(\"pip install completed\")\n",
        "import nest_asyncio\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "import lxml\n",
        "import newspaper\n",
        "\n",
        "print(f\"lxml version: {lxml.__version__}\")\n",
        "print(f\"newspaper version: {newspaper.__version__}\")\n",
        "\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "print(\"vader_lexicon downloaded\")\n",
        "nltk.download('stopwords')\n",
        "print(\"stopwords downloaded\")\n",
        "nltk.download('punkt')\n",
        "print(\"punkt downloaded\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "print(\"averaged_perceptron_tagger downloaded\")\n",
        "nltk.download('maxent_ne_chunker')\n",
        "print(\"maxent_ne_chunker downloaded\")\n",
        "nltk.download('words')\n",
        "print(\"words downloaded\")\n",
        "nltk.download('wordnet')\n",
        "print(\"wordnet downloaded\")\n",
        "nltk.download('punkt_tab')\n",
        "print('punkt_tab downloaded')\n",
        "\n",
        "\n",
        "print(\"Installation and NLTK data download completed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGNrZ1kNMSXk",
        "outputId": "047fee96-bd3e-4dc4-b7c1-543995bced96"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28.1 in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: praw in /usr/local/lib/python3.10/dist-packages (7.8.1)\n",
            "Requirement already satisfied: rake_nltk in /usr/local/lib/python3.10/dist-packages (1.0.6)\n",
            "Requirement already satisfied: newsapi-python in /usr/local/lib/python3.10/dist-packages (0.2.7)\n",
            "Requirement already satisfied: lxml_html_clean in /usr/local/lib/python3.10/dist-packages (0.3.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (6.2.0)\n",
            "Requirement already satisfied: newspaper3k in /usr/local/lib/python3.10/dist-packages (0.2.8)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.10/dist-packages (6.0.11)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.11.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: structlog in /usr/local/lib/python3.10/dist-packages (24.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (5.5.0)\n",
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.10/dist-packages (0.26.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pytrends in /usr/local/lib/python3.10/dist-packages (4.9.2)\n",
            "Requirement already satisfied: ratelimit in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (3.12.0)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: configparser in /usr/local/lib/python3.10/dist-packages (7.1.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (4.66.6)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in /usr/local/lib/python3.10/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from lxml_html_clean) (5.3.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach) (0.5.1)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (11.0.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.2)\n",
            "Requirement already satisfied: cssselect>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (1.2.0)\n",
            "Requirement already satisfied: tldextract>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (5.1.3)\n",
            "Requirement already satisfied: feedfinder2>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.0.4)\n",
            "Requirement already satisfied: jieba3k>=0.35.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.35.1)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Requirement already satisfied: tinysegmenter==0.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (0.3)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser) (1.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: Levenshtein==0.26.1 in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein) (0.26.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein==0.26.1->python-Levenshtein) (3.10.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from pytrends) (2.2.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable) (0.2.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (1.26.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2024.8.30)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.16.1)\n",
            "pip install completed\n",
            "lxml version: 5.3.0\n",
            "newspaper version: 0.2.8\n",
            "vader_lexicon downloaded\n",
            "stopwords downloaded\n",
            "punkt downloaded\n",
            "averaged_perceptron_tagger downloaded\n",
            "maxent_ne_chunker downloaded\n",
            "words downloaded\n",
            "wordnet downloaded\n",
            "punkt_tab downloaded\n",
            "Installation and NLTK data download completed successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 2: Mount Google Drive and Load Configuration\n",
        "from google.colab import drive\n",
        "import configparser\n",
        "import os\n",
        "import openai  # import the openai module\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to your config.ini in Google Drive\n",
        "config_path = '/content/drive/MyDrive/Secrets/config.ini'  # Update this path as needed\n",
        "\n",
        "# Check if config.ini exists at the specified path\n",
        "if not os.path.exists(config_path):\n",
        "    raise FileNotFoundError(f\"config.ini not found at {config_path}\")\n",
        "\n",
        "# Load configuration using configparser\n",
        "config = configparser.ConfigParser()\n",
        "config.read(config_path)\n",
        "\n",
        "# Retrieve OpenAI API key\n",
        "try:\n",
        "    openai_api_key = config.get('openai', 'api_key', fallback=None)\n",
        "except KeyError:\n",
        "    raise ValueError(\"OpenAI API key not found or invalid format in config.ini under [openai] section.\")\n",
        "\n",
        "if not openai_api_key:\n",
        "    raise ValueError(\"OpenAI API key not found or invalid format in config.ini under [openai] section.\")\n",
        "\n",
        "# Set the OpenAI API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "# Retrieve NewsAPI key\n",
        "try:\n",
        "    newsapi_key = config.get('newsapi', 'api_key')\n",
        "except KeyError:\n",
        "    raise ValueError(\"NewsAPI key not found in config.ini under [newsapi] section.\")\n",
        "\n",
        "# Retrieve caching configurations\n",
        "rss_cache_ttl = config.getint('CACHING', 'RSS_CACHE_TTL', fallback=86400)\n",
        "trend_cache_ttl = config.getint('CACHING', 'TREND_CACHE_TTL', fallback=3600)\n",
        "openai_cache_ttl = config.getint('CACHING', 'OPENAI_CACHE_TTL', fallback=86400)\n",
        "\n",
        "# Retrieve rate limiting configurations\n",
        "rss_calls_per_day = config.getint('RATE_LIMITING', 'RSS_CALLS_PER_DAY', fallback=1000)\n",
        "\n",
        "print(\"Configuration loaded successfully.\")\n"
      ],
      "metadata": {
        "id": "BNlyvpVs_QsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bcc066c-6a07-4207-dbdd-e4d30f7a8e5a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Configuration loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 3: Define Data Structures\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional\n",
        "\n",
        "@dataclass\n",
        "class GoogleTrend:\n",
        "    topic: str\n",
        "    interest: str\n",
        "    sentiment: str\n",
        "\n",
        "@dataclass\n",
        "class Trend:\n",
        "    title: str\n",
        "    description: str\n",
        "    source: str\n",
        "    approx_traffic: str\n",
        "    sentiment: str\n",
        "    google_trend: Optional[GoogleTrend] = None\n",
        "\n",
        "@dataclass\n",
        "class ScriptOptions:\n",
        "    style: str = \"Normal Script\"\n",
        "    tone: str = \"Informative\"\n",
        "    length: str = \"60 seconds\"\n",
        "\n",
        "print(\"Data structures defined successfully.\")\n"
      ],
      "metadata": {
        "id": "4wkdmt7yp0mR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85facd94-bf8c-4067-99fb-7e6f0c4976b9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data structures defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gIud78FOZR3M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6d6aa03-1e07-402c-db1d-3e75b25f5fc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration and initialization completed successfully.\n"
          ]
        }
      ],
      "source": [
        "# Block 4: Configuration and Initialization\n",
        "\n",
        "import configparser\n",
        "from pytrends.request import TrendReq # This import will now work\n",
        "from newsapi import NewsApiClient\n",
        "\n",
        "# Initialize OpenAI\n",
        "api_key = openai_api_key\n",
        "\n",
        "# Initialize NewsAPI\n",
        "newsapi = NewsApiClient(api_key=newsapi_key)\n",
        "\n",
        "# Initialize PyTrends with custom requests arguments\n",
        "pytrends = TrendReq(hl='en-US', tz=360, requests_args={'headers': {'User-Agent': 'Mozilla/5.0'}})\n",
        "\n",
        "print(\"Configuration and initialization completed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wGPK1IRBXfBm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d86802f3-ca15-45f8-b203-cd52b3525853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'__init__.py' already exists at 'utils/__init__.py'.\n",
            "Created 'utils/data_processing.py' successfully.\n",
            "\n",
            "Verifying the creation of 'data_processing.py' and '__init__.py':\n",
            "total 4\n",
            "-rw-r--r-- 1 root root 1534 Nov 13 19:24 data_processing.py\n",
            "-rw-r--r-- 1 root root    0 Nov 13 19:02 __init__.py\n"
          ]
        }
      ],
      "source": [
        "# Block 5: Create utils/data_processing.py and utils/__init__.py\n",
        "\n",
        "# Define the directory where helper functions will reside\n",
        "utils_dir = 'utils'\n",
        "\n",
        "# Create the 'utils' directory if it doesn't exist\n",
        "os.makedirs(utils_dir, exist_ok=True)\n",
        "\n",
        "# Define the path for the __init__.py file to make 'utils' a package\n",
        "init_path = os.path.join(utils_dir, '__init__.py')\n",
        "\n",
        "# Create an empty __init__.py file if it doesn't exist\n",
        "if not os.path.exists(init_path):\n",
        "    with open(init_path, 'w') as file:\n",
        "        pass  # Creating an empty __init__.py\n",
        "    print(f\"Created empty '__init__.py' at '{init_path}' to make 'utils' a package.\")\n",
        "else:\n",
        "    print(f\"'__init__.py' already exists at '{init_path}'.\")\n",
        "\n",
        "# Define the path for the data_processing.py file\n",
        "data_processing_path = os.path.join(utils_dir, 'data_processing.py')\n",
        "\n",
        "# Define the content for data_processing.py\n",
        "data_processing_code = \"\"\"\n",
        "# data_processing.py\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ne_chunk, pos_tag\n",
        "from nltk.tree import Tree\n",
        "\n",
        "def clean_text(text):\n",
        "    '''\n",
        "    Cleans the input text by removing URLs, special characters, and stopwords.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to clean.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned text.\n",
        "    '''\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\\\S+', '', text)\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^A-Za-z\\\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    # Join the words back into a single string\n",
        "    cleaned_text = ' '.join(filtered_words)\n",
        "    return cleaned_text\n",
        "\n",
        "def extract_entities(text):\n",
        "    '''\n",
        "    Extracts named entities from the input text.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to extract entities from.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of named entities.\n",
        "    '''\n",
        "    def get_entities(tree):\n",
        "        entities = []\n",
        "        for subtree in tree:\n",
        "            if isinstance(subtree, Tree):\n",
        "                entity = \" \".join([token for token, pos in subtree.leaves()])\n",
        "                entities.append(entity)\n",
        "        return entities\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = pos_tag(tokens)\n",
        "    chunked = ne_chunk(tagged)\n",
        "    entities = get_entities(chunked)\n",
        "    return entities\n",
        "\"\"\"\n",
        "\n",
        "# Write the data_processing.py file\n",
        "with open(data_processing_path, 'w') as file:\n",
        "    file.write(data_processing_code)\n",
        "\n",
        "print(f\"Created '{data_processing_path}' successfully.\")\n",
        "\n",
        "# Optional: Verify the creation by listing the 'utils' directory\n",
        "print(\"\\nVerifying the creation of 'data_processing.py' and '__init__.py':\")\n",
        "!ls -l utils/\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(f\"Current Working Directory: {os.getcwd()}\")\n"
      ],
      "metadata": {
        "id": "vLVzZkCSYp0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46392021-a6f9-4d50-f804-7f386d6aea18"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "C5RZdBozr4hJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a996d07-f599-49ca-9129-e76fb743e930"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the country for trending topics data (e.g., United States or US):\n",
            "Country: us\n",
            "\n",
            "You selected: United States\n",
            "\n",
            "Select the time frame for trending topics:\n",
            "A. Last 4 hours\n",
            "B. Last 24 hours\n",
            "C. Last 7 days\n",
            "Enter the letter of the time frame you're interested in: a\n",
            "                                                                                               \n",
            "\n",
            "Currently Trending in United States in the last 4 hours (Showing 1 to 10 of 10):\n",
            "\n",
            "+-----+-----------------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "| No. |         Topic         |               Description                |    Source   |   Approx Traffic   | Sentiment |\n",
            "+-----+-----------------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  1  |       John Thune      |  You didn't provide any text related to  | Google News | 1.6923076923076923 |  Positive |\n",
            "|     |                       |   \"Thune.\" Please provide the text you   |             |                    |           |\n",
            "|     |                       |             want summarized.             |             |                    |           |\n",
            "+-----+-----------------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  2  |        Pangolin       |  The text provided does not contain any  | Google News | 9.958579881656805  |  Positive |\n",
            "|     |                       |   specific information about Pangolins   |             |                    |           |\n",
            "|     |                       |  that can be summarized. Please provide  |             |                    |           |\n",
            "|     |                       |              more details.               |             |                    |           |\n",
            "+-----+-----------------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  3  |         Annex         | As an AI, I need the text to summarize.  | Google News | 16.633136094674555 |  Positive |\n",
            "|     |                       |  You've only provided the word \"Annex\"   |             |                    |           |\n",
            "|     |                       | and \"Summary:\". Please provide the text  |             |                    |           |\n",
            "|     |                       |           you want summarized.           |             |                    |           |\n",
            "+-----+-----------------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  4  |      Dave Coulier     |  The text provided does not contain any  | Google News | 4.260355029585799  |  Neutral  |\n",
            "|     |                       |    information or details about Dave     |             |                    |           |\n",
            "|     |                       |          Coulier to summarize.           |             |                    |           |\n",
            "+-----+-----------------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  5  |       Wednesday       |    The text provided does not contain    | Google News | 37.34319526627219  |  Positive |\n",
            "|     |                       | enough information to create a summary.  |             |                    |           |\n",
            "+-----+-----------------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  6  |       Cava stock      |  The text provided does not contain any  | Google News | 7.828402366863905  |  Positive |\n",
            "|     |                       | information to summarize. Please provide |             |                    |           |\n",
            "|     |                       |             a detailed text.             |             |                    |           |\n",
            "+-----+-----------------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  7  |    Whoopi Goldberg    | Whoopi Goldberg is an acclaimed American | Google News | 20.72189349112426  |  Positive |\n",
            "|     |                       |      actress, comedian, author, and      |             |                    |           |\n",
            "|     |                       |    television host. She has received     |             |                    |           |\n",
            "|     |                       | numerous awards for her work, including  |             |                    |           |\n",
            "|     |                       |   an Academy Award, a Grammy Award, an   |             |                    |           |\n",
            "|     |                       | Emmy Award, and a Tony Award, making her |             |                    |           |\n",
            "|     |                       | one of the few entertainers to have won  |             |                    |           |\n",
            "|     |                       |  all four major American entertainment   |             |                    |           |\n",
            "|     |                       |    awards. She is also known for her     |             |                    |           |\n",
            "|     |                       |   humanitarian work and her political    |             |                    |           |\n",
            "|     |                       |                activism.                 |             |                    |           |\n",
            "+-----+-----------------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  8  | India vs South Africa |  The text provided does not contain any  | Google News | 6.183431952662722  |  Neutral  |\n",
            "|     |                       | information or context about a specific  |             |                    |           |\n",
            "|     |                       | event, match, or topic related to India  |             |                    |           |\n",
            "|     |                       |  vs South Africa. Therefore, a summary   |             |                    |           |\n",
            "|     |                       |           cannot be produced.            |             |                    |           |\n",
            "+-----+-----------------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  9  |      Pete Hegseth     |     The text refers to Pete Hegseth      | Google News | 2.662721893491124  |  Positive |\n",
            "|     |                       |    potentially being the Secretary of    |             |                    |           |\n",
            "|     |                       |  Defense under Trump's administration.   |             |                    |           |\n",
            "+-----+-----------------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  10 |        Warriors       |  This text discusses a basketball game   | Google News | 11.497041420118343 |  Neutral  |\n",
            "|     |                       |  between the Golden State Warriors and   |             |                    |           |\n",
            "|     |                       |  the Dallas Mavericks, highlighting key  |             |                    |           |\n",
            "|     |                       |    players such as Klay Thompson and     |             |                    |           |\n",
            "|     |                       |     Stephen Curry from the Warriors.     |             |                    |           |\n",
            "+-----+-----------------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "Type 'more' or '+' to view more results, enter the number of the topic to select it, or any other key to exit: 1\n",
            "\n",
            "You selected: John Thune\n",
            "\n",
            "You can customize the script generation. If you wish to skip any option, just press Enter.\n",
            "Select script style (Flashy / Expressive / Normal): detailed\n",
            "Select script tone (e.g., Informative, Persuasive, Emotional): funny\n",
            "Enter script length in seconds (e.g., 60, 120): 120\n",
            "\n",
            "Selected Options - Style: detailed, Tone: funny, Length: 120 seconds\n",
            "\n",
            "Generating script for the selected topic...\n",
            "Script generated successfully.\n",
            "\n",
            "### Generated Script for 'John Thune':\n",
            "\n",
            "[Opening Scene]\n",
            "\n",
            "(Animated version of John Thune pops up on the screen)\n",
            "\n",
            "Narrator (Voiceover): \"Ladies and gentlemen, boys and girls, and all the political aficionados out there, put your hands together for the one, the only, Senator John Thune! \n",
            "\n",
            "(Thune takes a bow, grinning widely)\n",
            "\n",
            "Narrator (Voiceover): \"Now, you might be wondering, who's this sharply dressed, silver-haired gentleman? Well, let me enlighten you. This is South Dakota's pride and joy, the man who's been representing the Mount Rushmore State in the Senate since 2005!\"\n",
            "\n",
            "(Show Thune in a cowboy hat, standing in front of Mount Rushmore)\n",
            "\n",
            "Narrator (Voiceover): \"And folks, we've got a traffic update for you - the Thune train is currently cruising at a comfortable 1.6923076923076923 mph. Yes, you heard that right, we're dealing with some pretty precise metrics here!\"\n",
            "\n",
            "(Show a train with Thune's face on it, chugging along at a leisurely pace)\n",
            "\n",
            "Narrator (Voiceover): \"But wait, there's more! Did you know that our man Thune is a real-life Jack-of-all-Trades? Before his political career, he worked in the private sector, served at the Small Business Administration, and even spent a few years as a railroad executive. You can say, he's got tracks running through his veins!\" \n",
            "\n",
            "(Thune pops up, wearing a conductor's hat and blowing a train whistle)\n",
            "\n",
            "Narrator (Voiceover): \"And let's not forget his positive vibes - it's like he's running on pure, unfiltered sunshine! He's always got a hearty laugh, a pat on the back, and a kind word for everyone he meets.\"\n",
            "\n",
            "(Show Thune spreading sunshine, literally, with a big smile on his face)\n",
            "\n",
            "Narrator (Voiceover): \"So, there you have it, folks, the grand tour of the Thune-iverse. Remember, when it comes to John Thune, there's always more than meets the eye.\"\n",
            "\n",
            "(Show Thune waving goodbye, then disappearing in a puff of smoke)\n",
            "\n",
            "Narrator (Voiceover): \"Until next time, keep the Thune-itude alive!\"\n",
            "\n",
            "[End Scene]\n",
            "\n",
            "And... cut! That's a wrap, folks!\n",
            "\n",
            "**Source:** Google News\n",
            "\n",
            "====\n",
            "\n",
            "Script generation completed.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import structlog\n",
        "from prettytable import PrettyTable, HRuleStyle  # Import HRuleStyle for hrules\n",
        "from cachetools import TTLCache, cached\n",
        "from fuzzywuzzy import process\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from rake_nltk import Rake\n",
        "import feedparser\n",
        "import time\n",
        "import re\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "import openai\n",
        "import openai.error  # Add this line\n",
        "import textwrap  # For wrapping text in the table\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from newsapi import NewsApiClient\n",
        "from bs4 import BeautifulSoup\n",
        "import difflib\n",
        "import nltk\n",
        "import threading\n",
        "import configparser  # Import configparser\n",
        "import random\n",
        "import requests\n",
        "from newspaper import Article\n",
        "from pytrends.request import TrendReq\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "import pandas as pd\n",
        "pd.set_option('future.no_silent_downcasting', True)\n",
        "from random import uniform\n",
        "import time\n",
        "\n",
        "# ----------------------------\n",
        "# Structured Logging with structlog\n",
        "# ----------------------------\n",
        "\n",
        "structlog.configure(\n",
        "    processors=[\n",
        "        structlog.processors.TimeStamper(fmt=\"iso\"),\n",
        "        structlog.processors.JSONRenderer()\n",
        "    ],\n",
        "    context_class=dict,\n",
        "    logger_factory=structlog.stdlib.LoggerFactory(),\n",
        "    wrapper_class=structlog.stdlib.BoundLogger,\n",
        "    cache_logger_on_first_use=True,\n",
        ")\n",
        "\n",
        "logger = structlog.get_logger()\n",
        "\n",
        "# ----------------------------\n",
        "# Read API Keys from config.ini\n",
        "# ----------------------------\n",
        "\n",
        "# Initialize ConfigParser\n",
        "config = configparser.ConfigParser()\n",
        "config.read('/content/drive/MyDrive/Secrets/config.ini')  # Ensure this path is correct\n",
        "\n",
        "# Handle missing API keys gracefully\n",
        "try:\n",
        "    openai_api_key = config.get('openai', 'api_key')\n",
        "    newsapi_api_key = config.get('newsapi', 'api_key')\n",
        "except (configparser.NoSectionError, configparser.NoOptionError) as e:\n",
        "    logger.error(f\"Error reading API keys from config.ini: {e}\", exc_info=True)\n",
        "    sys.exit(\"Failed to read API keys from config.ini. Please ensure the file exists and contains the necessary keys.\")\n",
        "\n",
        "# Set your OpenAI API key from config.ini\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "# Initialize NewsApiClient with your API key from config.ini\n",
        "newsapi = NewsApiClient(api_key=newsapi_api_key)\n",
        "\n",
        "# ----------------------------\n",
        "# Caching\n",
        "# ----------------------------\n",
        "\n",
        "# Define caching constants\n",
        "TREND_CACHE_TTL = 3600       # 1 hour in seconds\n",
        "OPENAI_CACHE_TTL = 86400     # 1 day in seconds\n",
        "\n",
        "# Initialize caches with defined TTLs\n",
        "trends_cache = TTLCache(maxsize=100, ttl=TREND_CACHE_TTL)\n",
        "openai_cache = TTLCache(maxsize=1000, ttl=OPENAI_CACHE_TTL)\n",
        "\n",
        "# ----------------------------\n",
        "# Rate Limiting for RSS Feeds\n",
        "# ----------------------------\n",
        "\n",
        "ONE_DAY = 86400               # Seconds in one day\n",
        "RSS_CALLS_PER_DAY = 100       # Maximum number of RSS feed calls per day\n",
        "\n",
        "@sleep_and_retry\n",
        "@limits(calls=RSS_CALLS_PER_DAY, period=ONE_DAY)\n",
        "def fetch_rss_feed_sync(rss_url):\n",
        "    # Placeholder for synchronous RSS feed fetching if needed\n",
        "    pass\n",
        "\n",
        "# ----------------------------\n",
        "# Initialize Sentiment Analyzer and Keyword Extractor\n",
        "# ----------------------------\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize Sentiment Analyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "rake = Rake()\n",
        "\n",
        "# ----------------------------\n",
        "# OpenAI API Key and Semaphore\n",
        "# ----------------------------\n",
        "\n",
        "# Define a semaphore to limit concurrent requests\n",
        "google_trends_semaphore = asyncio.Semaphore(2)  # Reduce concurrency to 2\n",
        "\n",
        "\n",
        "# ----------------------------\n",
        "# Helper Functions\n",
        "# ----------------------------\n",
        "\n",
        "# Available countries with multiple codes\n",
        "available_countries = {\n",
        "    'United States': ['US', 'USA'],\n",
        "    'Canada': ['CA'],\n",
        "    'United Kingdom': ['GB', 'UK'],\n",
        "    'Australia': ['AU'],\n",
        "    'India': ['IN'],\n",
        "    'Germany': ['DE'],\n",
        "    'France': ['FR'],\n",
        "    'Japan': ['JP'],\n",
        "    'Brazil': ['BR'],\n",
        "    'South Korea': ['KR'],\n",
        "    'Argentina': ['AR'],\n",
        "    'Mexico': ['MX'],\n",
        "    'Singapore': ['SG'],\n",
        "    'Spain': ['ES'],\n",
        "    'Italy': ['IT'],\n",
        "    'Netherlands': ['NL'],\n",
        "    'Poland': ['PL'],\n",
        "    'Sweden': ['SE'],\n",
        "    'Switzerland': ['CH'],\n",
        "    # Add more countries and their codes as needed\n",
        "}\n",
        "\n",
        "def get_matching_country(input_country, available_countries):\n",
        "    \"\"\"\n",
        "    Matches user input to available countries.\n",
        "\n",
        "    Parameters:\n",
        "        input_country (str): The country input by the user.\n",
        "        available_countries (dict): Dictionary mapping country names to lists of codes.\n",
        "\n",
        "    Returns:\n",
        "        str or None: The matched country name or None if no match.\n",
        "    \"\"\"\n",
        "    normalized_input = input_country.lower()\n",
        "    for country, codes in available_countries.items():\n",
        "        # Check if input matches the country name\n",
        "        if normalized_input == country.lower():\n",
        "            return country\n",
        "        # Check if input matches any of the country codes\n",
        "        if normalized_input in [code.lower() for code in codes]:\n",
        "            return country\n",
        "    # If no direct match, attempt fuzzy matching\n",
        "    country_names = list(available_countries.keys())\n",
        "    closest_matches = difflib.get_close_matches(input_country, country_names, n=1, cutoff=0.8)\n",
        "    if closest_matches:\n",
        "        return closest_matches[0]\n",
        "    return None\n",
        "\n",
        "def sanitize_topic(topic):\n",
        "    \"\"\"\n",
        "    Sanitizes the topic string by removing unwanted characters.\n",
        "\n",
        "    Parameters:\n",
        "        topic (str): The topic to sanitize.\n",
        "\n",
        "    Returns:\n",
        "        str: The sanitized topic.\n",
        "    \"\"\"\n",
        "    return re.sub(r'[^\\w\\s]', '', topic)\n",
        "\n",
        "def extract_source(url):\n",
        "    \"\"\"\n",
        "    Extracts the main domain name from the URL to identify the source.\n",
        "\n",
        "    Parameters:\n",
        "        url (str): The URL of the news article.\n",
        "\n",
        "    Returns:\n",
        "        str: The name of the source.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from urllib.parse import urlparse\n",
        "        domain = urlparse(url).netloc\n",
        "        domain = domain.lower()\n",
        "        domain_mapping = {\n",
        "            'cbsnews.com': 'CBS News',\n",
        "            'cnn.com': 'CNN',\n",
        "            'foxnews.com': 'Fox News',\n",
        "            'abcnews.go.com': 'ABC News',\n",
        "            'bbc.co.uk': 'BBC',\n",
        "            'google.com': 'Google News',\n",
        "            'news.google.com': 'Google News',\n",
        "            'reuters.com': 'Reuters',\n",
        "            'theguardian.com': 'The Guardian',\n",
        "            'nytimes.com': 'The New York Times',\n",
        "            'usatoday.com': 'USA Today',\n",
        "            'fortworthstar.com': 'Fort Worth Star-Telegram',\n",
        "            'wcnc.com': 'WCNC',\n",
        "            'apnews.com': 'AP News',\n",
        "            'floridatoday.com': 'Florida Today',\n",
        "            'msnbc.com': 'MSNBC News',\n",
        "            # Add more mappings as needed\n",
        "        }\n",
        "        # Handle subdomains (e.g., edition.cnn.com)\n",
        "        domain_parts = domain.split('.')\n",
        "        if len(domain_parts) > 2:\n",
        "            domain = '.'.join(domain_parts[-2:])\n",
        "        return domain_mapping.get(domain, domain.capitalize())\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting source from URL '{url}': {e}\", exc_info=True)\n",
        "        return \"Unknown Source\"\n",
        "\n",
        "def broaden_query(query):\n",
        "    \"\"\"\n",
        "    Broadens the query by adding synonyms or related terms.\n",
        "\n",
        "    Parameters:\n",
        "        query (str): The original query.\n",
        "\n",
        "    Returns:\n",
        "        str: The broadened query.\n",
        "    \"\"\"\n",
        "    # Placeholder for query broadening logic\n",
        "    return query\n",
        "\n",
        "@cached(trends_cache)\n",
        "def fetch_google_trends_cached(topic, timeframe='now 7-d', max_retries=5):\n",
        "    \"\"\"\n",
        "    Fetches Google Trends data for the given topic with retry logic.\n",
        "    \"\"\"\n",
        "    pytrends = TrendReq(hl='en-US', tz=360)\n",
        "    attempt = 0\n",
        "    wait_time = 2  # Initial wait time in seconds\n",
        "\n",
        "    while attempt < max_retries:\n",
        "        try:\n",
        "            pytrends.build_payload([topic], timeframe=timeframe)\n",
        "            interest_over_time = pytrends.interest_over_time()\n",
        "            if interest_over_time is not None and not interest_over_time.empty:\n",
        "                avg_interest = interest_over_time[topic].mean()\n",
        "                sentiment = 'Neutral'\n",
        "                return {'topic': topic, 'interest': avg_interest, 'sentiment': sentiment}\n",
        "            else:\n",
        "                logger.error(f\"No data returned for topic '{topic}' from Google Trends.\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            # Gracefully handle rate limits (429) and other errors\n",
        "            if \"429\" in str(e):\n",
        "                logger.warning(f\"Rate limit hit for topic '{topic}'. Retrying after a delay...\")\n",
        "            else:\n",
        "                logger.error(f\"Error fetching Google Trends data for '{topic}': {e}\", exc_info=True)\n",
        "\n",
        "            time.sleep(wait_time)\n",
        "            wait_time = min(wait_time * 2, 60)  # Exponential backoff\n",
        "            attempt += 1\n",
        "\n",
        "    logger.error(f\"Max retries exceeded for Google Trends data fetch for '{topic}'.\")\n",
        "    return None\n",
        "\n",
        "def fetch_google_trends(topic, timeframe='now 7-d', retries=3, backoff_factor=2):\n",
        "    \"\"\"\n",
        "    Fetches Google Trends data with retries and backoff.\n",
        "    \"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            # Initialize pytrends\n",
        "            from pytrends.request import TrendReq\n",
        "            pytrends = TrendReq(hl='en-US', tz=360)\n",
        "            # Prepare the payload for pytrends\n",
        "            pytrends.build_payload([topic], timeframe=timeframe)\n",
        "            interest_over_time = pytrends.interest_over_time()\n",
        "            if not interest_over_time.empty:\n",
        "                # Get the average interest\n",
        "                avg_interest = interest_over_time[topic].mean()\n",
        "                sentiment = analyze_sentiment(topic)\n",
        "                return {'topic': topic, 'interest': avg_interest, 'sentiment': sentiment}\n",
        "            else:\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching Google Trends data for '{topic}': {e}\", exc_info=True)\n",
        "            time.sleep(backoff_factor * (2 ** attempt))\n",
        "    return None\n",
        "\n",
        "async def generate_summary_async(content, max_retries=5):\n",
        "    \"\"\"\n",
        "    Generates a summary of the content using OpenAI's GPT with retry logic.\n",
        "\n",
        "    Parameters:\n",
        "        content (str): The content to summarize.\n",
        "        max_retries (int): Maximum number of retries upon failure.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated summary.\n",
        "    \"\"\"\n",
        "    attempt = 0\n",
        "    wait_time = 1  # Initial wait time in seconds\n",
        "\n",
        "    while attempt < max_retries:\n",
        "        try:\n",
        "            async with openai_semaphore:\n",
        "                response = await openai.ChatCompletion.acreate(\n",
        "                    model=\"gpt-4\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes text.\"},\n",
        "                        {\"role\": \"user\", \"content\": f\"Summarize the following text:\\n\\n{content}\\n\\nSummary:\"}\n",
        "                    ],\n",
        "                    max_tokens=150,\n",
        "                    temperature=0.5,\n",
        "                )\n",
        "            summary = response.choices[0].message['content'].strip()\n",
        "            return summary\n",
        "        except openai.error.RateLimitError as e:\n",
        "    # Handle rate limit error\n",
        "\n",
        "            logger.error({\n",
        "                \"event\": \"RateLimitError in generate_summary_async\",\n",
        "                \"error\": str(e),\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            # Extract wait time from the error message if available\n",
        "            match = re.search(r\"Please try again in ([\\d\\.]+)s\", str(e))\n",
        "            if match:\n",
        "                wait_time = float(match.group(1))\n",
        "            else:\n",
        "                wait_time = min(wait_time * 2, 60)  # Exponential backoff with a max wait time\n",
        "            logger.info(f\"Rate limit exceeded. Waiting for {wait_time} seconds before retrying...\")\n",
        "            await asyncio.sleep(wait_time)\n",
        "            attempt += 1\n",
        "        except Exception as e:\n",
        "            logger.error({\n",
        "                \"event\": \"Error generating summary_async\",\n",
        "                \"error\": str(e),\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            return content  # Return original content if other errors occur\n",
        "    logger.error(\"Max retries exceeded for generate_summary_async. Returning original content.\")\n",
        "    return content\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    \"\"\"\n",
        "    Analyzes the sentiment of the given text using NLTK's VADER.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to analyze.\n",
        "\n",
        "    Returns:\n",
        "        str: 'Positive', 'Neutral', or 'Negative'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not text:\n",
        "            return 'Neutral'\n",
        "        text = str(text)  # Ensure text is a string\n",
        "        scores = sid.polarity_scores(text)\n",
        "        compound_score = scores['compound']\n",
        "        if compound_score >= 0.05:\n",
        "            return 'Positive'\n",
        "        elif compound_score <= -0.05:\n",
        "            return 'Negative'\n",
        "        else:\n",
        "            return 'Neutral'\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in analyze_sentiment: {e}\", exc_info=True)\n",
        "        return 'Neutral'\n",
        "\n",
        "def map_topic_to_trends_query(topic_title):\n",
        "    \"\"\"\n",
        "    Maps a topic title to a Google Trends query.\n",
        "\n",
        "    Parameters:\n",
        "        topic_title (str): The title of the topic.\n",
        "\n",
        "    Returns:\n",
        "        str: The mapped query.\n",
        "    \"\"\"\n",
        "    # Placeholder for mapping logic\n",
        "    return topic_title\n",
        "\n",
        "def extract_keywords(topic_title):\n",
        "    \"\"\"\n",
        "    Extracts keywords from the topic title using RAKE.\n",
        "\n",
        "    Parameters:\n",
        "        topic_title (str): The title of the topic.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of extracted keywords.\n",
        "    \"\"\"\n",
        "    rake.extract_keywords_from_text(topic_title)\n",
        "    return rake.get_ranked_phrases()\n",
        "\n",
        "def fetch_newsapi_articles(topic, page_size=5):\n",
        "    \"\"\"\n",
        "    Fetches articles from NewsAPI for a given topic.\n",
        "\n",
        "    Parameters:\n",
        "        topic (str): The topic to search articles for.\n",
        "        page_size (int): Number of articles to fetch.\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: A list of articles.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use the initialized NewsApiClient\n",
        "        all_articles = newsapi.get_everything(q=topic,\n",
        "                                              language='en',\n",
        "                                              sort_by='relevancy',\n",
        "                                              page_size=page_size)\n",
        "        if all_articles.get('status') != 'ok':\n",
        "            raise Exception(f\"NewsAPI Error: {all_articles.get('message', 'Unknown error')}\")\n",
        "        articles = all_articles['articles']\n",
        "        return articles\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching articles from NewsAPI: {e}\", exc_info=True)\n",
        "        return []\n",
        "\n",
        "# ----------------------------\n",
        "# Trend Data Classes\n",
        "# ----------------------------\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional\n",
        "\n",
        "@dataclass\n",
        "class GoogleTrend:\n",
        "    topic: str\n",
        "    interest: float\n",
        "    sentiment: str\n",
        "\n",
        "@dataclass\n",
        "class Trend:\n",
        "    title: str\n",
        "    description: str\n",
        "    source: str\n",
        "    approx_traffic: str\n",
        "    sentiment: str\n",
        "    google_trend: Optional[GoogleTrend] = None\n",
        "\n",
        "@dataclass\n",
        "class ScriptOptions:\n",
        "    style: str = \"Normal\"\n",
        "    tone: str = \"Informative\"\n",
        "    length: str = \"60 seconds\"\n",
        "\n",
        "# ----------------------------\n",
        "# Fetch Trending Topics Function\n",
        "# ----------------------------\n",
        "\n",
        "\n",
        "async def fetch_full_article_content(url):\n",
        "    try:\n",
        "        article = Article(url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        return article.text\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching full article content from {url}: {e}\", exc_info=True)\n",
        "        return None\n",
        "\n",
        "async def fetch_trending_topics_rss_async(geo='US', limit=10):\n",
        "    \"\"\"\n",
        "    Asynchronously fetches trending topics from multiple RSS feeds.\n",
        "    \"\"\"\n",
        "    rss_feeds = [\n",
        "        f\"https://trends.google.com/trends/trendingsearches/daily/rss?geo={geo}\",\n",
        "        f\"https://news.google.com/rss?hl=en-{geo}&gl={geo}&ceid={geo}:en\",\n",
        "        # Additional RSS feeds\n",
        "        \"http://rss.cnn.com/rss/edition.rss\",\n",
        "        \"http://news.yahoo.com/rss/\",\n",
        "        \"https://www.theguardian.com/uk/rss\",\n",
        "        \"https://news.un.org/feed/subscribe/en/news/all/rss.xml\"\n",
        "        # Add more feeds as needed\n",
        "    ]\n",
        "\n",
        "    trending_topics = []  # Initialize the list to store trending topics\n",
        "\n",
        "    tasks = []\n",
        "    for rss_url in rss_feeds:\n",
        "        tasks.append(fetch_rss_feed_async(rss_url, limit))\n",
        "\n",
        "    results = await asyncio.gather(*tasks)\n",
        "\n",
        "    for rss_url, result in zip(rss_feeds, results):\n",
        "        if not result:\n",
        "            continue\n",
        "        for entry in result:\n",
        "            # Process each entry\n",
        "            title = entry.get('title', 'No Title')\n",
        "            link = entry.get('link', '')\n",
        "            summary = entry.get('summary') or entry.get('description') or entry.get('title') or 'No description available.'\n",
        "            source = extract_source(link)\n",
        "            approx_traffic = 'N/A'\n",
        "\n",
        "            trending_topics.append({\n",
        "                'title': title,\n",
        "                'description': summary,\n",
        "                'source': source,\n",
        "                'approx_traffic': approx_traffic,\n",
        "                'sentiment': 'Neutral',  # Placeholder; actual sentiment will be determined later\n",
        "                'published': datetime.now(timezone.utc)\n",
        "            })\n",
        "    return trending_topics  # Correctly indented outside the loop\n",
        "\n",
        "async def fetch_rss_feed_async(rss_url, limit):\n",
        "    \"\"\"\n",
        "    Asynchronously fetches and parses an RSS feed.\n",
        "\n",
        "    Parameters:\n",
        "        rss_url (str): The URL of the RSS feed.\n",
        "        limit (int): Number of entries to fetch.\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: A list of RSS feed entries.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
        "    }\n",
        "    try:\n",
        "        async with aiohttp.ClientSession(headers=headers) as session:\n",
        "            async with session.get(rss_url, timeout=10) as response:\n",
        "                if response.status != 200:\n",
        "                    logger.error(f\"Failed to fetch RSS feed from {rss_url}. Status code: {response.status}\")\n",
        "                    return []\n",
        "                content = await response.text()\n",
        "                feed = feedparser.parse(content)\n",
        "                entries = feed.entries[:limit]\n",
        "                return entries\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching RSS feed from {rss_url}: {e}\", exc_info=True)\n",
        "        return []\n",
        "\n",
        "# ----------------------------\n",
        "# Aggregate Trends Data Function\n",
        "# ----------------------------\n",
        "\n",
        "async def aggregate_trends_data(rss_trends, selected_timeframe_hours):\n",
        "    \"\"\"\n",
        "    Aggregates data for each trend by fetching NewsAPI articles and Google Trends data.\n",
        "    \"\"\"\n",
        "    aggregated_trends = []\n",
        "    now = datetime.now(timezone.utc)\n",
        "\n",
        "    for trend in rss_trends:\n",
        "        # Filter based on the selected time frame\n",
        "        time_diff = now - trend['published']\n",
        "        if time_diff.total_seconds() > selected_timeframe_hours * 3600:\n",
        "            continue  # Skip trends outside the time frame\n",
        "\n",
        "        topic_title = trend['title']\n",
        "        # Clean the description by stripping HTML\n",
        "        soup = BeautifulSoup(trend['description'], 'html.parser')\n",
        "        clean_description = soup.get_text(separator=' ', strip=True)\n",
        "        # Optionally, limit the description length\n",
        "        clean_description = (clean_description[:200] + '...') if len(clean_description) > 200 else clean_description\n",
        "\n",
        "        # Fetch NewsAPI articles (if needed)\n",
        "        # newsapi_articles = fetch_newsapi_articles(topic_title, page_size=5)\n",
        "        # Process articles as needed (e.g., summarize, analyze sentiment)\n",
        "\n",
        "        # Fetch Google Trends data\n",
        "        google_trend_data = fetch_google_trends_cached(topic_title, timeframe='now 7-d')\n",
        "        if google_trend_data:\n",
        "            google_trend = GoogleTrend(**google_trend_data)\n",
        "            approx_traffic = str(google_trend.interest)\n",
        "        else:\n",
        "            google_trend = None\n",
        "            approx_traffic = 'Unavailable'\n",
        "\n",
        "        # Generate summary asynchronously if description is available\n",
        "        if clean_description and clean_description != 'No description available.':\n",
        "            summary = await generate_summary_async(clean_description)\n",
        "        else:\n",
        "            summary = clean_description  # Use existing description\n",
        "\n",
        "        sentiment = analyze_sentiment(summary)\n",
        "\n",
        "        aggregated_trends.append(Trend(\n",
        "            title=topic_title,\n",
        "            description=summary,\n",
        "            source=trend['source'],\n",
        "            approx_traffic=approx_traffic,\n",
        "            sentiment=sentiment,\n",
        "            google_trend=google_trend\n",
        "        ))\n",
        "\n",
        "        if len(aggregated_trends) >= 10:\n",
        "            break  # Limit to 10 trends\n",
        "\n",
        "    return aggregated_trends\n",
        "\n",
        "# ----------------------------\n",
        "# Define fetch_and_aggregate_trending_data Function\n",
        "# ----------------------------\n",
        "\n",
        "async def fetch_and_aggregate_trending_data(country_code, timeframe_hours):\n",
        "    \"\"\"\n",
        "    Fetches trending topics and aggregates data.\n",
        "\n",
        "    Parameters:\n",
        "        country_code (str): The country code for fetching trends.\n",
        "        timeframe_hours (int): The timeframe in hours to filter trends.\n",
        "\n",
        "    Returns:\n",
        "        List[Trend]: A list of aggregated Trend objects.\n",
        "    \"\"\"\n",
        "    rss_trends = await fetch_trending_topics_rss_async(geo=country_code, limit=20)\n",
        "    aggregated_trends = await aggregate_trends_data(rss_trends, timeframe_hours)\n",
        "    return aggregated_trends\n",
        "\n",
        "# ----------------------------\n",
        "# Generate and Display Script Function\n",
        "# ----------------------------\n",
        "\n",
        "async def generate_and_display_script_async(selected_topic_data: Trend):\n",
        "    \"\"\"\n",
        "    Prompts the user for script customization options, generates the script,\n",
        "    and displays it.\n",
        "\n",
        "    Parameters:\n",
        "        selected_topic_data (Trend): The data associated with the selected topic.\n",
        "    \"\"\"\n",
        "    print(\"\\nYou can customize the script generation. If you wish to skip any option, just press Enter.\")\n",
        "\n",
        "    # Select script style\n",
        "    style_input = input(\"Select script style (Flashy / Expressive / Normal): \").strip()\n",
        "    style = style_input if style_input else \"Normal\"\n",
        "\n",
        "    # Select script tone\n",
        "    tone_input = input(\"Select script tone (e.g., Informative, Persuasive, Emotional): \").strip()\n",
        "    tone = tone_input if tone_input else \"Informative\"\n",
        "\n",
        "    # Enter script length\n",
        "    length_input = input(\"Enter script length in seconds (e.g., 60, 120): \").strip()\n",
        "    length = f\"{length_input} seconds\" if length_input.isdigit() else \"60 seconds\"\n",
        "\n",
        "    # Create ScriptOptions instance\n",
        "    options = ScriptOptions(\n",
        "        style=style,\n",
        "        tone=tone,\n",
        "        length=length\n",
        "    )\n",
        "\n",
        "    print(\"\\nSelected Options - Style: {0}, Tone: {1}, Length: {2}\".format(options.style, options.tone, options.length))\n",
        "\n",
        "    print(\"\\nGenerating script for the selected topic...\")\n",
        "    try:\n",
        "        script = await generate_script_for_topic_async(selected_topic_data.title, selected_topic_data, options=options)\n",
        "        print(\"Script generated successfully.\")\n",
        "    except Exception as e:\n",
        "        logger.error({\n",
        "            \"error\": str(e),\n",
        "            \"event\": \"generate_script_error\",\n",
        "            \"timestamp\": datetime.utcnow().isoformat()\n",
        "        })\n",
        "        script = \"Failed to generate script.\"\n",
        "\n",
        "    # Display the script\n",
        "    print(f\"\\n### Generated Script for '{selected_topic_data.title}':\\n\")\n",
        "    print(script)\n",
        "    print(\"\\n**Source:**\", selected_topic_data.source)\n",
        "    print(\"\\n====\\n\")\n",
        "\n",
        "    print(\"Script generation completed.\")\n",
        "\n",
        "async def generate_script_for_topic_async(title, trend_data, options: ScriptOptions):\n",
        "    \"\"\"\n",
        "    Generates a script based on the topic and options using OpenAI's GPT.\n",
        "\n",
        "    Parameters:\n",
        "        title (str): The title of the topic.\n",
        "        trend_data (Trend): The trend data.\n",
        "        options (ScriptOptions): User-selected script options.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated script.\n",
        "    \"\"\"\n",
        "    # Construct the prompt for OpenAI\n",
        "    prompt = (\n",
        "        f\"Generate a {options.length} script for the topic '{title}' with a {options.tone} tone \"\n",
        "        f\"and a {options.style} style. The script should include the following details:\\n\"\n",
        "        f\"- Description: {trend_data.description}\\n\"\n",
        "        f\"- Sentiment: {trend_data.sentiment}\\n\"\n",
        "        f\"- Approximate Traffic: {trend_data.approx_traffic}\\n\\n\"\n",
        "        f\"Please ensure the script is engaging and suitable for a {options.length} presentation.\"\n",
        "    )\n",
        "\n",
        "    attempt = 0\n",
        "    max_retries = 5\n",
        "    wait_time = 1  # Initial wait time in seconds\n",
        "\n",
        "    while attempt < max_retries:\n",
        "        try:\n",
        "            async with openai_semaphore:\n",
        "                response = await openai.ChatCompletion.acreate(\n",
        "                    model=\"gpt-4\",\n",
        "                    messages=[\n",
        "                        {\n",
        "                            \"role\": \"system\",\n",
        "                            \"content\": \"You are a creative scriptwriter that generates scripts based on provided data.\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": prompt\n",
        "                        }\n",
        "                    ],\n",
        "                    max_tokens=500,\n",
        "                    temperature=0.7,\n",
        "                )\n",
        "            script = response.choices[0].message['content'].strip()\n",
        "            return script\n",
        "        except openai.error.RateLimitError as e:\n",
        "    # Handle rate limit error\n",
        "\n",
        "            logger.error({\n",
        "                \"event\": \"RateLimitError in generate_script_for_topic_async\",\n",
        "                \"error\": str(e),\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            # Extract wait time from the error message if available\n",
        "            match = re.search(r\"Please try again in ([\\d\\.]+)s\", str(e))\n",
        "            if match:\n",
        "                wait_time = float(match.group(1))\n",
        "            else:\n",
        "                wait_time = min(wait_time * 2, 60)  # Exponential backoff with a max wait time\n",
        "            logger.info(f\"Rate limit exceeded. Waiting for {wait_time} seconds before retrying...\")\n",
        "            await asyncio.sleep(wait_time)\n",
        "            attempt += 1\n",
        "        except Exception as e:\n",
        "            logger.error({\n",
        "                \"event\": \"Error generating script_async\",\n",
        "                \"error\": str(e),\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            return \"Failed to generate script due to an unexpected error.\"\n",
        "\n",
        "    logger.error(\"Max retries exceeded for generate_script_for_topic_async. Returning failure message.\")\n",
        "    return \"Failed to generate script after multiple attempts due to rate limits.\"\n",
        "\n",
        "# ----------------------------\n",
        "# Main Function\n",
        "# ----------------------------\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to execute the script workflow.\n",
        "    \"\"\"\n",
        "    async def run():\n",
        "        # Function to display a changing message every 5 seconds with an additional note\n",
        "        def flashing_message(stop_event):\n",
        "            messages = [\n",
        "                \"🔍 Gathering the latest trends... (This could take up to 2 minutes. Please wait.)\",\n",
        "                \"⏳ Processing data, please wait... (This could take up to 2 minutes. Please wait.)\",\n",
        "                \"✨ Almost there, thank you for your patience! (This could take up to 2 minutes. Please wait.)\"\n",
        "            ]\n",
        "            idx = 0\n",
        "            while not stop_event.is_set():\n",
        "                message = messages[idx % len(messages)]\n",
        "                print(f\"\\r{message}   \", end='', flush=True)\n",
        "                for _ in range(5):\n",
        "                    if stop_event.is_set():\n",
        "                        break\n",
        "                    time.sleep(1)\n",
        "                idx += 1\n",
        "                print('\\r' + ' ' * len(message) + '   ', end='', flush=True)\n",
        "\n",
        "        while True:\n",
        "            print(\"Enter the country for trending topics data (e.g., United States or US):\")\n",
        "            country_input = input(\"Country: \").strip()\n",
        "            matching_country = get_matching_country(country_input, available_countries)\n",
        "            if not matching_country:\n",
        "                print(\"No matching countries found. Please try again.\")\n",
        "                # Optionally, display available countries\n",
        "                print(\"Available countries are:\")\n",
        "                for country in available_countries.keys():\n",
        "                    print(f\"- {country}\")\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        selected_country = matching_country\n",
        "        # Retrieve all possible codes for the selected country\n",
        "        selected_country_codes = available_countries[selected_country]\n",
        "        selected_country_code = selected_country_codes[0]  # Use the first code by default\n",
        "        print(f\"\\nYou selected: {selected_country}\")\n",
        "\n",
        "        print(\"\\nSelect the time frame for trending topics:\")\n",
        "        print(\"A. Last 4 hours\")\n",
        "        print(\"B. Last 24 hours\")\n",
        "        print(\"C. Last 7 days\")\n",
        "        time_range_selection = input(\"Enter the letter of the time frame you're interested in: \").strip().upper()\n",
        "        time_range_mapping = {'A': 4, 'B': 24, 'C': 168}\n",
        "        selected_timeframe_hours = time_range_mapping.get(time_range_selection)\n",
        "        if not selected_timeframe_hours:\n",
        "            print(\"Invalid selection. Exiting.\")\n",
        "            return\n",
        "\n",
        "        # Start flashing message while fetching all data\n",
        "        stop_event = threading.Event()\n",
        "        thread = threading.Thread(target=flashing_message, args=(stop_event,))\n",
        "        thread.start()\n",
        "\n",
        "        # Fetch all data asynchronously with robust error handling\n",
        "        try:\n",
        "            # Run the async fetch_and_aggregate_trending_data function\n",
        "            aggregated_trends = await fetch_and_aggregate_trending_data(selected_country_code, selected_timeframe_hours)\n",
        "        except Exception as e:\n",
        "            logger.error({\n",
        "                \"error\": str(e),\n",
        "                \"event\": \"fetch_data_error\",\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            aggregated_trends = []\n",
        "\n",
        "        # Stop flashing message\n",
        "        stop_event.set()\n",
        "        thread.join()\n",
        "        print()  # Move to the next line after flashing message\n",
        "\n",
        "        if not aggregated_trends:\n",
        "            logger.error({\n",
        "                \"event\": \"no_trending_topics_found\",\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            print(\"No trending topics found.\")\n",
        "            return\n",
        "\n",
        "        # Display trending topics with pagination\n",
        "        # Pagination variables\n",
        "        batch_size = 10\n",
        "        total_trends = len(aggregated_trends)\n",
        "        current_index = 0\n",
        "\n",
        "        while current_index < total_trends:\n",
        "            # Determine the end index for the current batch\n",
        "            end_index = min(current_index + batch_size, total_trends)\n",
        "            batch_trends = aggregated_trends[current_index:end_index]\n",
        "\n",
        "            # Display the consolidated list with sentiments\n",
        "            print(f\"\\nCurrently Trending in {selected_country} in the last {selected_timeframe_hours} hours (Showing {current_index + 1} to {end_index} of {total_trends}):\\n\")\n",
        "            table = PrettyTable()\n",
        "            table.field_names = [\"No.\", \"Topic\", \"Description\", \"Source\", \"Approx Traffic\", \"Sentiment\"]\n",
        "            table.hrules = HRuleStyle.ALL  # Use HRuleStyle.ALL\n",
        "            table.max_width = 40  # Suitable for phone screens\n",
        "            for idx, trend in enumerate(batch_trends, start=current_index + 1):\n",
        "                title = textwrap.fill(trend.title, width=40)\n",
        "                description = textwrap.fill(trend.description, width=40)\n",
        "                source = trend.source\n",
        "                approx_traffic = trend.approx_traffic\n",
        "                sentiment = trend.sentiment\n",
        "                table.add_row([idx, title, description, source, approx_traffic, sentiment])\n",
        "            print(table)\n",
        "\n",
        "            # Prepare to handle user input for pagination or selection\n",
        "            while True:\n",
        "                user_input = input(\"Type 'more' or '+' to view more results, enter the number of the topic to select it, or any other key to exit: \").strip().lower()\n",
        "                if user_input in ['more', '+']:\n",
        "                    current_index = end_index\n",
        "                    break  # Continue to the next batch\n",
        "                elif user_input.isdigit():\n",
        "                    selected_idx = int(user_input)\n",
        "                    if 1 <= selected_idx <= total_trends:\n",
        "                        selected_topic_data = aggregated_trends[selected_idx - 1]\n",
        "                        selected_topic = selected_topic_data.title\n",
        "                        # Apply text wrapping to the selected topic message\n",
        "                        selected_topic_display = textwrap.fill(f\"You selected: {selected_topic}\", width=40)\n",
        "                        print(f\"\\n{selected_topic_display}\")\n",
        "                        # Generate scripts for the selected topic\n",
        "                        await generate_and_display_script_async(selected_topic_data)\n",
        "                        return  # Exit after script generation\n",
        "                    else:\n",
        "                        print(\"Invalid selection. Please enter a valid topic number.\")\n",
        "                else:\n",
        "                    print(\"Exiting the script.\")\n",
        "                    return\n",
        "\n",
        "        print(\"====\\nNo more trending topics available.\\n\")\n",
        "\n",
        "        # After all batches are displayed, prompt the user to select a topic\n",
        "        while True:\n",
        "            try:\n",
        "                selected_idx = input(\"Enter the number of the topic you're interested in (or type 0 to exit): \").strip()\n",
        "                if selected_idx == '0':\n",
        "                    print(\"Exiting the script.\")\n",
        "                    return\n",
        "                if selected_idx.isdigit():\n",
        "                    selected_idx = int(selected_idx)\n",
        "                    if 1 <= selected_idx <= total_trends:\n",
        "                        selected_topic_data = aggregated_trends[selected_idx - 1]\n",
        "                        selected_topic = selected_topic_data.title\n",
        "                        # Apply text wrapping to the selected topic message\n",
        "                        selected_topic_display = textwrap.fill(f\"You selected: {selected_topic}\", width=40)\n",
        "                        print(f\"\\n{selected_topic_display}\")\n",
        "                        # Generate scripts for the selected topic\n",
        "                        await generate_and_display_script_async(selected_topic_data)\n",
        "                        return  # Exit after script generation\n",
        "                    else:\n",
        "                        print(\"Invalid selection. Please enter a valid topic number.\")\n",
        "                else:\n",
        "                    print(\"Invalid input. Please enter a number.\")\n",
        "            except ValueError:\n",
        "                print(\"Invalid input. Please enter a valid number.\")\n",
        "\n",
        "    asyncio.run(run())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1s-3u0czUndpxCkgirEnqndEDGpEPjpJ_",
      "authorship_tag": "ABX9TyMD1+4uJgNCr5JyxbYaODcg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}