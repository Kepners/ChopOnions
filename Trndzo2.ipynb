{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kepners/ChopOnions/blob/main/Trndzo2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 1: Install Required Packages\n",
        "\n",
        "!pip install --upgrade openai==0.28.1 python-dotenv praw rake_nltk newsapi-python feedparser aiohttp nest_asyncio structlog beautifulsoup4 cachetools fuzzywuzzy python-Levenshtein nltk pytrends ratelimit prettytable bs4 configparser structlog\n",
        "print(\"pip install completed\")\n",
        "import nest_asyncio\n",
        "# Apply nest_asyncio to allow nested event loops\n",
        "nest_asyncio.apply()\n",
        "\n",
        "import nltk\n",
        "nltk.download('vader_lexicon')\n",
        "print(\"vader_lexicon downloaded\")\n",
        "nltk.download('stopwords')\n",
        "print(\"stopwords downloaded\")\n",
        "nltk.download('punkt')\n",
        "print(\"punkt downloaded\")\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "print(\"averaged_perceptron_tagger downloaded\")\n",
        "nltk.download('maxent_ne_chunker')\n",
        "print(\"maxent_ne_chunker downloaded\")\n",
        "nltk.download('words')\n",
        "print(\"words downloaded\")\n",
        "nltk.download('wordnet')\n",
        "print(\"wordnet downloaded\")\n",
        "nltk.download('punkt_tab')\n",
        "print('punkt_tab downloaded')\n",
        "\n",
        "\n",
        "print(\"Installation and NLTK data download completed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGNrZ1kNMSXk",
        "outputId": "f667329c-0116-4353-8b14-d76c7ef49aa1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28.1 in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: praw in /usr/local/lib/python3.10/dist-packages (7.8.1)\n",
            "Requirement already satisfied: rake_nltk in /usr/local/lib/python3.10/dist-packages (1.0.6)\n",
            "Requirement already satisfied: newsapi-python in /usr/local/lib/python3.10/dist-packages (0.2.7)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.10/dist-packages (6.0.11)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (3.10.10)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: structlog in /usr/local/lib/python3.10/dist-packages (24.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (5.5.0)\n",
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.10/dist-packages (0.26.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pytrends in /usr/local/lib/python3.10/dist-packages (4.9.2)\n",
            "Requirement already satisfied: ratelimit in /usr/local/lib/python3.10/dist-packages (2.2.1)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (3.12.0)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: configparser in /usr/local/lib/python3.10/dist-packages (7.1.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (4.66.6)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in /usr/local/lib/python3.10/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.8.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser) (1.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: Levenshtein==0.26.1 in /usr/local/lib/python3.10/dist-packages (from python-Levenshtein) (0.26.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein==0.26.1->python-Levenshtein) (3.10.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.10/dist-packages (from pytrends) (2.2.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from pytrends) (5.3.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable) (0.2.13)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25->pytrends) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2024.8.30)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp) (0.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25->pytrends) (1.16.0)\n",
            "pip install completed\n",
            "vader_lexicon downloaded\n",
            "stopwords downloaded\n",
            "punkt downloaded\n",
            "averaged_perceptron_tagger downloaded\n",
            "maxent_ne_chunker downloaded\n",
            "words downloaded\n",
            "wordnet downloaded\n",
            "punkt_tab downloaded\n",
            "Installation and NLTK data download completed successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 2: Mount Google Drive and Load Configuration\n",
        "from google.colab import drive\n",
        "import configparser\n",
        "import os\n",
        "import openai  # import the openai module\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to your config.ini in Google Drive\n",
        "config_path = '/content/drive/MyDrive/Secrets/config.ini'  # Update this path as needed\n",
        "\n",
        "# Check if config.ini exists at the specified path\n",
        "if not os.path.exists(config_path):\n",
        "    raise FileNotFoundError(f\"config.ini not found at {config_path}\")\n",
        "\n",
        "# Load configuration using configparser\n",
        "config = configparser.ConfigParser()\n",
        "config.read(config_path)\n",
        "\n",
        "# Retrieve OpenAI API key\n",
        "try:\n",
        "    openai_api_key = config.get('openai', 'api_key', fallback=None)\n",
        "except KeyError:\n",
        "    raise ValueError(\"OpenAI API key not found or invalid format in config.ini under [openai] section.\")\n",
        "\n",
        "if not openai_api_key:\n",
        "    raise ValueError(\"OpenAI API key not found or invalid format in config.ini under [openai] section.\")\n",
        "\n",
        "# Set the OpenAI API key\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "# Retrieve NewsAPI key\n",
        "try:\n",
        "    newsapi_key = config.get('newsapi', 'api_key')\n",
        "except KeyError:\n",
        "    raise ValueError(\"NewsAPI key not found in config.ini under [newsapi] section.\")\n",
        "\n",
        "# Retrieve caching configurations\n",
        "rss_cache_ttl = config.getint('CACHING', 'RSS_CACHE_TTL', fallback=86400)\n",
        "trend_cache_ttl = config.getint('CACHING', 'TREND_CACHE_TTL', fallback=3600)\n",
        "openai_cache_ttl = config.getint('CACHING', 'OPENAI_CACHE_TTL', fallback=86400)\n",
        "\n",
        "# Retrieve rate limiting configurations\n",
        "rss_calls_per_day = config.getint('RATE_LIMITING', 'RSS_CALLS_PER_DAY', fallback=1000)\n",
        "\n",
        "print(\"Configuration loaded successfully.\")\n"
      ],
      "metadata": {
        "id": "BNlyvpVs_QsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbd223dc-dac4-41d4-8b08-5236b5eb8d04"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Configuration loaded successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Block 3: Define Data Structures\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional\n",
        "\n",
        "@dataclass\n",
        "class GoogleTrend:\n",
        "    topic: str\n",
        "    interest: str\n",
        "    sentiment: str\n",
        "\n",
        "@dataclass\n",
        "class Trend:\n",
        "    title: str\n",
        "    description: str\n",
        "    source: str\n",
        "    approx_traffic: str\n",
        "    sentiment: str\n",
        "    google_trend: Optional[GoogleTrend] = None\n",
        "\n",
        "@dataclass\n",
        "class ScriptOptions:\n",
        "    style: str = \"Normal Script\"\n",
        "    tone: str = \"Informative\"\n",
        "    length: str = \"60 seconds\"\n",
        "\n",
        "print(\"Data structures defined successfully.\")\n"
      ],
      "metadata": {
        "id": "4wkdmt7yp0mR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ae4dd63-ed7f-4c0f-8422-3d7e5bf14149"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data structures defined successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gIud78FOZR3M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "240ac6b9-08ea-4d00-eb8d-49f891536ca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration and initialization completed successfully.\n"
          ]
        }
      ],
      "source": [
        "# Block 4: Configuration and Initialization\n",
        "\n",
        "import configparser\n",
        "from pytrends.request import TrendReq # This import will now work\n",
        "from newsapi import NewsApiClient\n",
        "\n",
        "# Initialize OpenAI\n",
        "api_key = openai_api_key\n",
        "\n",
        "# Initialize NewsAPI\n",
        "newsapi = NewsApiClient(api_key=newsapi_key)\n",
        "\n",
        "# Initialize PyTrends with custom requests arguments\n",
        "pytrends = TrendReq(hl='en-US', tz=360, requests_args={'headers': {'User-Agent': 'Mozilla/5.0'}})\n",
        "\n",
        "print(\"Configuration and initialization completed successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wGPK1IRBXfBm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a38c0c9c-8f60-476c-9a58-6dce9e00122f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'__init__.py' already exists at 'utils/__init__.py'.\n",
            "Created 'utils/data_processing.py' successfully.\n",
            "\n",
            "Verifying the creation of 'data_processing.py' and '__init__.py':\n",
            "total 4\n",
            "-rw-r--r-- 1 root root 1534 Nov 13 11:33 data_processing.py\n",
            "-rw-r--r-- 1 root root    0 Nov 13 11:09 __init__.py\n"
          ]
        }
      ],
      "source": [
        "# Block 5: Create utils/data_processing.py and utils/__init__.py\n",
        "\n",
        "# Define the directory where helper functions will reside\n",
        "utils_dir = 'utils'\n",
        "\n",
        "# Create the 'utils' directory if it doesn't exist\n",
        "os.makedirs(utils_dir, exist_ok=True)\n",
        "\n",
        "# Define the path for the __init__.py file to make 'utils' a package\n",
        "init_path = os.path.join(utils_dir, '__init__.py')\n",
        "\n",
        "# Create an empty __init__.py file if it doesn't exist\n",
        "if not os.path.exists(init_path):\n",
        "    with open(init_path, 'w') as file:\n",
        "        pass  # Creating an empty __init__.py\n",
        "    print(f\"Created empty '__init__.py' at '{init_path}' to make 'utils' a package.\")\n",
        "else:\n",
        "    print(f\"'__init__.py' already exists at '{init_path}'.\")\n",
        "\n",
        "# Define the path for the data_processing.py file\n",
        "data_processing_path = os.path.join(utils_dir, 'data_processing.py')\n",
        "\n",
        "# Define the content for data_processing.py\n",
        "data_processing_code = \"\"\"\n",
        "# data_processing.py\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import ne_chunk, pos_tag\n",
        "from nltk.tree import Tree\n",
        "\n",
        "def clean_text(text):\n",
        "    '''\n",
        "    Cleans the input text by removing URLs, special characters, and stopwords.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to clean.\n",
        "\n",
        "    Returns:\n",
        "        str: The cleaned text.\n",
        "    '''\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\\\S+', '', text)\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^A-Za-z\\\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    # Join the words back into a single string\n",
        "    cleaned_text = ' '.join(filtered_words)\n",
        "    return cleaned_text\n",
        "\n",
        "def extract_entities(text):\n",
        "    '''\n",
        "    Extracts named entities from the input text.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to extract entities from.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of named entities.\n",
        "    '''\n",
        "    def get_entities(tree):\n",
        "        entities = []\n",
        "        for subtree in tree:\n",
        "            if isinstance(subtree, Tree):\n",
        "                entity = \" \".join([token for token, pos in subtree.leaves()])\n",
        "                entities.append(entity)\n",
        "        return entities\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged = pos_tag(tokens)\n",
        "    chunked = ne_chunk(tagged)\n",
        "    entities = get_entities(chunked)\n",
        "    return entities\n",
        "\"\"\"\n",
        "\n",
        "# Write the data_processing.py file\n",
        "with open(data_processing_path, 'w') as file:\n",
        "    file.write(data_processing_code)\n",
        "\n",
        "print(f\"Created '{data_processing_path}' successfully.\")\n",
        "\n",
        "# Optional: Verify the creation by listing the 'utils' directory\n",
        "print(\"\\nVerifying the creation of 'data_processing.py' and '__init__.py':\")\n",
        "!ls -l utils/\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(f\"Current Working Directory: {os.getcwd()}\")\n"
      ],
      "metadata": {
        "id": "vLVzZkCSYp0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22d28f1e-34a9-4d78-a252-e97ead9f007b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Working Directory: /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "C5RZdBozr4hJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97379317-e345-48cd-996e-d05fddf4e3c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the country for trending topics data (e.g., United States or US):\n",
            "Country: us\n",
            "\n",
            "You selected: United States\n",
            "\n",
            "Select the time frame for trending topics:\n",
            "A. Last 4 hours\n",
            "B. Last 24 hours\n",
            "C. Last 7 days\n",
            "Enter the letter of the time frame you're interested in: b\n",
            "🔍 Gathering the latest trends... (This could take up to 2 minutes. Please wait.)   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏳ Processing data, please wait... (This could take up to 2 minutes. Please wait.)   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n",
            "/usr/local/lib/python3.10/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n",
            "/usr/local/lib/python3.10/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✨ Almost there, thank you for your patience! (This could take up to 2 minutes. Please wait.)   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:{\"exc_info\": true, \"event\": \"Error fetching Google Trends data for 'John Ratcliffe': The request failed: Google returned a response with code 429\", \"timestamp\": \"2024-11-13T11:33:36.532351Z\"}\n",
            "/usr/local/lib/python3.10/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n",
            "/usr/local/lib/python3.10/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Gathering the latest trends... (This could take up to 2 minutes. Please wait.)   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:__main__:{\"exc_info\": true, \"event\": \"Error fetching Google Trends data for 'Skai Jackson': The request failed: Google returned a response with code 429\", \"timestamp\": \"2024-11-13T11:33:40.591434Z\"}\n",
            "/usr/local/lib/python3.10/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n",
            "/usr/local/lib/python3.10/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n",
            "/usr/local/lib/python3.10/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏳ Processing data, please wait... (This could take up to 2 minutes. Please wait.)   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pytrends/request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df = df.fillna(False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r                                                                                    \n",
            "\n",
            "Currently Trending in United States in the last 24 hours (Showing 1 to 10 of 10):\n",
            "\n",
            "+-----+------------------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "| No. |         Topic          |               Description                |    Source   |   Approx Traffic   | Sentiment |\n",
            "+-----+------------------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  1  |        Warriors        |   The text mentions a game between the   | Google News |  10.8698224852071  |  Positive |\n",
            "|     |                        |  Dallas Mavericks and the Warriors. Key  |             |                    |           |\n",
            "|     |                        |  players like Klay Thompson and Stephen  |             |                    |           |\n",
            "|     |                        | Curry, who are part of the Warriors, are |             |                    |           |\n",
            "|     |                        |               highlighted.               |             |                    |           |\n",
            "+-----+------------------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  2  |      Kristi Noem       |  The text provided does not contain any  | Google News | 7.615384615384615  |  Neutral  |\n",
            "|     |                        | details or context about Kristi Noem to  |             |                    |           |\n",
            "|     |                        |                summarize.                |             |                    |           |\n",
            "+-----+------------------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  3  |    Duke basketball     |  The text mentions Kentucky basketball   | Google News | 5.970414201183432  |  Neutral  |\n",
            "|     |                        |   and a person named Cooper Flagg, but   |             |                    |           |\n",
            "|     |                        |     does not provide any additional      |             |                    |           |\n",
            "|     |                        |    information or context about them.    |             |                    |           |\n",
            "+-----+------------------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  4  |      Kyle Singler      |  The text provided does not contain any  | Google News | 4.272189349112426  |  Neutral  |\n",
            "|     |                        |   information to summarize as it only    |             |                    |           |\n",
            "|     |                        |    mentions the name \"Kyle Singler\".     |             |                    |           |\n",
            "+-----+------------------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  5  |     John Ratcliffe     |  The text provided does not contain any  | Google News | 2.1301775147928996 |  Positive |\n",
            "|     |                        | information to summarize. Please provide |             |                    |           |\n",
            "|     |                        |  a detailed text about John Ratcliffe.   |             |                    |           |\n",
            "+-----+------------------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  6  |        Celtics         |  The text provided does not contain any  | Google News | 11.171597633136095 |  Positive |\n",
            "|     |                        |  specific information or context about   |             |                    |           |\n",
            "|     |                        |   the Celtics that can be summarized.    |             |                    |           |\n",
            "|     |                        |    Please provide more detailed text.    |             |                    |           |\n",
            "+-----+------------------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  7  |      Skai Jackson      |  Skai Jackson is a prominent figure in   | Google News | 6.165680473372781  |  Positive |\n",
            "|     |                        |  the entertainment industry, known for   |             |                    |           |\n",
            "|     |                        |            her acting skills.            |             |                    |           |\n",
            "+-----+------------------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  8  |     Mike Huckabee      |  The text provided does not contain any  | Google News | 4.6982248520710055 |  Positive |\n",
            "|     |                        |    information or context about Mike     |             |                    |           |\n",
            "|     |                        | Huckabee to summarize. Please provide a  |             |                    |           |\n",
            "|     |                        |              detailed text.              |             |                    |           |\n",
            "+-----+------------------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  9  | Dancing with the Stars |  The text refers to the 2024 edition of  | Google News | 4.0828402366863905 |  Neutral  |\n",
            "|     |                        |  the television show \"Dancing with the   |             |                    |           |\n",
            "|     |                        |                 Stars.\"                  |             |                    |           |\n",
            "+-----+------------------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "|  10 |        NBA Cup         |    The text provided does not contain    | Google News | 6.3017751479289945 |  Positive |\n",
            "|     |                        | enough information to create a summary.  |             |                    |           |\n",
            "|     |                        |   Please provide a more detailed text.   |             |                    |           |\n",
            "+-----+------------------------+------------------------------------------+-------------+--------------------+-----------+\n",
            "Type 'more' or '+' to view more results, enter the number of the topic to select it, or any other key to exit: 10\n",
            "\n",
            "You selected: NBA Cup\n",
            "\n",
            "You can customize the script generation. If you wish to skip any option, just press Enter.\n",
            "Select script style (Flashy / Expressive / Normal): informative\n",
            "Select script tone (e.g., Informative, Persuasive, Emotional): funny\n",
            "Enter script length in seconds (e.g., 60, 120): 34\n",
            "\n",
            "Selected Options - Style: informative, Tone: funny, Length: 34 seconds\n",
            "\n",
            "Generating script for the selected topic...\n",
            "Script generated successfully.\n",
            "\n",
            "### Generated Script for 'NBA Cup':\n",
            "\n",
            "(Open with energetic, upbeat music)\n",
            "\n",
            "Narrator: (Excitedly) Welcome, hoop fanatics! We're talking about the 'NBA Cup' today! \n",
            "\n",
            "(Record scratch sound)\n",
            "\n",
            "Narrator: (Cheerfully) Now, you might be thinking, \"What? NBA Cup? Isn't it called the NBA Championship?\" \n",
            "\n",
            "(Laugh track)\n",
            "\n",
            "Narrator: (Chuckling) Yes, you are absolutely right, my friend! But, let's just call it NBA Cup for today, shall we? A little humor never hurt anyone!\n",
            "\n",
            "(Funny horn sound)\n",
            "\n",
            "Narrator: (Animatedly) This event is the pinnacle of professional basketball, where teams dribble, dunk, and dish out assists for the ultimate glory.\n",
            "\n",
            "(Basketball swishing sound)\n",
            "\n",
            "Narrator: (Lightly) And hey, with an approximate traffic of 6.3, it's safe to say that it's more popular than your favorite cat video on the internet! \n",
            "\n",
            "(Laugh track)\n",
            "\n",
            "Narrator: (Concluding) So, whether you're a fan of the Lakers, the Celtics, or any other team, remember, the NBA Cup - oh, I mean Championship, is where legends are made!\n",
            "\n",
            "(Ending on a high note)\n",
            "\n",
            "Narrator: (Laughing) Catch you on the court, folks!\n",
            "\n",
            "(End with upbeat music)\n",
            "\n",
            "**Source:** Google News\n",
            "\n",
            "====\n",
            "\n",
            "Script generation completed.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import asyncio\n",
        "import aiohttp\n",
        "import structlog\n",
        "from prettytable import PrettyTable, HRuleStyle  # Import HRuleStyle for hrules\n",
        "from cachetools import TTLCache, cached\n",
        "from fuzzywuzzy import process\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from rake_nltk import Rake\n",
        "import feedparser\n",
        "import time\n",
        "import re\n",
        "from ratelimit import limits, sleep_and_retry\n",
        "import openai\n",
        "import openai.error  # Add this line\n",
        "import textwrap  # For wrapping text in the table\n",
        "from datetime import datetime, timedelta, timezone\n",
        "from newsapi import NewsApiClient\n",
        "from bs4 import BeautifulSoup\n",
        "import difflib\n",
        "import nltk\n",
        "import threading\n",
        "import configparser  # Import configparser\n",
        "\n",
        "# ----------------------------\n",
        "# Structured Logging with structlog\n",
        "# ----------------------------\n",
        "\n",
        "structlog.configure(\n",
        "    processors=[\n",
        "        structlog.processors.TimeStamper(fmt=\"iso\"),\n",
        "        structlog.processors.JSONRenderer()\n",
        "    ],\n",
        "    context_class=dict,\n",
        "    logger_factory=structlog.stdlib.LoggerFactory(),\n",
        "    wrapper_class=structlog.stdlib.BoundLogger,\n",
        "    cache_logger_on_first_use=True,\n",
        ")\n",
        "\n",
        "logger = structlog.get_logger()\n",
        "\n",
        "# ----------------------------\n",
        "# Read API Keys from config.ini\n",
        "# ----------------------------\n",
        "\n",
        "# Initialize ConfigParser\n",
        "config = configparser.ConfigParser()\n",
        "config.read('/content/drive/MyDrive/Secrets/config.ini')  # Ensure this path is correct\n",
        "\n",
        "# Handle missing API keys gracefully\n",
        "try:\n",
        "    openai_api_key = config.get('openai', 'api_key')\n",
        "    newsapi_api_key = config.get('newsapi', 'api_key')\n",
        "except (configparser.NoSectionError, configparser.NoOptionError) as e:\n",
        "    logger.error(f\"Error reading API keys from config.ini: {e}\", exc_info=True)\n",
        "    sys.exit(\"Failed to read API keys from config.ini. Please ensure the file exists and contains the necessary keys.\")\n",
        "\n",
        "# Set your OpenAI API key from config.ini\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "# Initialize NewsApiClient with your API key from config.ini\n",
        "newsapi = NewsApiClient(api_key=newsapi_api_key)\n",
        "\n",
        "# ----------------------------\n",
        "# Caching\n",
        "# ----------------------------\n",
        "\n",
        "# Define caching constants\n",
        "TREND_CACHE_TTL = 3600       # 1 hour in seconds\n",
        "OPENAI_CACHE_TTL = 86400     # 1 day in seconds\n",
        "\n",
        "# Initialize caches with defined TTLs\n",
        "trends_cache = TTLCache(maxsize=100, ttl=TREND_CACHE_TTL)\n",
        "openai_cache = TTLCache(maxsize=1000, ttl=OPENAI_CACHE_TTL)\n",
        "\n",
        "# ----------------------------\n",
        "# Rate Limiting for RSS Feeds\n",
        "# ----------------------------\n",
        "\n",
        "ONE_DAY = 86400               # Seconds in one day\n",
        "RSS_CALLS_PER_DAY = 100       # Maximum number of RSS feed calls per day\n",
        "\n",
        "@sleep_and_retry\n",
        "@limits(calls=RSS_CALLS_PER_DAY, period=ONE_DAY)\n",
        "def fetch_rss_feed_sync(rss_url):\n",
        "    # Placeholder for synchronous RSS feed fetching if needed\n",
        "    pass\n",
        "\n",
        "# ----------------------------\n",
        "# Initialize Sentiment Analyzer and Keyword Extractor\n",
        "# ----------------------------\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Initialize Sentiment Analyzer\n",
        "sid = SentimentIntensityAnalyzer()\n",
        "rake = Rake()\n",
        "\n",
        "# ----------------------------\n",
        "# OpenAI API Key and Semaphore\n",
        "# ----------------------------\n",
        "\n",
        "# Define a semaphore to limit concurrent OpenAI API calls\n",
        "openai_semaphore = asyncio.Semaphore(5)  # Adjust the number as needed\n",
        "\n",
        "# ----------------------------\n",
        "# Helper Functions\n",
        "# ----------------------------\n",
        "\n",
        "# Available countries with multiple codes\n",
        "available_countries = {\n",
        "    'United States': ['US', 'USA'],\n",
        "    'Canada': ['CA'],\n",
        "    'United Kingdom': ['GB', 'UK'],\n",
        "    'Australia': ['AU'],\n",
        "    'India': ['IN'],\n",
        "    'Germany': ['DE'],\n",
        "    'France': ['FR'],\n",
        "    'Japan': ['JP'],\n",
        "    'Brazil': ['BR'],\n",
        "    'South Korea': ['KR'],\n",
        "    'Argentina': ['AR'],\n",
        "    'Mexico': ['MX'],\n",
        "    'Singapore': ['SG'],\n",
        "    'Spain': ['ES'],\n",
        "    'Italy': ['IT'],\n",
        "    'Netherlands': ['NL'],\n",
        "    'Poland': ['PL'],\n",
        "    'Sweden': ['SE'],\n",
        "    'Switzerland': ['CH'],\n",
        "    # Add more countries and their codes as needed\n",
        "}\n",
        "\n",
        "def get_matching_country(input_country, available_countries):\n",
        "    \"\"\"\n",
        "    Matches user input to available countries.\n",
        "\n",
        "    Parameters:\n",
        "        input_country (str): The country input by the user.\n",
        "        available_countries (dict): Dictionary mapping country names to lists of codes.\n",
        "\n",
        "    Returns:\n",
        "        str or None: The matched country name or None if no match.\n",
        "    \"\"\"\n",
        "    normalized_input = input_country.lower()\n",
        "    for country, codes in available_countries.items():\n",
        "        # Check if input matches the country name\n",
        "        if normalized_input == country.lower():\n",
        "            return country\n",
        "        # Check if input matches any of the country codes\n",
        "        if normalized_input in [code.lower() for code in codes]:\n",
        "            return country\n",
        "    # If no direct match, attempt fuzzy matching\n",
        "    country_names = list(available_countries.keys())\n",
        "    closest_matches = difflib.get_close_matches(input_country, country_names, n=1, cutoff=0.8)\n",
        "    if closest_matches:\n",
        "        return closest_matches[0]\n",
        "    return None\n",
        "\n",
        "def sanitize_topic(topic):\n",
        "    \"\"\"\n",
        "    Sanitizes the topic string by removing unwanted characters.\n",
        "\n",
        "    Parameters:\n",
        "        topic (str): The topic to sanitize.\n",
        "\n",
        "    Returns:\n",
        "        str: The sanitized topic.\n",
        "    \"\"\"\n",
        "    return re.sub(r'[^\\w\\s]', '', topic)\n",
        "\n",
        "def extract_source(url):\n",
        "    \"\"\"\n",
        "    Extracts the main domain name from the URL to identify the source.\n",
        "\n",
        "    Parameters:\n",
        "        url (str): The URL of the news article.\n",
        "\n",
        "    Returns:\n",
        "        str: The name of the source.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from urllib.parse import urlparse\n",
        "        domain = urlparse(url).netloc\n",
        "        domain = domain.lower()\n",
        "        domain_mapping = {\n",
        "            'cbsnews.com': 'CBS News',\n",
        "            'cnn.com': 'CNN',\n",
        "            'foxnews.com': 'Fox News',\n",
        "            'abcnews.go.com': 'ABC News',\n",
        "            'bbc.co.uk': 'BBC',\n",
        "            'google.com': 'Google News',\n",
        "            'news.google.com': 'Google News',\n",
        "            'reuters.com': 'Reuters',\n",
        "            'theguardian.com': 'The Guardian',\n",
        "            'nytimes.com': 'The New York Times',\n",
        "            'usatoday.com': 'USA Today',\n",
        "            'fortworthstar.com': 'Fort Worth Star-Telegram',\n",
        "            'wcnc.com': 'WCNC',\n",
        "            'apnews.com': 'AP News',\n",
        "            'floridatoday.com': 'Florida Today',\n",
        "            'msnbc.com': 'MSNBC News',\n",
        "            # Add more mappings as needed\n",
        "        }\n",
        "        # Handle subdomains (e.g., edition.cnn.com)\n",
        "        domain_parts = domain.split('.')\n",
        "        if len(domain_parts) > 2:\n",
        "            domain = '.'.join(domain_parts[-2:])\n",
        "        return domain_mapping.get(domain, domain.capitalize())\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error extracting source from URL '{url}': {e}\", exc_info=True)\n",
        "        return \"Unknown Source\"\n",
        "\n",
        "def broaden_query(query):\n",
        "    \"\"\"\n",
        "    Broadens the query by adding synonyms or related terms.\n",
        "\n",
        "    Parameters:\n",
        "        query (str): The original query.\n",
        "\n",
        "    Returns:\n",
        "        str: The broadened query.\n",
        "    \"\"\"\n",
        "    # Placeholder for query broadening logic\n",
        "    return query\n",
        "\n",
        "@cached(trends_cache)\n",
        "def fetch_google_trends_cached(topic, timeframe='now 7-d', max_retries=5):\n",
        "    \"\"\"\n",
        "    Fetches Google Trends data for the given topic with retry logic.\n",
        "\n",
        "    Parameters:\n",
        "        topic (str): The topic to search for.\n",
        "        timeframe (str): The time frame for the trends data.\n",
        "        max_retries (int): Maximum number of retries upon failure.\n",
        "\n",
        "    Returns:\n",
        "        dict: Google Trends data including interest and sentiment.\n",
        "    \"\"\"\n",
        "    attempt = 0\n",
        "    wait_time = 1  # Initial wait time in seconds\n",
        "\n",
        "    while attempt < max_retries:\n",
        "        try:\n",
        "            # Initialize pytrends\n",
        "            from pytrends.request import TrendReq\n",
        "            pytrends = TrendReq(hl='en-US', tz=360)\n",
        "            # Prepare the payload for pytrends\n",
        "            pytrends.build_payload([topic], timeframe=timeframe)\n",
        "            interest_over_time = pytrends.interest_over_time()\n",
        "            if not interest_over_time.empty:\n",
        "                # Get the average interest\n",
        "                avg_interest = interest_over_time[topic].mean()\n",
        "                sentiment = 'Neutral'  # Placeholder; actual sentiment will be determined later\n",
        "                return {'topic': topic, 'interest': avg_interest, 'sentiment': sentiment}\n",
        "            else:\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            error_message = str(e)\n",
        "            logger.error(f\"Error fetching Google Trends data for '{topic}': {e}\", exc_info=True)\n",
        "            if '429' in error_message or 'Too Many Requests' in error_message:\n",
        "                # Handle 429 error with exponential backoff\n",
        "                time.sleep(wait_time)\n",
        "                wait_time = min(wait_time * 2, 60)  # Exponential backoff with max wait time\n",
        "                attempt += 1\n",
        "                continue\n",
        "            else:\n",
        "                # For other exceptions, do not retry\n",
        "                return None\n",
        "    logger.error(f\"Max retries exceeded for Google Trends data fetch for '{topic}'.\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def fetch_google_trends(topic, timeframe='now 7-d', retries=3, backoff_factor=2):\n",
        "    \"\"\"\n",
        "    Fetches Google Trends data with retries and backoff.\n",
        "    \"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            # Initialize pytrends\n",
        "            from pytrends.request import TrendReq\n",
        "            pytrends = TrendReq(hl='en-US', tz=360)\n",
        "            # Prepare the payload for pytrends\n",
        "            pytrends.build_payload([topic], timeframe=timeframe)\n",
        "            interest_over_time = pytrends.interest_over_time()\n",
        "            if not interest_over_time.empty:\n",
        "                # Get the average interest\n",
        "                avg_interest = interest_over_time[topic].mean()\n",
        "                sentiment = analyze_sentiment(topic)\n",
        "                return {'topic': topic, 'interest': avg_interest, 'sentiment': sentiment}\n",
        "            else:\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error fetching Google Trends data for '{topic}': {e}\", exc_info=True)\n",
        "            time.sleep(backoff_factor * (2 ** attempt))\n",
        "    return None\n",
        "\n",
        "async def generate_summary_async(content, max_retries=5):\n",
        "    \"\"\"\n",
        "    Generates a summary of the content using OpenAI's GPT with retry logic.\n",
        "\n",
        "    Parameters:\n",
        "        content (str): The content to summarize.\n",
        "        max_retries (int): Maximum number of retries upon failure.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated summary.\n",
        "    \"\"\"\n",
        "    attempt = 0\n",
        "    wait_time = 1  # Initial wait time in seconds\n",
        "\n",
        "    while attempt < max_retries:\n",
        "        try:\n",
        "            async with openai_semaphore:\n",
        "                response = await openai.ChatCompletion.acreate(\n",
        "                    model=\"gpt-4\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": \"You are a helpful assistant that summarizes text.\"},\n",
        "                        {\"role\": \"user\", \"content\": f\"Summarize the following text:\\n\\n{content}\\n\\nSummary:\"}\n",
        "                    ],\n",
        "                    max_tokens=150,\n",
        "                    temperature=0.5,\n",
        "                )\n",
        "            summary = response.choices[0].message['content'].strip()\n",
        "            return summary\n",
        "        except openai.error.RateLimitError as e:\n",
        "    # Handle rate limit error\n",
        "\n",
        "            logger.error({\n",
        "                \"event\": \"RateLimitError in generate_summary_async\",\n",
        "                \"error\": str(e),\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            # Extract wait time from the error message if available\n",
        "            match = re.search(r\"Please try again in ([\\d\\.]+)s\", str(e))\n",
        "            if match:\n",
        "                wait_time = float(match.group(1))\n",
        "            else:\n",
        "                wait_time = min(wait_time * 2, 60)  # Exponential backoff with a max wait time\n",
        "            logger.info(f\"Rate limit exceeded. Waiting for {wait_time} seconds before retrying...\")\n",
        "            await asyncio.sleep(wait_time)\n",
        "            attempt += 1\n",
        "        except Exception as e:\n",
        "            logger.error({\n",
        "                \"event\": \"Error generating summary_async\",\n",
        "                \"error\": str(e),\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            return content  # Return original content if other errors occur\n",
        "    logger.error(\"Max retries exceeded for generate_summary_async. Returning original content.\")\n",
        "    return content\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    \"\"\"\n",
        "    Analyzes the sentiment of the given text using NLTK's VADER.\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to analyze.\n",
        "\n",
        "    Returns:\n",
        "        str: 'Positive', 'Neutral', or 'Negative'\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not text:\n",
        "            return 'Neutral'\n",
        "        text = str(text)  # Ensure text is a string\n",
        "        scores = sid.polarity_scores(text)\n",
        "        compound_score = scores['compound']\n",
        "        if compound_score >= 0.05:\n",
        "            return 'Positive'\n",
        "        elif compound_score <= -0.05:\n",
        "            return 'Negative'\n",
        "        else:\n",
        "            return 'Neutral'\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error in analyze_sentiment: {e}\", exc_info=True)\n",
        "        return 'Neutral'\n",
        "\n",
        "def map_topic_to_trends_query(topic_title):\n",
        "    \"\"\"\n",
        "    Maps a topic title to a Google Trends query.\n",
        "\n",
        "    Parameters:\n",
        "        topic_title (str): The title of the topic.\n",
        "\n",
        "    Returns:\n",
        "        str: The mapped query.\n",
        "    \"\"\"\n",
        "    # Placeholder for mapping logic\n",
        "    return topic_title\n",
        "\n",
        "def extract_keywords(topic_title):\n",
        "    \"\"\"\n",
        "    Extracts keywords from the topic title using RAKE.\n",
        "\n",
        "    Parameters:\n",
        "        topic_title (str): The title of the topic.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: A list of extracted keywords.\n",
        "    \"\"\"\n",
        "    rake.extract_keywords_from_text(topic_title)\n",
        "    return rake.get_ranked_phrases()\n",
        "\n",
        "def fetch_newsapi_articles(topic, page_size=5):\n",
        "    \"\"\"\n",
        "    Fetches articles from NewsAPI for a given topic.\n",
        "\n",
        "    Parameters:\n",
        "        topic (str): The topic to search articles for.\n",
        "        page_size (int): Number of articles to fetch.\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: A list of articles.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Use the initialized NewsApiClient\n",
        "        all_articles = newsapi.get_everything(q=topic,\n",
        "                                              language='en',\n",
        "                                              sort_by='relevancy',\n",
        "                                              page_size=page_size)\n",
        "        if all_articles.get('status') != 'ok':\n",
        "            raise Exception(f\"NewsAPI Error: {all_articles.get('message', 'Unknown error')}\")\n",
        "        articles = all_articles['articles']\n",
        "        return articles\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching articles from NewsAPI: {e}\", exc_info=True)\n",
        "        return []\n",
        "\n",
        "# ----------------------------\n",
        "# Trend Data Classes\n",
        "# ----------------------------\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Optional\n",
        "\n",
        "@dataclass\n",
        "class GoogleTrend:\n",
        "    topic: str\n",
        "    interest: float\n",
        "    sentiment: str\n",
        "\n",
        "@dataclass\n",
        "class Trend:\n",
        "    title: str\n",
        "    description: str\n",
        "    source: str\n",
        "    approx_traffic: str\n",
        "    sentiment: str\n",
        "    google_trend: Optional[GoogleTrend] = None\n",
        "\n",
        "@dataclass\n",
        "class ScriptOptions:\n",
        "    style: str = \"Normal\"\n",
        "    tone: str = \"Informative\"\n",
        "    length: str = \"60 seconds\"\n",
        "\n",
        "# ----------------------------\n",
        "# Fetch Trending Topics Function\n",
        "# ----------------------------\n",
        "\n",
        "async def fetch_trending_topics_rss_async(geo='US', limit=10):\n",
        "    \"\"\"\n",
        "    Asynchronously fetches trending topics from multiple RSS feeds.\n",
        "    \"\"\"\n",
        "    rss_feeds = [\n",
        "        f\"https://trends.google.com/trends/trendingsearches/daily/rss?geo={geo}\",\n",
        "        f\"https://news.google.com/rss?hl=en-{geo}&gl={geo}&ceid={geo}:en\",\n",
        "        # Additional RSS feeds\n",
        "        \"http://rss.cnn.com/rss/edition.rss\",\n",
        "        \"http://news.yahoo.com/rss/\",\n",
        "        \"https://www.theguardian.com/uk/rss\",\n",
        "        \"https://news.un.org/feed/subscribe/en/news/all/rss.xml\"\n",
        "        # Add more feeds as needed\n",
        "    ]\n",
        "\n",
        "    trending_topics = []  # Initialize the list to store trending topics\n",
        "\n",
        "    tasks = []\n",
        "    for rss_url in rss_feeds:\n",
        "        tasks.append(fetch_rss_feed_async(rss_url, limit))\n",
        "\n",
        "    results = await asyncio.gather(*tasks)\n",
        "\n",
        "    for rss_url, result in zip(rss_feeds, results):\n",
        "        if not result:\n",
        "            continue\n",
        "        for entry in result:\n",
        "            # Process each entry\n",
        "            title = entry.get('title', 'No Title')\n",
        "            link = entry.get('link', '')\n",
        "            summary = entry.get('summary') or entry.get('description') or entry.get('title') or 'No description available.'\n",
        "            source = extract_source(link)\n",
        "            approx_traffic = 'N/A'\n",
        "\n",
        "            trending_topics.append({\n",
        "                'title': title,\n",
        "                'description': summary,\n",
        "                'source': source,\n",
        "                'approx_traffic': approx_traffic,\n",
        "                'sentiment': 'Neutral',  # Placeholder; actual sentiment will be determined later\n",
        "                'published': datetime.now(timezone.utc)\n",
        "            })\n",
        "    return trending_topics  # Correctly indented outside the loop\n",
        "\n",
        "async def fetch_rss_feed_async(rss_url, limit):\n",
        "    \"\"\"\n",
        "    Asynchronously fetches and parses an RSS feed.\n",
        "\n",
        "    Parameters:\n",
        "        rss_url (str): The URL of the RSS feed.\n",
        "        limit (int): Number of entries to fetch.\n",
        "\n",
        "    Returns:\n",
        "        List[dict]: A list of RSS feed entries.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'\n",
        "    }\n",
        "    try:\n",
        "        async with aiohttp.ClientSession(headers=headers) as session:\n",
        "            async with session.get(rss_url, timeout=10) as response:\n",
        "                if response.status != 200:\n",
        "                    logger.error(f\"Failed to fetch RSS feed from {rss_url}. Status code: {response.status}\")\n",
        "                    return []\n",
        "                content = await response.text()\n",
        "                feed = feedparser.parse(content)\n",
        "                entries = feed.entries[:limit]\n",
        "                return entries\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error fetching RSS feed from {rss_url}: {e}\", exc_info=True)\n",
        "        return []\n",
        "\n",
        "# ----------------------------\n",
        "# Aggregate Trends Data Function\n",
        "# ----------------------------\n",
        "\n",
        "async def aggregate_trends_data(rss_trends, selected_timeframe_hours):\n",
        "    \"\"\"\n",
        "    Aggregates data for each trend by fetching NewsAPI articles and Google Trends data.\n",
        "    \"\"\"\n",
        "    aggregated_trends = []\n",
        "    now = datetime.now(timezone.utc)\n",
        "\n",
        "    for trend in rss_trends:\n",
        "        # Filter based on the selected time frame\n",
        "        time_diff = now - trend['published']\n",
        "        if time_diff.total_seconds() > selected_timeframe_hours * 3600:\n",
        "            continue  # Skip trends outside the time frame\n",
        "\n",
        "        topic_title = trend['title']\n",
        "        # Clean the description by stripping HTML\n",
        "        soup = BeautifulSoup(trend['description'], 'html.parser')\n",
        "        clean_description = soup.get_text(separator=' ', strip=True)\n",
        "        # Optionally, limit the description length\n",
        "        clean_description = (clean_description[:200] + '...') if len(clean_description) > 200 else clean_description\n",
        "\n",
        "        # Fetch NewsAPI articles (if needed)\n",
        "        # newsapi_articles = fetch_newsapi_articles(topic_title, page_size=5)\n",
        "        # Process articles as needed (e.g., summarize, analyze sentiment)\n",
        "\n",
        "        # Fetch Google Trends data\n",
        "        google_trend_data = fetch_google_trends_cached(topic_title, timeframe='now 7-d')\n",
        "        if google_trend_data:\n",
        "            google_trend = GoogleTrend(**google_trend_data)\n",
        "            approx_traffic = str(google_trend.interest)\n",
        "        else:\n",
        "            google_trend = None\n",
        "            approx_traffic = 'Unknown'\n",
        "\n",
        "        # Generate summary asynchronously if description is available\n",
        "        if clean_description and clean_description != 'No description available.':\n",
        "            summary = await generate_summary_async(clean_description)\n",
        "        else:\n",
        "            summary = clean_description  # Use existing description\n",
        "\n",
        "        sentiment = analyze_sentiment(summary)\n",
        "\n",
        "        aggregated_trends.append(Trend(\n",
        "            title=topic_title,\n",
        "            description=summary,\n",
        "            source=trend['source'],\n",
        "            approx_traffic=approx_traffic,\n",
        "            sentiment=sentiment,\n",
        "            google_trend=google_trend\n",
        "        ))\n",
        "\n",
        "        if len(aggregated_trends) >= 10:\n",
        "            break  # Limit to 10 trends\n",
        "\n",
        "    return aggregated_trends\n",
        "\n",
        "# ----------------------------\n",
        "# Define fetch_and_aggregate_trending_data Function\n",
        "# ----------------------------\n",
        "\n",
        "async def fetch_and_aggregate_trending_data(country_code, timeframe_hours):\n",
        "    \"\"\"\n",
        "    Fetches trending topics and aggregates data.\n",
        "\n",
        "    Parameters:\n",
        "        country_code (str): The country code for fetching trends.\n",
        "        timeframe_hours (int): The timeframe in hours to filter trends.\n",
        "\n",
        "    Returns:\n",
        "        List[Trend]: A list of aggregated Trend objects.\n",
        "    \"\"\"\n",
        "    rss_trends = await fetch_trending_topics_rss_async(geo=country_code, limit=20)\n",
        "    aggregated_trends = await aggregate_trends_data(rss_trends, timeframe_hours)\n",
        "    return aggregated_trends\n",
        "\n",
        "# ----------------------------\n",
        "# Generate and Display Script Function\n",
        "# ----------------------------\n",
        "\n",
        "async def generate_and_display_script_async(selected_topic_data: Trend):\n",
        "    \"\"\"\n",
        "    Prompts the user for script customization options, generates the script,\n",
        "    and displays it.\n",
        "\n",
        "    Parameters:\n",
        "        selected_topic_data (Trend): The data associated with the selected topic.\n",
        "    \"\"\"\n",
        "    print(\"\\nYou can customize the script generation. If you wish to skip any option, just press Enter.\")\n",
        "\n",
        "    # Select script style\n",
        "    style_input = input(\"Select script style (Flashy / Expressive / Normal): \").strip()\n",
        "    style = style_input if style_input else \"Normal\"\n",
        "\n",
        "    # Select script tone\n",
        "    tone_input = input(\"Select script tone (e.g., Informative, Persuasive, Emotional): \").strip()\n",
        "    tone = tone_input if tone_input else \"Informative\"\n",
        "\n",
        "    # Enter script length\n",
        "    length_input = input(\"Enter script length in seconds (e.g., 60, 120): \").strip()\n",
        "    length = f\"{length_input} seconds\" if length_input.isdigit() else \"60 seconds\"\n",
        "\n",
        "    # Create ScriptOptions instance\n",
        "    options = ScriptOptions(\n",
        "        style=style,\n",
        "        tone=tone,\n",
        "        length=length\n",
        "    )\n",
        "\n",
        "    print(\"\\nSelected Options - Style: {0}, Tone: {1}, Length: {2}\".format(options.style, options.tone, options.length))\n",
        "\n",
        "    print(\"\\nGenerating script for the selected topic...\")\n",
        "    try:\n",
        "        script = await generate_script_for_topic_async(selected_topic_data.title, selected_topic_data, options=options)\n",
        "        print(\"Script generated successfully.\")\n",
        "    except Exception as e:\n",
        "        logger.error({\n",
        "            \"error\": str(e),\n",
        "            \"event\": \"generate_script_error\",\n",
        "            \"timestamp\": datetime.utcnow().isoformat()\n",
        "        })\n",
        "        script = \"Failed to generate script.\"\n",
        "\n",
        "    # Display the script\n",
        "    print(f\"\\n### Generated Script for '{selected_topic_data.title}':\\n\")\n",
        "    print(script)\n",
        "    print(\"\\n**Source:**\", selected_topic_data.source)\n",
        "    print(\"\\n====\\n\")\n",
        "\n",
        "    print(\"Script generation completed.\")\n",
        "\n",
        "async def generate_script_for_topic_async(title, trend_data, options: ScriptOptions):\n",
        "    \"\"\"\n",
        "    Generates a script based on the topic and options using OpenAI's GPT.\n",
        "\n",
        "    Parameters:\n",
        "        title (str): The title of the topic.\n",
        "        trend_data (Trend): The trend data.\n",
        "        options (ScriptOptions): User-selected script options.\n",
        "\n",
        "    Returns:\n",
        "        str: The generated script.\n",
        "    \"\"\"\n",
        "    # Construct the prompt for OpenAI\n",
        "    prompt = (\n",
        "        f\"Generate a {options.length} script for the topic '{title}' with a {options.tone} tone \"\n",
        "        f\"and a {options.style} style. The script should include the following details:\\n\"\n",
        "        f\"- Description: {trend_data.description}\\n\"\n",
        "        f\"- Sentiment: {trend_data.sentiment}\\n\"\n",
        "        f\"- Approximate Traffic: {trend_data.approx_traffic}\\n\\n\"\n",
        "        f\"Please ensure the script is engaging and suitable for a {options.length} presentation.\"\n",
        "    )\n",
        "\n",
        "    attempt = 0\n",
        "    max_retries = 5\n",
        "    wait_time = 1  # Initial wait time in seconds\n",
        "\n",
        "    while attempt < max_retries:\n",
        "        try:\n",
        "            async with openai_semaphore:\n",
        "                response = await openai.ChatCompletion.acreate(\n",
        "                    model=\"gpt-4\",\n",
        "                    messages=[\n",
        "                        {\n",
        "                            \"role\": \"system\",\n",
        "                            \"content\": \"You are a creative scriptwriter that generates scripts based on provided data.\"\n",
        "                        },\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": prompt\n",
        "                        }\n",
        "                    ],\n",
        "                    max_tokens=500,\n",
        "                    temperature=0.7,\n",
        "                )\n",
        "            script = response.choices[0].message['content'].strip()\n",
        "            return script\n",
        "        except openai.error.RateLimitError as e:\n",
        "    # Handle rate limit error\n",
        "\n",
        "            logger.error({\n",
        "                \"event\": \"RateLimitError in generate_script_for_topic_async\",\n",
        "                \"error\": str(e),\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            # Extract wait time from the error message if available\n",
        "            match = re.search(r\"Please try again in ([\\d\\.]+)s\", str(e))\n",
        "            if match:\n",
        "                wait_time = float(match.group(1))\n",
        "            else:\n",
        "                wait_time = min(wait_time * 2, 60)  # Exponential backoff with a max wait time\n",
        "            logger.info(f\"Rate limit exceeded. Waiting for {wait_time} seconds before retrying...\")\n",
        "            await asyncio.sleep(wait_time)\n",
        "            attempt += 1\n",
        "        except Exception as e:\n",
        "            logger.error({\n",
        "                \"event\": \"Error generating script_async\",\n",
        "                \"error\": str(e),\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            return \"Failed to generate script due to an unexpected error.\"\n",
        "\n",
        "    logger.error(\"Max retries exceeded for generate_script_for_topic_async. Returning failure message.\")\n",
        "    return \"Failed to generate script after multiple attempts due to rate limits.\"\n",
        "\n",
        "# ----------------------------\n",
        "# Main Function\n",
        "# ----------------------------\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to execute the script workflow.\n",
        "    \"\"\"\n",
        "    async def run():\n",
        "        # Function to display a changing message every 5 seconds with an additional note\n",
        "        def flashing_message(stop_event):\n",
        "            messages = [\n",
        "                \"🔍 Gathering the latest trends... (This could take up to 2 minutes. Please wait.)\",\n",
        "                \"⏳ Processing data, please wait... (This could take up to 2 minutes. Please wait.)\",\n",
        "                \"✨ Almost there, thank you for your patience! (This could take up to 2 minutes. Please wait.)\"\n",
        "            ]\n",
        "            idx = 0\n",
        "            while not stop_event.is_set():\n",
        "                message = messages[idx % len(messages)]\n",
        "                print(f\"\\r{message}   \", end='', flush=True)\n",
        "                for _ in range(5):\n",
        "                    if stop_event.is_set():\n",
        "                        break\n",
        "                    time.sleep(1)\n",
        "                idx += 1\n",
        "                print('\\r' + ' ' * len(message) + '   ', end='', flush=True)\n",
        "\n",
        "        while True:\n",
        "            print(\"Enter the country for trending topics data (e.g., United States or US):\")\n",
        "            country_input = input(\"Country: \").strip()\n",
        "            matching_country = get_matching_country(country_input, available_countries)\n",
        "            if not matching_country:\n",
        "                print(\"No matching countries found. Please try again.\")\n",
        "                # Optionally, display available countries\n",
        "                print(\"Available countries are:\")\n",
        "                for country in available_countries.keys():\n",
        "                    print(f\"- {country}\")\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        selected_country = matching_country\n",
        "        # Retrieve all possible codes for the selected country\n",
        "        selected_country_codes = available_countries[selected_country]\n",
        "        selected_country_code = selected_country_codes[0]  # Use the first code by default\n",
        "        print(f\"\\nYou selected: {selected_country}\")\n",
        "\n",
        "        print(\"\\nSelect the time frame for trending topics:\")\n",
        "        print(\"A. Last 4 hours\")\n",
        "        print(\"B. Last 24 hours\")\n",
        "        print(\"C. Last 7 days\")\n",
        "        time_range_selection = input(\"Enter the letter of the time frame you're interested in: \").strip().upper()\n",
        "        time_range_mapping = {'A': 4, 'B': 24, 'C': 168}\n",
        "        selected_timeframe_hours = time_range_mapping.get(time_range_selection)\n",
        "        if not selected_timeframe_hours:\n",
        "            print(\"Invalid selection. Exiting.\")\n",
        "            return\n",
        "\n",
        "        # Start flashing message while fetching all data\n",
        "        stop_event = threading.Event()\n",
        "        thread = threading.Thread(target=flashing_message, args=(stop_event,))\n",
        "        thread.start()\n",
        "\n",
        "        # Fetch all data asynchronously with robust error handling\n",
        "        try:\n",
        "            # Run the async fetch_and_aggregate_trending_data function\n",
        "            aggregated_trends = await fetch_and_aggregate_trending_data(selected_country_code, selected_timeframe_hours)\n",
        "        except Exception as e:\n",
        "            logger.error({\n",
        "                \"error\": str(e),\n",
        "                \"event\": \"fetch_data_error\",\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            aggregated_trends = []\n",
        "\n",
        "        # Stop flashing message\n",
        "        stop_event.set()\n",
        "        thread.join()\n",
        "        print()  # Move to the next line after flashing message\n",
        "\n",
        "        if not aggregated_trends:\n",
        "            logger.error({\n",
        "                \"event\": \"no_trending_topics_found\",\n",
        "                \"timestamp\": datetime.utcnow().isoformat()\n",
        "            })\n",
        "            print(\"No trending topics found.\")\n",
        "            return\n",
        "\n",
        "        # Display trending topics with pagination\n",
        "        # Pagination variables\n",
        "        batch_size = 10\n",
        "        total_trends = len(aggregated_trends)\n",
        "        current_index = 0\n",
        "\n",
        "        while current_index < total_trends:\n",
        "            # Determine the end index for the current batch\n",
        "            end_index = min(current_index + batch_size, total_trends)\n",
        "            batch_trends = aggregated_trends[current_index:end_index]\n",
        "\n",
        "            # Display the consolidated list with sentiments\n",
        "            print(f\"\\nCurrently Trending in {selected_country} in the last {selected_timeframe_hours} hours (Showing {current_index + 1} to {end_index} of {total_trends}):\\n\")\n",
        "            table = PrettyTable()\n",
        "            table.field_names = [\"No.\", \"Topic\", \"Description\", \"Source\", \"Approx Traffic\", \"Sentiment\"]\n",
        "            table.hrules = HRuleStyle.ALL  # Use HRuleStyle.ALL\n",
        "            table.max_width = 40  # Suitable for phone screens\n",
        "            for idx, trend in enumerate(batch_trends, start=current_index + 1):\n",
        "                title = textwrap.fill(trend.title, width=40)\n",
        "                description = textwrap.fill(trend.description, width=40)\n",
        "                source = trend.source\n",
        "                approx_traffic = trend.approx_traffic\n",
        "                sentiment = trend.sentiment\n",
        "                table.add_row([idx, title, description, source, approx_traffic, sentiment])\n",
        "            print(table)\n",
        "\n",
        "            # Prepare to handle user input for pagination or selection\n",
        "            while True:\n",
        "                user_input = input(\"Type 'more' or '+' to view more results, enter the number of the topic to select it, or any other key to exit: \").strip().lower()\n",
        "                if user_input in ['more', '+']:\n",
        "                    current_index = end_index\n",
        "                    break  # Continue to the next batch\n",
        "                elif user_input.isdigit():\n",
        "                    selected_idx = int(user_input)\n",
        "                    if 1 <= selected_idx <= total_trends:\n",
        "                        selected_topic_data = aggregated_trends[selected_idx - 1]\n",
        "                        selected_topic = selected_topic_data.title\n",
        "                        # Apply text wrapping to the selected topic message\n",
        "                        selected_topic_display = textwrap.fill(f\"You selected: {selected_topic}\", width=40)\n",
        "                        print(f\"\\n{selected_topic_display}\")\n",
        "                        # Generate scripts for the selected topic\n",
        "                        await generate_and_display_script_async(selected_topic_data)\n",
        "                        return  # Exit after script generation\n",
        "                    else:\n",
        "                        print(\"Invalid selection. Please enter a valid topic number.\")\n",
        "                else:\n",
        "                    print(\"Exiting the script.\")\n",
        "                    return\n",
        "\n",
        "        print(\"====\\nNo more trending topics available.\\n\")\n",
        "\n",
        "        # After all batches are displayed, prompt the user to select a topic\n",
        "        while True:\n",
        "            try:\n",
        "                selected_idx = input(\"Enter the number of the topic you're interested in (or type 0 to exit): \").strip()\n",
        "                if selected_idx == '0':\n",
        "                    print(\"Exiting the script.\")\n",
        "                    return\n",
        "                if selected_idx.isdigit():\n",
        "                    selected_idx = int(selected_idx)\n",
        "                    if 1 <= selected_idx <= total_trends:\n",
        "                        selected_topic_data = aggregated_trends[selected_idx - 1]\n",
        "                        selected_topic = selected_topic_data.title\n",
        "                        # Apply text wrapping to the selected topic message\n",
        "                        selected_topic_display = textwrap.fill(f\"You selected: {selected_topic}\", width=40)\n",
        "                        print(f\"\\n{selected_topic_display}\")\n",
        "                        # Generate scripts for the selected topic\n",
        "                        await generate_and_display_script_async(selected_topic_data)\n",
        "                        return  # Exit after script generation\n",
        "                    else:\n",
        "                        print(\"Invalid selection. Please enter a valid topic number.\")\n",
        "                else:\n",
        "                    print(\"Invalid input. Please enter a number.\")\n",
        "            except ValueError:\n",
        "                print(\"Invalid input. Please enter a valid number.\")\n",
        "\n",
        "    asyncio.run(run())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1s-3u0czUndpxCkgirEnqndEDGpEPjpJ_",
      "authorship_tag": "ABX9TyOgkDp+FEFG76eTeQPcPPPz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}